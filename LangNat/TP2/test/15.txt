en statistique , l ’ analyse discriminante linéaire ( aussi lda , en anglais : linear discriminant analysis ) fait partie des techniques d ’ analyse discriminante prédictive . il s ’ agit d ’ expliquer et de prédire l ’ appartenance d ’ un individu à une classe ( groupe ) prédéfinie à partir de ses caractéristiques mesurées à l ’ aide de variables prédictives . la variable à prédire est forcément catégorielle ( discrète ) , elle possède 3 modalités dans notre exemple . les variables prédictives sont a priori toutes continues . il est néanmoins possible de traiter les variables prédictives discrètes moyennant une préparation adéquate des données . l ’ analyse discriminante linéaire peut être comparée aux méthodes supervisées développées en apprentissage automatique et à la régression logistique développée en statistique . nous disposons d ’ un échantillon de n observations réparties dans k groupes d ’ effectifs n _ k. notons y la variable à prédire , elle prend ses valeurs dans l ’ ensemble \ { y _ 1 , \ dots , y _ k \ } des classes . nous disposons de j variables prédictives x = ( x _ 1 , \ dots , x _ j ) . nous notons \ mu _ k les centres de gravité des nuages de points conditionnels et w _ k leurs matrice de variance-covariance . l ’ objectif est de produire une règle d ’ affectation x ( \ omega ) \ mapsto y ( \ omega ) qui permet de prédire , pour une observation \ omega donnée , sa valeur associée de y à partir des valeurs prises par x. l ’ approche non-paramétrique n ’ effectue aucune hypothèse sur cette distribution mais propose une procédure d ’ estimation locale des probabilités , au voisinage de l ’ observation \ omega \ , à classer . les procédures les plus connues sont la méthode d&apos; estimation par noyau et la méthode des plus proches voisins . la principale difficulté est de définir de manière adéquate le voisinage . la seconde approche effectue une hypothèse sur la distribution des nuages de points conditionnels , on parle dans ce cas d ’ analyse discriminante paramétrique . l ’ hypothèse la plus communément utilisée est sans aucun doute l ’ hypothèse de multinormalité ( voir loi normale ) . la règle d ’ affectation devient donc y ( \ omega ) = \ arg \ max _ k d ( y = y _ k , x ( \ omega ) ) . si l ’ on développe complètement le score discriminant , nous constatons qu ’ il s ’ exprime en fonction du carré et du produit croisé entre les variables prédictives . on parle alors d ’ analyse discriminante quadratique . très utilisée en recherche car elle se comporte très bien , en termes de performances , par rapport aux autres méthodes , elle est moins répandue auprès des praticiens . en effet , l ’ expression du score discriminant étant assez complexe , il est difficile de discerner clairement le sens de la causalité entre les variables prédictives et la classe d ’ appartenance . il est notamment mal aisé de distinguer les variables réellement déterminantes dans le classement , l ’ interprétation des résultats est assez périlleuse . une seconde hypothèse permet de simplifier encore les calculs , c ’ est l ’ hypothèse d ’ homoscédasticité : les matrices de variances covariances sont identiques d ’ un groupe à l ’ autre . géométriquement , cela veut dire que les nuages de points ont la même forme ( et volume ) dans l ’ espace de représentation . en développant l ’ expression du score discriminant après introduction de l ’ hypothèse d ’ homoscédasticité , on constate qu ’ elle s ’ exprime linéairement par rapport aux variables prédictives . \ end { matrix } \ right . cette présentation est séduisante à plus d ’ un titre . il est possible , en étudiant la valeur et le signe des coefficients , de déterminer le sens des causalités dans le classement . de même , il devient possible , comme nous le verrons plus loin , d ’ évaluer le rôle significatif des variables dans la prédiction . les hypothèses de multinormalité et d ’ homoscédasticité peuvent sembler trop contraignantes , restreignant la portée de l ’ analyse discriminante linéaire dans la pratique . la notion clé qu ’ il faut retenir en statistique est la notion de robustesse . même si les hypothèses de départ ne sont pas trop respectées , une méthode peut quand même s ’ appliquer . c ’ est le cas de l ’ analyse discriminante linéaire . le plus important est de le considérer comme un séparateur linéaire . dans ce cas , si les nuages de points sont séparables linéairement dans l ’ espace de représentation , elle peut fonctionner correctement . par rapport aux autres techniques linéaires telles que la régression logistique , l ’ analyse discriminante présente des performances comparables . elle peut être lésée néanmoins lorsque l ’ hypothèse d ’ homoscédasticité est très fortement violée . de manière classique en apprentissage supervisé , pour évaluer les performances d&apos; une fonction de classement , nous confrontons ses prédictions avec les vraies valeurs de la variable à prédire sur un fichier de données . le tableau croisé qui en résulte s ’ appelle une matrice de confusion avec : en ligne les vraies classes d ’ appartenance , en colonne les classes d ’ appartenance prédites . le taux d ’ erreur ou taux de mauvais classement est tout simplement le nombre de mauvais classement , lorsque la prédiction ne coïncide pas avec la vraie valeur , rapporté à l ’ effectif du fichier de données . le taux d ’ erreur a de séduisant qu ’ il est d ’ interprétation aisée , il s ’ agit d ’ un estimateur de la probabilité de se tromper si l ’ on applique la fonction de classement dans la population . attention cependant , on parle de taux biaisé ou taux d&apos; erreur en résubstitution , le taux d ’ erreur mesuré sur les données qui ont servi à construire la fonction de classement . tout simplement parce que les données sont juges et parties dans ce schéma . la bonne procédure serait de construire la fonction de classement sur une fraction des données , dites d&apos; apprentissage ; puis de l ’ évaluer sur une autre fraction de données , dite de test . le taux d ’ erreur en test ainsi mesuré est un indicateur digne de foi . la pratique veut que la répartition des données en apprentissage et test soit de 2 / 3 – 1 / 3 . mais en réalité , il n ’ y a pas de règle véritable . le plus important est de concilier deux exigences contradictoires : en avoir suffisamment en test pour obtenir une estimation stable de l ’ erreur , tout en réservant suffisamment en apprentissage pour ne pas pénaliser la méthode d ’ apprentissage . lorsque les effectifs sont faibles , et que le partage apprentissage-test des données n ’ est pas possible , il existe des méthodes de ré-échantillonnage telles que la validation croisée ou le bootstrap pour évaluer l ’ erreur de classement . le taux d ’ erreur permet d ’ évaluer et de comparer des méthodes , quelles que soient leurs hypothèses sous-jacentes . dans le cas de l ’ analyse discriminante linéaire , nous pouvons exploiter le modèle probabiliste pour réaliser des tests d ’ hypothèses . un premier test permet de répondre à la question suivante : est-il possible de discerner les nuages de points dans l ’ espace de représentation . rapporté dans le cadre multinormal , cela revient à vérifier si les centres de gravité conditionnels sont confondus ( hypothèse nulle ) ou si un au moins de ces centres de gravité s ’ écarte significativement des autres ( hypothèse alternative ) . la table des valeurs critiques de la loi de wilks étant rarement disponible dans les logiciels , on utilise couramment les transformations de bartlett et de rao qui suivent respectivement une loi du khi-2 et de fisher . avec un prisme différent , nous constatons que ce test peut s ’ exprimer comme une généralisation multidimensionnelle de l ’ analyse de variance à un facteur ( anova ) , on parle dans ce cas de manova ( multidimensional analysis of variance ) . comme dans toutes les méthodes linéaires , il est possible d ’ évaluer individuellement chaque variable prédictive , et éventuellement d ’ éliminer celles qui ne sont pas significatives dans la discrimination . elle suit une loi de fisher à ( k-1 , n-k-j ) degrés de liberté . une analyse discriminante linéaire a été lancée sur les flea beetles décrites dans l&apos; article analyse discriminante . les résultats sont les suivants . la matrice de confusion indique qu&apos; une seule erreur a été commise , un « concinna » a été classé en « heikertingeri » . le taux d&apos; erreur associé est de 1,35 % . ce résultat est à relativiser , il a été établi sur les données ayant servi à l&apos; apprentissage . les centres de gravité des trois nuages de points s&apos; écartent significativement . c&apos; est ce que nous indique la statistique de wilks dans la section manova . les probabilités critiques associées , transformation de bartlett et de rao , sont proches de 0 . ce résultat numérique confirme l&apos; impression visuelle laissée par la projection des nuages de points dans l&apos; espace de représentation ( voir analyse discriminante ) . la variable à prédire comportant 3 modalités , nous obtenons 3 fonctions de classement linéaires . l&apos; évaluation individuelle des variables dans la discrimination indique qu&apos; elles sont toutes les deux très significatives ( p-value proches de 0 ) . pour classer une nouvelle observation avec les coordonnées ( width = 150 et angle = 15 ) , nous appliquons les fonctions de la manière suivante . sur la base de ces calculs , nous affectons à cette observation la classe « concinna » . m. bardos , analyse discriminante - application au risque et scoring financier , dunod , 2001 . g. celeux , j.-p. nakache , analyse discriminante sur variables qualitatives , polytechnica , 1994 .
