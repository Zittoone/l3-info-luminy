l ’ analyse sémantique latente ( lsa , de l&apos; anglais : latent semantic analysis ) ou indexation sémantique latente ( ou lsi , de l&apos; anglais : latent semantic indexation ) est un procédé de traitement des langues naturelles , dans le cadre de la sémantique vectorielle . la lsa fut brevetée en 1988template : en dépôt de brevet par scott deerwester , susan dumais , george furnas , richard harshman , thomas landauer , karen lochbaum et lynn streeter et publiée en 1990template : article . elle permet d&apos; établir des relations entre un ensemble de documents et les termes qu&apos; ils contiennent , en construisant des « concepts » liés aux documents et aux termes . la lsa utilise une matrice qui décrit l&apos; occurrence de certains termes dans les documents . c&apos; est une matrice creuse dont les lignes correspondent aux « termes » et dont les colonnes correspondent aux « documents » . les « termes » sont généralement des mots tronqués ou ramenés à leur radical , issus de l&apos; ensemble du corpus . on a donc le nombre d&apos; apparition d&apos; un mot dans chaque document , et pour tous les mots . ce nombre est normalisé en utilisant la pondération tf-idf ( de l&apos; anglais : term frequency – inverse document frequency ) , combinaison de deux techniques : un coefficient de la matrice est d&apos; autant plus grand qu&apos; il apparaît beaucoup dans un document , et qu&apos; il est rare — pour les mettre en avant . cette matrice est courante dans les modèles sémantiques standards , comme le modèle vectoriel , quoique sa forme matricielle ne soit pas systématique , étant donné qu&apos; on ne se sert que rarement des propriétés mathématiques des matrices . la lsa transforme la matrice des occurrences en une « relation » entre les termes et des « concepts » , et une relation entre ces concepts et les documents . on peut donc relier des documents entre eux. étant donné une requête , traduire les termes de la requête dans l&apos; espace des concepts , pour retrouver des documents liés sémantiquement ( recherche d&apos; information ) . trouver la meilleure similarité entre petits groupes de termes , de façon sémantique ( c&apos; est-à-dire dans le contexte d&apos; un corpus de connaissance ) , comme par exemple dans la modélisation de la réponse aux questionnaires à choix multiples ( qcm ) template : article. la polysémie d&apos; un mot fait qu&apos; il a plusieurs sens selon le contexte — on pourrait de même éviter des documents contenant le mot recherché , mais dans une acception qui ne correspond pas à ce que l&apos; on désire ou au domaine considéré. la matrice d&apos; origine peut être présumée « trop creuse » : elle contient plutôt les mots propres à chaque documents que les termes liés à plusieurs documents — c&apos; est également un problème de synonymie . ce regroupement est beaucoup plus difficile à interpréter — il est justifié d&apos; un point de vue mathématique , mais n&apos; est pas pertinent pour un locuteur humain . de même , une colonne de cette matrice est un vecteur qui correspond à un document , et dont les composantes sont l&apos; importance dans son propre contenu de chaque terme . \ textbf { t } _ i ^ t \ textbf { t } _ p ( = \ textbf { t } _ p ^ t \ textbf { t } _ i ) . \ textbf { d } _ j ^ t \ textbf { d } _ q = \ textbf { d } _ q ^ t \ textbf { d } _ j. les valeurs \ sigma _ 1 , \ dots , \ sigma _ l sont les valeurs singulières de x. d&apos; autre part , les vecteurs u _ 1 , \ dots , u _ l et v _ 1 , \ dots , v _ l sont respectivement singuliers à gauches et à droite . on remarque également que la seule partie de u qui contribue à \ textbf { t } _ i est la i ième ligne . on note désormais ce vecteur \ hat \ textrm { t } _ i. de même la seule partie de v ^ t qui contribue à \ textbf { d } _ j est la j ième colonne , que l&apos; on note \ hat \ textrm { d } _ j. lorsqu&apos; on sélectionne les k plus grandes valeurs singulières , ainsi que les vecteurs singuliers correspondants dans u et v , on obtient une approximation de rang k de la matrice des occurrenceson peut même montrer que c&apos; est la meilleure approximation , au sens de la norme de frobenius . une preuve est donnée dans l&apos; article sur la décomposition en valeurs singulières . . le point important , c&apos; est qu&apos; en faisant cette approximation , les vecteurs « termes » et « documents » sont traduits dans l&apos; espace des « concepts » . voir dans quelle mesure les documents j et q sont liés , dans l&apos; espace des concepts , en comparant les vecteurs \ sigma _ k \ hat \ textbf { d } _ j et \ sigma _ k \ hat \ textbf { d } _ q. on peut faire cela en évaluant la similarité cosinus . \ hat \ textbf { q } = \ sigma _ k ^ { -1 } u _ k ^ t \ textbf { q } avant de comparer ce vecteur au corpus . la décomposition en valeurs singulières est généralement calculée par des méthodes optimisées pour les matrices larges — par exemple l&apos; algorithme de lanczos — par des programmes itératifs , ou encore par des réseaux de neurones , cette dernière approche ne nécessitant pas que l&apos; intégralité de la matrice soit gardée en mémoiretemplate : en template : lien conférence . celles du modèle sac de mots , sur lequel elle est basée , où le texte est représenté comme un ensemble non ordonné de mots . l&apos; impossibilité ( dans le modèle de base ) de prendre en compte la polysémie ( c&apos; est-à-dire les sens multiples d&apos; un mot ) , car un mot ne correspond qu&apos; à un seul point de l&apos; espace sémantique . le modèle statistique de l&apos; analyse sémantique latente ne correspond pas aux données observées : elle suppose que les mots et documents forment ensemble un modèle gaussien ( c&apos; est l&apos; hypothèse ergodique ) , alors qu&apos; on observe une distribution de poisson . ainsi , une approche plus récente est l&apos; analyse sémantique latente probabiliste , ou plsa ( de l&apos; anglais : probabilistic latent semantic analysis ) , basée sur un modèle multinomial . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « latent semantic analysis » ( voir la liste des auteurs ) . ( en ) michael w. berry , susan t. dumais , gavin w. o&apos; brien , « using linear algebra for intelligent information retrieval » , siam review , vol. 37 , no 4 , ‎ &lt; time class = &quot; nowrap &quot; datetime = &quot; 1995-12 &quot; &gt; décembre 1995 &lt; / time &gt; , p. 573-595 ( lire en ligne ) ( pdf ) . ( en ) thomas hofmann , probabilistic latent semantic analysis , uncertainty in artificial intelligence , 1999 . ( en ) the semantic indexing project , un projet open source d&apos; indexation sémantique latente .
