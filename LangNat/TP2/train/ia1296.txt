vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants . consultez la liste des tâches à accomplir en page de discussion . l&apos; inférence bayésienne est une méthode d&apos; inférence permettant de déduire la probabilité d&apos; un événement à partir de celles d&apos; autres événements déjà évalués . elle s&apos; appuie principalement sur le théorème de bayes . dans la logique d&apos; aristote développée dans l&apos; algèbre de boole et le calcul des propositions , une proposition ne peut être que vraie ou fausse , et les règles d&apos; inférence ne font intervenir que ces deux valeurs . le raisonnement bayésien s&apos; intéresse aux cas où une proposition pourrait être vraie ou fausse , non pas en raison de son rapport logique à des axiomes tenus pour assurément vrais , mais selon des observations où subsiste une incertitude . on attribue à toute proposition une valeur dans l&apos; intervalle ouvert allant de 0 ( faux à coup sûr ) à 1 ( vrai à coup sûr ) stricto sensu , du fait de la définition de la probabilité par l&apos; intégrale de lebesgue , à coup sûr reste une approximation . la probabilité pour qu&apos; en tirant au hasard les décimales d&apos; un nombre réel ( caractère par caractère , par exemple ) on tombe exactement sur un entier ou sur un rationnel est nulle . les entiers et les rationnels n&apos; en existent pas moins . on est ici à la frontière entre les mathématiques et l&apos; ontologie &#91; réf. souhaitée &#93; catégorie : article à référence souhaitée . . quand un événement possède plus de deux issues possibles , on considère une distribution de probabilité pour ces issues . exemple : présumer qu&apos; un dé est non pipé , c&apos; est attribuer , avant toute expérience , la probabilité 1 / 6 à chacune de ses faces . de toutes les distributions possibles - une infinité - répondant aux contraintes du problème ( à savoir ici que la somme des six probabilités soit 1 ) , c&apos; est celle d&apos; entropie maximale qui sera choisie , car c&apos; est celle qui contient le moins d&apos; information , et donc la moins prévenuel&apos; entropie de la distribution est définie comme \ textstyle - \ sum _ { i = 1 } ^ np _ i \ log _ b p _ i. une probabilité pi égale pour chaque i maximise l&apos; entropie ici . . l&apos; inférence bayésienne révise la probabilité des propositions au fur et à mesure des observations , incluant , dans l&apos; analyse de thomas bayes qui lui donne son nom , la première opinion ( a priori ) sur la probabilité des prémisses . le raisonnement bayésien interprète la probabilité comme la simple traduction numérique d&apos; un état de connaissance , du degré de confiance accordé à une hypothèse ( voir le théorème de cox-jaynes ) . jaynes utilisait à ce sujet avec ses étudiants la métaphore d&apos; un robot à logique inductive . pour les autres disciplines des statistiques et des probabilités , celle-ci restait alors souvent interprétée comme le passage à la limite de la fréquence d&apos; un événement , ou comme le chiffrage du rapport entre cas favorables et cas possibles dans une expérience de pensée . l&apos; inférence bayésienne effectue des calculs sur les énoncés probabilistes . ces énoncés doivent être clairs et concis afin d&apos; éviter toute confusion . l&apos; inférence bayésienne est particulièrement utile dans les problèmes d&apos; induction . les méthodes bayésiennes se distinguent des méthodes dites standards par l&apos; usage systématique de règles formelles raffinant les probabilités par l&apos; observation . avant de passer à la description de ces règles , familiarisons-nous avec la notation employée . la notation probabiliste reprend pour base la notation classique des événements en probabilité qui elle-même s&apos; inspire de la notation logique . soit deux événements a et b quelconques. désigne l&apos; événement « non survenue de a » . désigne l&apos; événement « survenue de a et de b » . désigne l&apos; événement « survenue de a ou de b » . la théorie bayésienne introduit les notations suivantes , exprimant la probabilité au sens bayésien et la notion de probabilité conditionnelle. désigne la probabilité de survenue de l&apos; événement a. désigne la probabilité de survenue de l&apos; événement a sachant que l&apos; événement b est survenu . il existe seulement deux règles pour combiner les probabilités , à partir desquelles est bâti tout l&apos; édifice bayésien . ces règles sont les règles d&apos; addition et de multiplication . en conséquence , si on connaît dans le détail les causes possibles d&apos; une conséquence observée et leurs probabilités , l&apos; observation des effets permet de remonter aux causes . remarquez que l&apos; inversion de la probabilité introduit le terme p ( \ mathrm { a } ) , la probabilité a priori de l&apos; événement a , indépendamment de l&apos; événement b. cette estimation a priori est ignorée par les autres méthodes probabilistes . dans la pratique , quand une probabilité est très proche de 0 ou de 1 , seule l&apos; observation d&apos; éléments considérés eux-mêmes comme très improbables est susceptible de la modifier . ev est une abréviation pour weight of evidence , parfois traduit en français par le mot « évidence » ; la formulation la plus conforme à l&apos; expression anglaise d&apos; origine serait le mot à mot « poids de témoignage » ou « poids de la preuve » , mais par une coïncidence amusante « évidence » se montre très approprié en français pour cet usage précis . l&apos; utilisation du logarithme fait varier la valeur de l&apos; évidence sur tout le domaine des nombres réels quand la probabilité va de 0 à 1 , avec une meilleure lisibilité des très petites ( 10 ^ -5 , 10 ^ -10 … ) et des très grandes ( 0,999999 , 0,999999999 ) probabilités , faciles à confondre intuitivement . l&apos; intérêt de cette notation , outre qu&apos; elle évite d&apos; avoir trop de décimales au voisinage de 0 et de 1 , est qu&apos; elle permet de présenter l&apos; apport d&apos; une observation sous une forme indépendante des observateurs , donc objective : il faut le même poids de témoignage pour faire passer un événement d&apos; une plausibilité de -4 ( probabilité 10-4 avec logarithme en base 10 ) à -3 ( probabilité 10-3 ) que pour le faire passer de -1 ( probabilité 0,09 ) à 0 ( probabilité 0,5 soit une chance sur deux ) , ce qui n&apos; était pas évident en gardant la représentation probabiliste pure. le dit ( pour decimal digit ) ) , avec des logarithmes à base 10 sans multiplicateur , aussi appelé hartley ( symbole hart ) , du nom de ralph hartley qui le proposa en 1928. le nats utilisant les logarithmes népériens , dits aussi naturels . si on prend le logarithme en base 2 , l&apos; évidence s&apos; exprime en bits : \ scriptstyle \ mathrm { ev } ( p ) = \ log _ { 2 } \ frac { p } { ( 1-p ) } . on a evdb ≃ 3,0103 evbit. comme pour tout autre modèle , les effets de différents choix a priori peuvent être considérés de front. les méthodes statistiques classiques , dites aussi fréquentistes utilisent des méthodes personnelles pour traiter des fréquences impersonnelles . les bayésiens font donc le choix de modéliser leurs attentes en début de processus ( quitte à réviser ce premier jugement en donnant des poids de plus en plus faibles aux a priori au fur et à mesure des observations ) , tandis que les statisticiens classiques se fixaient a priori une méthode et une hypothèse arbitraires et ne traitaient les données qu&apos; ensuite . la possibilité de diminuer automatiquement le poids des a priori au fur et à mesure de l ’ acquisition des données a permis aux modèles bayésiens d&apos; être largement utilisés en data mining . en effet , contrairement aux méthodes classiques , il ne nécessitent que peu d&apos; intervention humaine pour redéfinir à grande vitesse de nombreuses classes hypothèses en éliminant les moins validées par les données du moment . les deux approches se complètent , la statistique étant en général préférable lorsque les informations sont abondantes et d&apos; un faible coût de collecte . ainsi , un sondage d&apos; opinion ne coûte que quelques euros et un test en fin de chaîne de fabrication que quelques centimes : les statistiques classiques conviennent alors parfaitement . lorsqu&apos; il est question de s&apos; informer en effectuant un forage pétrolier , le coût des mesures devient tel que les méthodes bayésiennes , qui les minimisent , sont préférables ( voir aussi morphologie mathématique ) . en cas de profusion de données , les résultats sont asymptotiquement les mêmes dans chaque méthode , la bayésienne étant simplement plus coûteuse en calcul . la diminution énorme des coûts de calcul consécutive à la loi de moore a joué dans la popularité grandissante des méthodes bayésiennes de 1970 à 2010 &#91; réf. nécessaire &#93; catégorie : article à référence nécessaire . en revanche , la méthode bayésienne permet de traiter des cas où la statistique ne disposerait pas suffisamment de données pour qu&apos; on puisse en appliquer les théorèmes limites . le psi-test bayésien ( qui est utilisé pour déterminer la plausibilité d&apos; une distribution par rapport à des observations ) est asymptotiquement convergent avec le χ ² des statistiques classiques à mesure que le nombre d&apos; observations devient grand . le choix apparemment arbitraire d&apos; une distance euclidienne dans le χ ² est ainsi parfaitement justifié a posteriori par le raisonnement bayésien &#91; réf. insuffisante &#93; . un cycle de cours de stanislas dehaene au collège de france intitulé psychologie cognitive expérimentale mentionne l&apos; inférence bayésienne dans les titres de quatre de ses sept exposés . une des conférences se nomme du reste l&apos; implémentation neuronale des mécanismes bayésiens . c&apos; est une revanche posthume pour jaynes dont une communication de 1957 sur le probable fonctionnement bayésien du cerveau avait été rejetée comme « non en rapport avec le sujet des neurosciences » template : harvsp . . les références données font état de travaux similaires dans plusieurs pays . voir également les articles logit et régression logistique . un important article souvent citétemplate : pdf template : ena fast-learning algorithm for deep belief nets , de geoffrey e. hinton , simon osindero ( université de toronto ) et yee-whye teh ( université de singapour ) a introduit la notion de deep learning efficace à partir de réseaux bayésiens . cette démarche fut induite pragmatiquement par application du théorème de bayes bien connu en dehors d&apos; un strict modèle probabiliste antérieurement validé . après la publication posthume des travaux de bayes , abel et laplace adhérèrent immédiatement au raisonnement bayesien ( le second en tire même la loi de succession qui porte son nom ) . le théorème de cox le formalisa sur des bases axiomatiques indépendantes de la théorie classique des probabilités et les travaux de good , jeffreys , tribus et jaynes la vulgarisèrent . l&apos; approche fréquentiste se prêtait en effet mieux aux problèmes alors rencontrés ( grands volumes de données très irrégulières , par exemple en agriculture ) et aux outils disponibles ( essentiellement comptables - quatre opérations de base - et manuels ou mécanographiques , donc limités et lents ) . l&apos; usage de l&apos; approche bayésienne était limité à un champ d&apos; applications restreint parce que demandant des calculs plus complexes , et pour cette raison onéreux à cette époque . l&apos; effondrement du coût des calculs entraîné par le développement de l&apos; informatique a permis un usage plus courant des méthodes bayésiennes , notamment dans le cadre de l&apos; intelligence artificielle : perception automatique , reconnaissance visuelle ou de la parole , deep learning . cette notation est souvent attribuée à i. j. good . ce dernier en attribuait cependant la paternité à alan turing et , indépendamment , à d&apos; autres chercheurs dont harold jeffreys . c&apos; est peu après les publications de jeffreys qu&apos; on découvrit qu&apos; alan turing avait déjà travaillé sur cette question en nommant les quantités correspondantes log-odds dans ses travaux personnels . la position des statistiques classiques est de dire qu&apos; on ne peut pas tirer de conclusion significative de trois tirages ( en effet , un côté étant déterminé par le premier lancer , on a bien une probabilité 1 / 8 d&apos; avoir les trois tirages suivants du côté identique avec une pièce parfaitement honnête , ce qui ne fournit pas les 95 % de certitude demandés traditionnellement ) . l&apos; approche bayésienne mesurera simplement que cette probabilité de 1 / 8ème déplace linéairement de 10 log10 ( 1 / 8 / 7 / 8 ) = - 8,45 db l&apos; évidence d&apos; honnêteté de la pièce . si nous lui accordions 40 db ( pièce sortie par exemple de notre propre porte-monnaie et lancée par nous ) , cette évidence passe à 31,55 db . en d&apos; autres termes , la probabilité subjectivetribus rappelle que toute probabilité , parce qu&apos; elle traduit un simple état de connaissance , est - par construction - subjective de sa normalité reste élevée ( 30 db correspondent à une probabilité de 10-3 environ que la pièce soit biaisée ) . si en revanche la pièce est fournie par un individu que nous jugeons louche et que nous estimions à 0 db son évidence d&apos; honnêteté ( autant de chances d&apos; être bonne que biaisée ) , cette évidence passe à -8,45 db , ce qui correspond maintenant à une probabilité subjective de 87,5 % que la pièce soit biaisée , et nous serions avisés de mettre fin au jeu . nous dirions aujourd&apos; hui , 2016 , que cette évidence inférieure à -40 db rend extrêmement improbable qu&apos; il y ait équiprobabilité entre la naissance d&apos; une fille et celle d&apos; un garçon . laplace n&apos; emploie toutefois pas cette terminologie , qui n&apos; existe pas encore à son époque . par précaution , laplace effectue ensuite le même calcul sur d&apos; autres statistiques concernant londres et à paris , qui confirment ce résultat . il naît donc davantage de garçons que de filles , ce qu&apos; expliquera - cette fois-ci pour toute la classe des mammifères - la théorie synthétique de l&apos; évolution . un médecin effectue le dépistage d&apos; une maladie à l&apos; aide d&apos; un test fourni par un laboratoire . le test donne un résultat booléen : soit positif , soit négatif . les études sur des groupes tests ont montré que , lorsque le patient est porteur de la maladie , le test est positif dans 90 % des cas . pour un patient non atteint de la maladie , le test est positif dans un cas sur 100 ( faux positif ) . le médecin reçoit un résultat positif pour le test d&apos; un patient . il souhaiterait savoir quelle est la probabilité que le patient soit réellement atteint de la maladie . nous pouvons remarquer que le résultat du calcul dépend de p ( m ) soit la probabilité globale que le patient soit malade , autrement dit , de la proportion de malades dans la population à laquelle appartient le patient . nous constatons que bien que le test soit positif pour 90 % des personnes atteintes et produise seulement 1 % de faux positif , le résultat est extrêmement peu concluant . ce résultat qui peut sembler paradoxal parait plus évident si nous effectuons une analyse de population sur 1 million de personnes. sur les 999 990 saines , 1 % soit environ 10 000 seront des faux positifs . finalement sur 1 million de tests , nous obtiendrons 10 009 tests positifs dont seulement 9 vrais positifs . la probabilité qu&apos; un patient ayant un résultat positif soit malade reste donc faible car la maladie est dans l&apos; absolu extrêmement rare . d&apos; un tel résultat nous pourrions conclure que le test est complètement inutile , pourtant il faut noter que la probabilité de trouver un patient malade par ce test reste 90 fois supérieure à une recherche par tirage aléatoire ( p ( m ) = 0.00001 ) . sur les 999 000 saines , 1 % soit 9 990 seront des faux positifs . finalement sur 1 million de tests , nous obtiendrons 10 890 tests positifs dont seulement 900 vrais positifs . la probabilité qu&apos; un patient ayant un résultat positif soit malade s&apos; établit donc à 900 ÷ 10 890 , soit 8,3 % , ce qui reste faible , mais est tout de même 83 fois plus que dans la population générale . si la maladie est épidémique , avec une personne sur dix touchée , on trouvera le test concluant , puisque la probabilité pour qu&apos; une personne revenant avec un test positif soit malade sera de 91 % . reprenons les trois cas d&apos; application du test . imaginons deux boîtes de biscuits . l&apos; une , a , comporte 30 biscuits au chocolat et 10 ordinaires . l&apos; autre , b , en comporte 20 de chaque sorte . notons ha la proposition « le gâteau vient de la boîte a » et hb la proposition « le gâteau vient de la boîte b » . si lorsqu&apos; on a les yeux bandés les boîtes ne se distinguent que par leur nom , nous avons p ( ha ) = p ( hb ) , et la somme fait 1 , puisque nous avons bien choisi une boîte , soit une probabilité de 0,5 pour chaque proposition . si nous imposons une probabilité a priori quelconque de suspecter une boîte particulière plutôt que l&apos; autre , le même calcul effectué avec cette probabilité a priori fournit également 0,53 bit . c&apos; est là une manifestation de la règle de cohérence qui constituait l&apos; un des desiderata de cox . article détaillé : problème du char d&apos; assaut allemand . supposons qu&apos; un pays numérote les plaques minéralogiques de ses véhicules de 1 en 1 à partir de 1 . nous observons n plaques différentes . pour n supérieur à 3 , on démontre par la méthode de bayes que le meilleur estimateur du numéro en cours ne dépend que du nombre d&apos; observations et de la plus haute immatriculation trouvée smaxtribus 1969 , p. 248 , prend l&apos; exemple de l&apos; estimation de la production de pompes d&apos; un concurrent par le relevé de leurs numéros de série . le calcul se trouve à la page suivante . l&apos; estimation est d&apos; autant plus exacte que le nombre d&apos; observations est grand . la variance de l&apos; estimation elle-même est inversement proportionnelle au carré de n. les ouvrages relatifs à l&apos; utilisation sont plus rares que les ouvrages d&apos; enseignement généraux . les méthodes bayésiennes , plus coûteuses , ne justifient ce surcoût que si les enjeux et risques financiers sont importants ( prospection pétrolière , recherche de médicaments … ) . ce sont dans ces deux cas des sociétés privées ( pétroliers , laboratoires pharmaceutiques … ) qui les financent , et celles-ci n&apos; ont pas vocation à donner à leurs concurrents des informations financées avec les fonds de leurs actionnaires ( voir propriété intellectuelle ) . certains problèmes ludiques comme les tentatives de prédictions dans certaines séries ( travail de richard vale sur game of thrones &#91; 1 &#93; en sont également une utilisation possible . ( en ) josé m. bernardo et adrian f.m. smith , bayesian theory , new york , john wiley , &lt; time &gt; 2000 &lt; / time &gt; ( 1re éd. 1994 ) , ( référence de l&apos; approche formelle de la théorie bayésienne via les fonctions de perte et la théorie de la décision ) . ( en ) andrew gelman , john b carlin , hal s stern et donald b rubin , bayesian data analysis , chapman crc , &lt; time &gt; 2003 &lt; / time &gt; . ( en ) edwin t. jaynes , « how does the brain do plausible reasoning ? » , dans g. j. erickson and c. r. smith ( eds . ) , science and engineering -- maximum-entropy and bayesian methods , dordrecht , kluwer , &lt; time &gt; 1988 &lt; / time &gt; ( lire en ligne ) . cet article fut publié pour la première fois en tant que stanford microwave laboratory report en 1957 . auparavant , jayne l&apos; avait soumis aux ire transactions on information theory , qui en rejeta une version longue , disponible avec l&apos; opinion des membres du comité de lecture et la réponse de jayne ici . jaynes , e.t. ( 2003 ) probability theory : the logic of science ( en anglais ) . davic mckay , information theory , inference , and learning algorithms , cambridge university press , 2005 . robert , c.p. ( 1992 ) l&apos; analyse statistique bayésienne . economica , paris . robert , c.p. ( 1994 ) . the bayesian choice : a decision theoretic motivation . new york : springer verlag ( première édition , en français : l&apos; analyse statistique bayésienne , paris : economica , 1992 ; traduit en français en 2006 chez springer-verlag , paris ) . francisco j. samaniego , a comparison of the bayesian and frequentist approaches to estimation , 2010 , ( isbn 978-1-4419-5940-9 ) . ( en ) myron tribus , rational descriptions , decisions and designs , pergamon press , &lt; time &gt; 1969 &lt; / time &gt; ( lire en ligne ) . myron tribus ( trad . jacques pezier ) , décisions rationnelles dans l&apos; incertain &#91; « rational descriptions , decisions and designs » &#93; , paris , masson , &lt; time &gt; 1972 &lt; / time &gt; , 503 p. . satoshi watanabe , knowing and guessing : a quantitative study of inference and information , wiley , 1969 , ( isbn 0471921300 et 9780471921301 ) .
