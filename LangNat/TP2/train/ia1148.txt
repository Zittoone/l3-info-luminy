en mathématiques , une chaîne de markov est un processus de markov à temps discret , ou à temps discret et à espace d&apos; états discret . un processus de markov est un processus stochastique possédant la propriété de markov : l&apos; information utile pour la prédiction du futur est entièrement contenue dans l&apos; état présent du processus et n&apos; est pas dépendante des états antérieurs ( le système n&apos; a pas de « mémoire » ) . les processus de markov portent le nom de leur découvreur , andreï markov . un processus de markov à temps discret est une séquence \ scriptstyle \ x _ 0 , \ scriptstyle \ x _ 1 , \ scriptstyle \ x _ 2 , \ scriptstyle \ x _ 3 , \ \ dots de variables aléatoires à valeurs dans l ’ espace des états , qu&apos; on notera \ scriptstyle \ e \ dans la suite . la valeur \ scriptstyle \ x _ n \ est létat du processus à l&apos; instant \ scriptstyle \ n. les applications où l&apos; espace d&apos; états \ scriptstyle \ e \ est fini ou dénombrable sont innombrables : on parle alors de chaîne de markov ou de chaînes de markov à espace d&apos; états discret . les propriétés essentielles des processus de markov généraux , par exemple les propriétés de récurrence et d&apos; ergodicité , s&apos; énoncent ou se démontrent plus simplement dans le cas des chaînes de markov à espace d&apos; états discret . cet article concerne précisément les chaînes de markov à espace d&apos; états discret . andreï markov a publié les premiers résultats sur les chaînes de markov à espace d&apos; états fini en 1906 . une généralisation à un espace d&apos; états infini dénombrable a été publiée par kolmogorov en 1936 . les processus de markov sont liés au mouvement brownien et à l&apos; hypothèse ergodique , deux sujets de physique statistique qui ont été très importants au début du xxe siècle . article détaillé : propriété de markov . \ mathbb { p } \ big ( x _ { n + 1 } = j &amp; \ mid \ , x _ 0 = i _ 0 , x _ 1 = i _ 1 , \ ldots , x _ { n-1 } = i _ { n-1 } , x _ n = i \ big ) = \ mathbb { p } \ left ( x _ { n + 1 } = j \ mid x _ n = i \ right ) . dans la suite de l&apos; article on ne considèrera que des chaînes de markov homogènes . pour une application intéressante des chaînes de markov non homogènes à l&apos; optimisation combinatoire , voir l&apos; article recuit simulé . il existe une propriété de markov forte , liée à la notion de temps d&apos; arrêt : cette propriété de markov forte est cruciale pour la démonstration de résultats importants ( divers critères de récurrence , loi forte des grands nombres pour les chaînes de markov ) . elle est énoncée dans l&apos; article « propriété de markov » . et supposons que la suite \ scriptstyle \ y \ est indépendante de \ scriptstyle \ x _ 0 . \ alors \ scriptstyle \ x \ est une chaîne de markov homogène. où les variables aléatoires \ scriptstyle \ y _ { n } \ sont des variables aléatoires indépendantes et uniformes sur \ scriptstyle \ &#91; \ ! &#91; 1,11 &#93; \ ! &#93; \ : ce sont les numéros successifs des vignettes tirées des tablettes de chocolat . le temps moyen nécessaire pour compléter la collection ( ici le nombre de tablettes que petit pierre doit acheter en moyenne pour compléter sa collec &apos; ) est , pour une collection de \ scriptstyle \ n \ vignettes au total , de \ scriptstyle \ n \ , h _ n , \ où \ scriptstyle \ h _ n \ est le \ scriptstyle \ n \ e nombre harmonique . par exemple , \ scriptstyle \ 11 \ , h _ { 11 } = 33,2 \ dots \ quad tablettes de chocolat. p _ { i , j } = \ mathbb { p } \ left ( x _ { 1 } = j \ mid x _ 0 = i \ right ) . la famille de nombres \ scriptstyle \ p = \ left ( p _ { i , j } \ right ) _ { ( i , j ) \ in e ^ 2 } est appelée matrice de transition , noyau de transition , ou opérateur de transition de la chaîne de markov . la terminologie matrice de transition est la plus utilisée , mais elle n&apos; est appropriée , en toute rigueur , que lorsque , pour un entier \ scriptstyle \ n \ ge 1 , \ scriptstyle \ e = \ { 1,2 , \ ldots , n \ } . lorsque \ scriptstyle \ e \ est fini , par exemple de cardinal \ scriptstyle \ n , on peut toujours numéroter les éléments de \ scriptstyle \ e \ arbitrairement de 1 à \ scriptstyle \ n , ce qui règle le problème , mais imparfaitement , car cette renumérotation est contre-intuitive dans beaucoup d&apos; exemples. x _ { i } = 0 \ text { ou } 1 \ text { selon que la puce n } ^ { \ circ } \ i \ text { est sur le 1er ou sur le 2eme chien . } alors \ scriptstyle \ e \ possède \ scriptstyle \ 2 ^ n \ éléments , mais les numéroter de 1 à \ scriptstyle \ 2 ^ n \ serait malcommode pour suivre l&apos; évolution du système , qui consiste à choisir une des \ scriptstyle \ n \ coordonnées de \ scriptstyle \ x \ au hasard et à changer sa valeur. puisque , par exemple , le nombre de puces sur le dos du chien n ° 1 passe de k à k-1 si c&apos; est une de ces k puces qui est choisie pour sauter , parmi les n puces présentes dans le « système » . ce modèle porte plus souvent le nom de « modèle des urnes d&apos; ehrenfest » . il a été introduit en 1907 par tatiana et paul ehrenfest pour illustrer certains des « paradoxes » apparus dans les fondements de la mécanique statistique naissante , et pour modéliser le bruit rose . le modèle des urnes d&apos; ehrenfest était considéré par le mathématicien mark kac comme « probablement l&apos; un des modèles les plus instructifs de toute la physique » . ( ab ) _ { i , j } = \ sum _ { k = 1 } ^ na _ { i , k } b _ { k , j } . proposition — la matrice de transition \ scriptstyle \ p = \ left ( p _ { i , j } \ right ) _ { ( i , j ) \ in e ^ 2 } \ est stochastique : la somme des termes de n&apos; importe quelle ligne de \ scriptstyle \ p \ donne toujours 1 . \ forall i \ in e , \ quad \ sum _ { j \ in e } p _ { i , j } = 1 . \ mathbb { p } \ left ( x _ { 0 } = i _ 0 , x _ { 1 } = i _ 1 , \ ldots , x _ { n-1 } = i _ { n-1 } , x _ { n } = i _ n \ right ) = \ mathbb { p } \ left ( x _ { 0 } = i _ 0 \ right ) \ p _ { i _ { 0 } , i _ 1 } \ , p _ { i _ { 1 } , i _ 2 } \ , \ ldots \ , p _ { i _ { n-2 } , i _ { n-1 } } \ , p _ { i _ { n-1 } , i _ n } . en vertu de la propriété de markov faible , donc si \ scriptstyle \ p _ { n-1 } \ a l&apos; expression attendue , alors \ scriptstyle \ p _ { n } \ aussi . p ^ k = \ left ( p ^ { ( k ) } _ { i , j } \ right ) _ { ( i , j ) \ in e ^ 2 } . pour conclure , on divise les deux termes extrêmes de cette suite d&apos; égalités par \ scriptstyle \ \ mathbb { p } \ left ( x _ n = i \ right ) , sauf si ce dernier terme est nul , auquel cas on peut définir \ scriptstyle \ \ mathbb { p } \ left ( x _ { n + k } = j \ mid x _ n = i \ right ) \ arbitrairement , donc , par exemple , égal à \ scriptstyle \ p ^ { ( k ) } _ { i , j } . par une simple application de la formule des probabilités totales , on en déduit les lois marginales de la chaîne de markov . \ mu _ { n + k } = \ mu _ { n } p ^ k \ quad \ text { et } \ quad \ mu _ { n } = \ mu _ { 0 } p ^ n. article détaillé : graphe d&apos; une chaîne de markov et classification des états . \ { j \ leftarrow i \ } \ quad \ leftrightarrow \ quad \ left \ { \ exists n \ ge 0 \ text { tel que } p ^ { ( n ) } _ { i , j } &gt; 0 \ right \ } . \ { j \ leftrightarrow i \ } \ quad \ leftrightarrow \ quad \ left \ { j \ leftarrow i \ text { et } i \ leftarrow j \ right \ } . la relation communiquer , notée \ scriptstyle \ \ leftrightarrow , \ est une relation d&apos; équivalence . quand on parle de classe en parlant des états d&apos; une chaîne de markov , c&apos; est général aux classes d&apos; équivalence pour la relation \ scriptstyle \ \ leftrightarrow \ qu&apos; on fait référence . si tous les états communiquent , la chaîne de markov est dite irréductible . \ { c \ leftarrow c ^ { \ prime } \ } \ quad \ leftrightarrow \ quad \ left \ { \ exists ( i , j ) \ in c \ times c ^ { \ prime } , \ qquad i \ leftarrow j \ right \ } \ quad \ leftrightarrow \ quad \ left \ { \ forall ( i , j ) \ in c \ times c ^ { \ prime } , \ qquad i \ leftarrow j \ right \ } . la relation \ scriptstyle \ \ leftarrow \ est une relation d&apos; ordre entre les classes d&apos; équivalence . une classe est dite finale si elle ne conduit à aucune autre , i.e. si la classe est minimale pour la relation \ scriptstyle \ \ leftarrow . \ sinon , elle est dite transitoire . l&apos; appartenance à une classe finale ou transitoire a des conséquences sur les propriétés probabilistes d&apos; un état de la chaîne de markov , en particulier sur son statut d&apos; état récurrent ou d&apos; état transient . le nombre et la nature des classes finales dicte la structure de l&apos; ensemble des probabilités stationnaires , qui résument de manière précise le comportement asymptotique de la chaîne de markov , comme on peut le voir à la prochaine section et dans les deux exemples détaillés à la fin de cette page . si deux états communiquent , ils ont la même période : on peut donc parler de la période d&apos; une classe d&apos; états . si la période vaut 1 , la classe est dite apériodique . l&apos; apériodicité des états d&apos; une chaîne de markov conditionne la convergence de la loi de \ scriptstyle \ x _ n \ vers la probabilité stationnaire , voir la page probabilité stationnaire d&apos; une chaîne de markov . la classification des états et leur période se lisent de manière simple sur le graphe de la chaîne de markov . toutefois , si tous les éléments de la matrice de transition sont strictement positifs , la chaîne de markov est irréductible et apériodique : dessiner le graphe de la chaîne de markov est alors superflu . article détaillé : probabilité stationnaire d&apos; une chaîne de markov . \ forall j \ in e , \ qquad \ pi _ j = \ sum _ { i \ in e } \ pi _ i \ , p _ { i , j } . \ sum _ { i \ in e } \ pi _ i \ = \ 1 . plus généralement , la chaîne de markov est un processus stationnaire si et seulement si sa loi initiale est une probabilité stationnaire . article détaillé : récurrence et transience d&apos; une chaîne de markov. un état est récurrent positif si l&apos; espérance du temps de premier retour en cet état , partant de cet état , est finie . si une chaîne de markov possède au moins un état récurrent positif , alors il existe une probabilité stationnaire . s&apos; il existe une probabilité stationnaire \ scriptstyle \ \ pi \ telle que \ scriptstyle \ \ pi _ i &gt; 0 \ , alors l&apos; état \ scriptstyle \ i \ est récurrent positif , et réciproquement . \ { \ pi _ i &gt; 0 \ } \ leftrightarrow \ { i \ in c \ } \ leftrightarrow \ { i \ text { est recurrent positif } \ } . \ scriptstyle \ c = e. \ = \ \ sum _ { i \ in e } f ( i ) \ , \ pi _ i \ = \ \ pi f. = \ \ sum _ { i \ in a } \ \ pi _ i \ = \ \ pi ( a ) . cela permet d&apos; approcher la probabilité stationnaire par la distribution empirique ( qui est un histogramme construit à partir d&apos; une séquence particulière ) , comme dans le cas de la marche aléatoire avec barrière. préserve la mesure , ce qui fait de la chaîne de markov un système dynamique mesuré . la loi forte des grands nombres entraine alors que la chaîne de markov est un système dynamique ergodique . l&apos; ergodicité est à la fois plus forte que la loi forte des grands nombres car on peut en déduire , par exemple , que \ scriptstyle \ \ frac { 1 } { n } \ ; \ sum _ { k = 0 } ^ { n-1 } f ( x _ k , x _ { k + 1 } ) , \ a pour limite presque sûre \ scriptstyle \ \ sum _ { i , j \ in e } f ( i , j ) \ , \ pi _ ip _ { i , j } , \ mais elle est aussi plus faible car elle réclame en principe la stationnarité de la chaîne de markov , ce qui n&apos; est pas le cas de la loi forte des grands nombres . si une chaîne de markov est irréductible et si son espace d&apos; états est fini , tous ses états sont récurrents positifs . la loi forte des grands nombres est alors en vigueur . plus généralement , tous les éléments d&apos; une classe finale finie sont récurrents positifs , que l&apos; espace d&apos; états soit fini ou bien infini dénombrable . dans les formules qui précèdent , l&apos; élément ( i , j ) est la probabilité de la transition de i à j. la somme des éléments d&apos; une ligne vaut toujours 1 et la distribution stationnaire est donnée par le vecteur propre gauche de la matrice de transition . on rencontre parfois des matrices de transition dans lesquelles le terme ( i , j ) est la probabilité de transition de j vers i , auquel cas la matrice de transition est simplement la transposée de celle décrite ici . la somme des éléments d&apos; une colonne vaut alors 1 . de plus , la distribution stationnaire du système est alors donnée par le vecteur propre droit de la matrice de transition , au lieu du vecteur propre gauche . doudou le hamster ne connaît que trois endroits dans sa cage : les copeaux où il dort , la mangeoire où il mange et la roue où il fait de l&apos; exercice . ses journées sont assez semblables les unes aux autres , et son activité se représente aisément par une chaîne de markov . toutes les minutes , il peut soit changer d&apos; activité , soit continuer celle qu&apos; il était en train de faire . l&apos; appellation processus sans mémoire n&apos; est pas du tout exagérée pour parler de doudou . quand il dort , il a 9 chances sur 10 de ne pas se réveiller la minute suivante . quand il se réveille , il y a 1 chance sur 2 qu&apos; il aille manger et 1 chance sur 2 qu&apos; il parte faire de l&apos; exercice . le repas ne dure qu&apos; une minute , après il fait autre chose . après avoir mangé , il y a 3 chances sur 10 qu&apos; il parte courir dans sa roue , mais surtout 7 chances sur 10 qu&apos; il retourne dormir . courir est fatigant pour doudou ; il y a 8 chances sur 10 qu&apos; il retourne dormir au bout d&apos; une minute . sinon il continue en oubliant qu&apos; il est déjà un peu fatigué. on ne dessine pas les boucles ( flèche d&apos; un état vers lui-même ) . cependant elles existent ; leur probabilité est sous-entendue car on sait que la somme des probabilités des flèches partant de chaque état doit être égale à 1 . prenons l&apos; hypothèse que doudou dort lors de la première minute de l&apos; étude . ainsi , après une minute , on a 90 % de chances que doudou dorme encore , 5 % qu&apos; il mange et 5 % qu&apos; il coure . après 2 minutes , il y a 4,5 % de chances que le hamster mange . l&apos; exemple qui suit a pour but de montrer l&apos; importance de la modélisation du système . une bonne modélisation permet de répondre à des questions complexes avec des calculs simples. haut fonctionnaire. les citoyens sont citoyens à vie et transmettent leur citoyenneté à leur lignée ( on pourrait croire que le nombre de citoyens tend à augmenter et qu&apos; au bout d&apos; un certain temps , tous sont citoyens mais historiquement , dans les civilisations qui suivaient ce schéma , les citoyens sont décimés par les guerres et de nouveaux esclaves arrivent régulièrement de l&apos; étranger ) . ils peuvent aussi se porter candidats lors des élections annuelles afin de devenir hauts-fonctionnaires ( magistrats ) . au terme de leur mandat , ils peuvent être réélus ou redevenir de simples citoyens . pour modéliser des élections qui ne seraient pas annuelles , il faudrait de même ajouter des états fictifs ( année d&apos; élection , un an depuis la dernière élection , etc. ) . où p _ i ^ { n } est la probabilité d&apos; être dans la classe sociale i au bout de n années sachant que la lignée étudiée est partie de l&apos; état d&apos; esclave . on obtient ainsi la répartition de la population dans les différentes classes sociales ( au bout de n années ) . en multipliant ce vecteur y par l&apos; effectif total de la population , on obtient les effectifs de chaque classe au bout de n années . on ajoute un sommet absorbant car une fois qu&apos; une lignée a fini un mandat , on ne tient plus compte d&apos; elle . si certains lecteurs font preuve d&apos; esprit critique , ils diront peut-être que le modèle est faux car les lignées comportant un élu ne participent plus aux élections . il n&apos; en est rien . en effet , le nombre d&apos; élus est proportionnel au nombre de citoyens . ne pas réinjecter les anciens hauts-fonctionnaires parmi les candidats ne change donc en rien la probabilité pour un citoyen d&apos; être élu car , la population des citoyens étant plus restreinte , le nombre de postes offerts l&apos; est aussi . ce modèle permet de répondre avec exactitude à la question posée . en faisant les mêmes calculs qu&apos; aux questions précédentes on obtient en dernière ligne du vecteur solution le pourcentage de lignées ayant accompli au moins un mandat ou bien l&apos; effectif ( si on multiplie par l&apos; effectif total de la population ) . autrement dit , modéliser à nouveau le problème permet de répondre à la question qui semblait si compliquée par un simple calcul de puissances d&apos; une matrice . les systèmes markoviens sont très présents en physique particulièrement en physique statistique . plus généralement l&apos; hypothèse markovienne est souvent invoquée lorsque des probabilités sont utilisées pour modéliser l&apos; état d&apos; un système , en supposant toutefois que l&apos; état futur du système peut être déduit du passé avec un historique assez faible . le célèbre article de 1948 de claude shannon , a mathematical theory of communication , qui fonde la théorie de l&apos; information , commence en introduisant la notion d&apos; entropie à partir d&apos; une modélisation markovienne de la langue anglaise . il montre ainsi le degré de prédictibilité de la langue anglaise , muni d&apos; un simple modèle d&apos; ordre 1 . bien que simples , de tels modèles permettent de bien représenter les propriétés statistiques des systèmes et de réaliser des prédictions efficaces sans décrire la structure complète des systèmes . en compression , la modélisation markovienne permet la réalisation de techniques de codage entropique très efficaces , comme le codage arithmétique . de très nombreux algorithmes en reconnaissance des formes ou en intelligence artificielle comme l&apos; algorithme de viterbi , utilisé dans la grande majorité des systèmes de téléphonie mobile pour la correction d&apos; erreurs , font l&apos; hypothèse d&apos; un processus markovien sous-jacent . l&apos; indice de popularité d&apos; une page web ( pagerank ) tel qu&apos; il est utilisé par google est défini par une chaîne de markov . il est défini par la probabilité d&apos; être dans cette page à partir d&apos; un état quelconque de la chaine de markov représentant le web . si n est le nombre de pages web connues , et une page i a k _ i liens , alors sa probabilité de transition vers une page liée ( vers laquelle elle pointe ) est p _ i ^ l = \ tfrac { 1-q } { k _ i } + \ tfrac qn et p _ i ^ { nl } = \ tfrac qn pour toutes les autres ( pages non liées ) . notons qu&apos; on a bien k _ i p _ i ^ l + ( n-k _ i ) p _ i ^ { nl } = 1 . le paramètre q vaut environ 0,15 . les chaînes de markov sont un outil fondamental pour modéliser les processus en théorie des files d&apos; attente et en statistiques . les chaînes de markov sont couramment employées en sûreté de fonctionnement pour les calculs de fiabilité et de disponibilité des systèmes techniques , en particulier pour modéliser des séquences d&apos; évènements de type pannes , réparations , changements de configuration . les chaînes de markov sont également utilisées en bioinformatique pour modéliser les relations entre symboles successifs d&apos; une même séquence ( de nucléotides par exemple ) , en allant au-delà du modèle polynomial . les modèles markoviens cachés ont également diverses utilisations , telles que la segmentation ( définition de frontières de régions au sein de séquences de gènes ou de protéines dont les propriétés chimiques varient ) , l&apos; alignement multiple , la prédiction de fonction , ou la découverte de gènes ( les modèles markoviens cachés sont plus « flexibles » que les définitions strictes de type codon start + multiples codons + codons stop et ils sont donc plus adaptés pour les eucaryotes ( à cause de la présence d&apos; introns dans le génome de ceux-ci ) ou pour la découverte de pseudo-gènes ) . bruno sericola : chaînes de markov - théorie , algorithmes et applications . hermes / lavoisier , 2013 .
