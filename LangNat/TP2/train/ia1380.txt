vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants . la méthode de l&apos; entropie-croisée ( ce ) attribuée à reuven rubinstein est une méthode générale d&apos; optimisation de type monte-carlo , combinatoire ou continue , et d&apos; échantillonnage préférentiel . la méthode a été conçue à l&apos; origine pour la simulation d&apos; événements rares , où des densités de probabilité très faibles doivent être estimées correctement , par exemple dans l&apos; analyse de la sécurité des réseaux , les modèles de file d&apos; attente , ou l&apos; analyse des performances des systèmes de télécommunication . la méthode ce peut être appliquée à tout problème d&apos; optimisation combinatoire où les observations sont bruitées comme le problème du voyageur de commerce , l&apos; optimisation quadratique , le problème d&apos; alignement de séquences d&apos; adn , le problème de la coupure maximale et les problèmes d&apos; allocation de mémoire , tout comme des problèmes d&apos; optimisation continue avec de nombreux extrema locaux . créer aléatoirement un échantillon de données ( trajectoires , vecteurs , etc. ) selon un mécanisme spécifique . mettre à jour les paramètres du mécanisme de création aléatoire à partir de l&apos; échantillon de données pour produire un meilleur échantillon à l&apos; itération suivante . cette étape implique de minimiser l&apos; entropie croisée ou la divergence de kullback-leibler. g ^ * ( \ mathbf { x } ) = h ( \ mathbf { x } ) f ( \ mathbf { x } ; \ mathbf { u } ) / \ ell . cependant \ ell est un paramètre inconnu . la méthode ce propose d&apos; approcher la densité optimale en sélectionnant les éléments de l&apos; échantillon qui sont les plus proches ( au sens de kullback-leibler ) de la densité optimale g ^ * . choisir un vecteur des paramètres initial \ mathbf { v } ^ { ( 0 ) } ; poser t = 1 . si l&apos; algorithme a convergé alors stopper ; sinon , incrémenter t de 1 recommencer à l&apos; étape 2 . quand h ( \ mathbf { x } ) = \ mathrm { i } _ { \ { \ mathbf { x } \ in a \ } } et f ( \ mathbf { x } _ i ; \ mathbf { u } ) = f ( \ mathbf { x } _ i ; \ mathbf { v } ^ { ( t-1 ) } ) , alors \ mathbf { v } ^ { ( t ) } correspond à l&apos; estimateur du maximum de vraisemblance basé sur les \ mathbf { x } _ k \ in a. le même algorithme ce peut être utilisé pour l&apos; optimisation et l&apos; estimation . s ( x ) = \ textrm { e } ^ { - ( x-2 ) ^ 2 } + 0.8 \ , \ textrm { e } ^ { - ( x + 2 ) ^ 2 } . dont les paramètres sont la moyenne \ mu _ t \ , et la variance \ sigma _ t ^ 2 ( tel que \ boldsymbol { \ theta } = ( \ mu , \ sigma ^ 2 ) ici ) . soit minimisée . ce qui est fait en utilisant la version échantillonnée ( contrepartie stochastique ) du problème de la minimisation de la divergence kl , comme précédemment dans l&apos; étape 3 . il se trouve que pour ce choix de distribution les paramètres qui minimisent la version stochastique du problème sont la moyenne et la variance empirique de l&apos; échantillon d&apos; élite qui est composé des tirages dont la valeur de la fonction score est \ geq \ gamma . le plus mauvais des éléments de l&apos; échantillon d&apos; élite sert de paramètre de niveau à l&apos; itération suivante . cela donne l&apos; algorithme stochastique suivant pour ce problème . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « cross-entropy method » ( voir la liste des auteurs ) . de boer , p-t . , kroese , d.p , mannor , s. and rubinstein , r.y. ( 2005 ) . a tutorial on the cross-entropy method . annals of operations research , 134 ( 1 ) , 19 — 67 . rubinstein , r.y. ( 1997 ) . optimization of computer simulation models with rare events , european journal of operations research , 99 , 89-112 . rubinstein , r.y. , kroese , d.p. ( 2004 ) . the cross-entropy method : a unified approach to combinatorial optimization , monte-carlo simulation , and machine learning , springer-verlag , new york .
