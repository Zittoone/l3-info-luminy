les algorithmes d ’ optimisation cherchent à déterminer le jeu de paramètres d ’ entrée d ’ une fonction donnant à cette fonction la valeur maximale ou minimale . on cherchera par exemple la découpe optimale d ’ une tôle pour en fabriquer le plus grand nombre de boîtes de conserve possible ( ou d ’ un tissu pour en faire le plus grand nombre de chemises possibles , etc. ) . cette optimisation peut se faire sans contrainte ou sous contrainte , le second cas se ramenant au premier dans le cas des fonctions dérivables par la méthode du multiplicateur de lagrange ( et des fonctions non-dérivables par l ’ algorithme d ’ everett ) . le problème est insoluble en tant que tel si l ’ on ne connaît rien de la fonction ( il existe peut-être une combinaison très particulière de valeurs d ’ entrées lui donnant ponctuellement une valeur extrêmement haute ou basse , qui pourrait échapper à l ’ algorithme . aussi existe-t-il plusieurs classes d ’ algorithmes liés aux différentes connaissances qu ’ on peut avoir sur la fonction . si celle-ci est dérivable , l ’ une des plus performantes est celle du gradient conjugué . aucune méthode connue en 2004 ( à part l ’ énumération exhaustive ou l ’ analyse algébrique ) ne permet de trouver avec certitude un extremum global d ’ une fonction . les extrema déterminables sont toujours locaux à un domaine , et demandent souvent même en ce cas quelques caractéristiques à la fonction , par exemple dans certains cas la continuité . les métaheuristiques sont une classe d ’ algorithmes d ’ optimisation qui tentent d ’ obtenir une valeur approchée de l ’ optimum global dans le cas de problèmes d ’ optimisation difficile . elles ne donnent cependant aucune garantie sur la fiabilité du résultat . soit a l ’ algorithme du problème d ’ optimisation associé au problème de décision p. opt ( i ) est la solution optimale pour l ’ instance i du problème p. cout ( i ) est la valeur k &apos; de la solution j. a : i ( p ) \ rightarrow s : i \ rightarrow j \ in s ( i ) , tel que c _ p ( i , j ) = oui .
