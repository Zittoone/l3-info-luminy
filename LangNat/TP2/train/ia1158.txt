la classification naïve bayésienne est un type de classification bayésienne probabiliste simple basée sur le théorème de bayes avec une forte indépendance ( dite naïve ) des hypothèses . elle met en œuvre un classifieur bayésien naïf , ou classifieur naïf de bayes , appartenant à la famille des classifieurs linéaires . un terme plus approprié pour le modèle probabiliste sous-jacent pourrait être « modèle à caractéristiques statistiquement indépendantes » . en termes simples , un classifieur bayésien naïf suppose que l&apos; existence d&apos; une caractéristique pour une classe , est indépendante de l&apos; existence d&apos; autres caractéristiques . un fruit peut être considéré comme une pomme s&apos; il est rouge , arrondi , et fait une dizaine de centimètres . même si ces caractéristiques sont liées dans la réalité , un classifieur bayésien naïf déterminera que le fruit est une pomme en considérant indépendamment ces caractéristiques de couleur , de forme et de taille . selon la nature de chaque modèle probabiliste , les classifieurs bayésiens naïfs peuvent être entraînés efficacement dans un contexte d&apos; apprentissage supervisé . dans beaucoup d&apos; applications pratiques , l&apos; estimation des paramètres pour les modèles bayésiens naïfs repose sur le maximum de vraisemblance . autrement dit , il est possible de travailler avec le modèle bayésien naïf sans se préoccuper de probabilité bayésienne ou utiliser les méthodes bayésiennes . malgré leur modèle de conception « naïf » et ses hypothèses de base extrêmement simplistes , les classifieurs bayésiens naïfs ont fait preuve d&apos; une efficacité plus que suffisante dans beaucoup de situations réelles complexes . en 2004 , un article a montré qu&apos; il existe des raisons théoriques derrière cette efficacité inattendueharry zhang &quot; the optimality of naive bayes &quot; . conférence flairs2004 . ( disponible en ligne en anglais : pdf ) . toutefois , une autre étude de 2006 montre que des approches plus récentes ( arbres renforcés , forêts aléatoires ) permettent d&apos; obtenir de meilleurs résultatscaruana , r. and niculescu-mizil , a. : &quot; an empirical comparison of supervised learning algorithms &quot; . proceedings of the 23rd international conference on machine learning , 2006 . ( disponible en ligne en anglais pdf ) . l&apos; avantage du classifieur bayésien naïf est qu&apos; il requiert relativement peu de données d&apos; entraînement pour estimer les paramètres nécessaires à la classification , à savoir moyennes et variances des différentes variables . en effet , l&apos; hypothèse d&apos; indépendance des variables permet de se contenter de la variance de chacune d&apos; entre elle pour chaque classe , sans avoir à calculer de matrice de covariance. où c est une variable de classe dépendante dont les instances ou classes sont peu nombreuses , conditionnée par plusieurs variables caractéristiques f _ 1 , \ dots , f _ n. lorsque le nombre de caractéristiques n est grand , ou lorsque ces caractéristiques peuvent prendre un grand nombre de valeurs , baser ce modèle sur des tableaux de probabilités devient impossible . par conséquent , nous le dérivons pour qu&apos; il soit plus facilement soluble . en pratique , seul le numérateur nous intéresse , puisque le dénominateur ne dépend pas de c et les valeurs des caractéristiques f _ i sont données . le dénominateur est donc en réalité constant . = p ( c ) \ p ( f _ 1 \ vert c ) \ p ( f _ 2 \ vert c , f _ 1 ) \ p ( f _ 3 \ vert c , f _ 1 , f _ 2 ) \ \ cdots \ p ( f _ n \ vert c , f _ 1 , f _ 2 , f _ 3 , \ dots , f _ { n-1 } ) . p ( f _ 1 , \ dots , f _ n \ vert c ) = p ( f _ 1 \ vert c ) \ p ( f _ 2 \ vert c ) \ p ( f _ 3 \ vert c ) \ \ cdots \ p ( f _ n \ vert c ) = \ prod _ { i = 1 } ^ n p ( f _ i \ vert c ) . où z ( appelé « évidence » ) est un facteur d&apos; échelle qui dépend uniquement de f _ 1 , \ dots , f _ n , à savoir une constante dans la mesure où les valeurs des variables caractéristiques sont connues . les modèles probabilistes ainsi décrits sont plus faciles à manipuler , puisqu&apos; ils peuvent être factorisés par l&apos; antérieure p ( c ) ( probabilité a priori de c ) et les lois de probabilité indépendantes p ( f _ i \ vert c ) . s&apos; il existe k classes pour c et si le modèle pour chaque fonction p ( f _ i \ vert c = c ) peut être exprimé selon r paramètres , alors le modèle bayésien naïf correspondant dépend de ( k − 1 ) + n r k paramètres . dans la pratique , on observe souvent des modèles où k = 2 ( classification binaire ) et r = 1 ( les caractéristiques sont alors des variables de bernoulli ) . dans ce cas , le nombre total de paramètres du modèle bayésien naïf ainsi décrit est de 2n + 1 , avec n le nombre de caractéristiques binaires utilisées pour la classification . tous les paramètres du modèle ( probabilités a priori des classes et lois de probabilités associées aux différentes caractéristiques ) peuvent faire l&apos; objet d&apos; une approximation par rapport aux fréquences relatives des classes et caractéristiques dans l&apos; ensemble des données d&apos; entraînement . il s&apos; agit d&apos; une estimation du maximum de vraisemblance des probabilités . les probabilités a priori des classes peuvent par exemple être calculées en se basant sur l&apos; hypothèse que les classes sont équiprobables ( i.e chaque antérieure = 1 / ( nombre de classes ) ) , ou bien en estimant chaque probabilité de classe sur la base de l&apos; ensemble des données d&apos; entraînement ( i.e antérieure de c = ( nombre d&apos; échantillons de c ) / ( nombre d&apos; échantillons total ) ) . pour estimer les paramètres d&apos; une loi de probabilité relative à une caractéristique précise , il est nécessaire de présupposer le type de la loi en question ; sinon , il faut générer des modèles non-paramétriques pour les caractéristiques appartenant à l&apos; ensemble de données d&apos; entraînementgeorge h. john and pat langley ( 1995 ) . estimating continuous distributions in bayesian classifiers . proceedings of the eleventh conference on uncertainty in artificial intelligence. pp. 338-345 . morgan kaufmann , san mateo . . lorsque l&apos; on travaille avec des caractéristiques qui sont des variables aléatoires continues , on suppose généralement que les lois de probabilités correspondantes sont des lois normales , dont on estimera l&apos; espérance et la variance. où n est le nombre d&apos; échantillons et x _ i est la valeur d&apos; un échantillon donné . \ sigma ^ 2 = \ frac { 1 } { ( n-1 ) } \ sum _ { i = 1 } ^ n \ left ( x _ i - \ mu \ right ) ^ 2 \ , . si , pour une certaine classe , une certaine caractéristique ne prend jamais une valeur donnée dans l&apos; ensemble de données d&apos; entraînement , alors l&apos; estimation de probabilité basée sur la fréquence aura pour valeur zéro . cela pose un problème puisque l&apos; on aboutit à l&apos; apparition d&apos; un facteur nul lorsque les probabilités sont multipliées . par conséquent , on corrige les estimations de probabilités avec des probabilités fixées à l&apos; avance . \ mathrm { classifieur } ( f _ 1 , \ dots , f _ n ) = \ underset { c } { \ operatorname { argmax } } \ p ( c = c ) \ displaystyle \ prod _ { i = 1 } ^ n p ( f _ i = f _ i \ vert c = c ) . fait étonnant , malgré les hypothèses d&apos; indépendance relativement simplistes , le classifieur bayésien naïf a plusieurs propriétés qui le rendent très pratique dans les cas réels . en particulier , la dissociation des lois de probabilités conditionnelles de classe entre les différentes caractéristiques aboutit au fait que chaque loi de probabilité peut être estimée indépendamment en tant que loi de probabilité à une dimension . cela permet d&apos; éviter nombre de problèmes venant du fléau de la dimension , par exemple le besoin de disposer d&apos; ensembles de données d&apos; entraînement dont la quantité augmente exponentiellement avec le nombre de caractéristiques . comme tous les classifieurs probabilistes utilisant la règle de décision du maximum a posteriori , il classifie correctement du moment que la classe adéquate est plus probable que toutes les autres . par conséquent les probabilités de classe n&apos; ont pas à être estimées de façon très précises . le classifieur dans l&apos; ensemble est suffisamment robuste pour ne pas tenir compte de sérieux défauts dans son modèle de base de probabilités naïves . la documentation citée en fin d&apos; article détaille d&apos; autres raisons pour le succès empirique des classifieurs bayésiens naïfs . problème : classifier chaque personne en tant qu&apos; individu du sexe masculin ou féminin , selon les caractéristiques mesurées . les caractéristiques comprennent la taille , le poids , et la pointure . on suppose pour des raisons pratiques que les classes sont équiprobables , à savoir p ( masculin ) = p ( féminin ) = 0.5 ( selon le contexte , cette hypothèse peut être inappropriée ) . si l&apos; on détermine p ( c ) d&apos; après la fréquence des échantillons par classe dans l&apos; ensemble de données d&apos; entraînement , on aboutit au même résultat . nous souhaitons déterminer quelle probabilité postérieure est la plus grande , celle que l&apos; échantillon soit de sexe masculin , ou celle qu&apos; il soit de sexe féminin . le terme évidence ( également appelé constante de normalisation ) peut être calculé car la somme des postérieures vaut 1 . comme la postérieure féminin est supérieure à la postérieure masculin , l&apos; échantillon est plus probablement de sexe féminin . le théorème de bayes nous permet d&apos; inférer la probabilité en termes de vraisemblance . ( cette technique de rapport de vraisemblance est une technique courante en statistiques . le document peut donc être classifié comme suit : il s&apos; agit de spam si p ( s \ vert d ) &gt; p ( \ neg s \ vert d ) ( i.e. si \ ln { p ( s \ vert d ) \ over p ( \ neg s \ vert d ) } &gt; 0 ) , sinon il s&apos; agit de courrier normal . hand , dj , &amp; yu , k. ( 2001 ) . &quot; idiot&apos; s bayes - not so stupid after all ? &quot; international statistical review . vol 69 part 3 , pages 385-399 . issn 0306-7734 . webb , g. i. , j. boughton , and z. wang ( 2005 ) . not so naive bayes : aggregating one-dependence estimators . machine learning 58 ( 1 ) . netherlands : springer , pages 5-24 . minsky , m. ( 1961 ) . &quot; steps toward artificial intelligence . &quot; proceedings of the ire 49 ( 1 ) : 8-30 . imsl collection d&apos; algorithmes mathématiques et statistiques en c / c + + , fortran , java and c # / .net. les routines de data mining dans les imsl comportent un classifieur bayésien naïf . winnow content recommendation classifieur bayésien naïf open source fonctionnant avec de petits ensembles de données d&apos; entraînement . haute performance , c , unix .
