le théorème de cox-jaynes ( 1946 ) codifie et quantifie la démarche d&apos; apprentissage en se fondant sur cinq postulats ( desiderata ) simples . cette codification coïncide avec celle de probabilité , historiquement d&apos; origine toute différente . le théorème doit son nom au physicien richard threlkeld cox qui en a formulé la version originale . cox formalise la notion intuitive de plausibilité sous une forme numérique . il démontre que , si les plausibilités satisfont à un ensemble d&apos; hypothèses , la seule façon cohérente de les manipuler est d&apos; utiliser un système isomorphe à la théorie des probabilités . ce système induit une interprétation « logique » des probabilités indépendante de celle de fréquence . elle fournit une base rationnelle au mécanisme d&apos; induction logique , et donc à l&apos; apprentissage automatique . qui plus est , le théorème , sous les conditions imposées par les postulats , implique que toute autre forme de prise en compte des informations dans le cadre de cette représentation particulière de la connaissance serait en fait biaisée . il s&apos; agit donc d&apos; un résultat extrêmement forttemplate : harvsp , template : harvsp . . les résultats de cox n&apos; avaient touché qu&apos; une audience restreinte avant qu&apos; edwin thompson jaynes ne redécouvrît ce théorème et n&apos; en défrichât une série d&apos; implications pour les méthodes bayésiennes &#91; réf. insuffisante &#93; . irving john good en explora les conséquences dans le domaine de l&apos; intelligence artificielletemplate : article . . stanislas dehaene utilise le théorème , sa construction et ses applications dans le cadre de l&apos; étude des processus cognitifs humainscours au collège de france ( 2011-2012 ) , 10 janvier 2012 : introduction au raisonnement bayésien et à ses applications . , suivant en cela une idée déjà énoncée en 1988 par jaynestemplate : harvsp . article détaillé : paradoxe de hempel , dit de l&apos; ornithologie en chambre . cox pose cinq desiderata pour un robot qui raisonnerait selon une logique inductive. on désire un calculateur de situations général , non destiné à un usage particulier. pas de rétention d&apos; informationtemplate : harv . . il faut pouvoir à tout moment dire de deux plausibilités laquelle est plus grande que l&apos; autre . cette relation d&apos; ordre suggère une représentation quantitative , et la forme numérique semble commode . une représentation sous forme de nombres entiers poserait un problème , aucune plausibilité ne pouvant se glisser entre deux représentées par des entiers successifs . il faut donc un ensemble continu . des rationnels conviendraient . a fortiori , les nombres réels conviennent . la convention adoptée , arbitrairement , est que des plausibilités plus grandes seront représentées par des nombres plus grands . ce qui nous paraît évident ne doit pas être contredit par le modèle . cette règle simple en apparence n&apos; est pas toujours facile à appliquer dans le cas de préférences collectives , comme le montrent le paradoxe de condorcet et le théorème d&apos; impossibilité d&apos; arrow ) . si une conclusion peut être obtenue par plus d&apos; un moyen , alors tous ces moyens doivent bien donner le même résultat . cette règle élimine du champ d&apos; examen les « heuristiques multiples » dès lors qu&apos; elles pourraient contenir entre elles des contradictions ( comme le font par exemple parfois les critères de savage et de wald , se réclamant tous deux du minimax de la théorie des jeux ) . le robot doit toujours prendre en compte la totalité de l&apos; information qui lui est fournie . il ne doit pas en ignorer délibérément une partie et fonder ses conclusions sur le reste . en d&apos; autres termes , le robot doit être totalement non idéologique , neutre de point de vue . le robot représente des états de connaissance équivalents par des plausibilités équivalentes . si deux problèmes sont identiques à un simple étiquetage de propositions près , le robot doit affecter les mêmes plausibilités aux deux cas . deux propositions doivent donc être considérées a priori comme de plausibilité équivalente quand elles ne se distinguent que par leur nom , ce qui n&apos; arrive guère que dans des cas très particuliers , comme pour des pièces ou des dés non pipés . sans rentrer dans les équations , l&apos; idée est que lorsque deux plausibilités du même état se composent , la plausibilité composée est nécessairement égale ou supérieure à la plus grande des deux . il s&apos; agit ici du cas inverse : quand deux plausibilités doivent toutes deux être vérifiées pour qu&apos; un état puisse exister , cet état ne peut avoir de plausibilité plus grande que la plus petite des deux précédentes . good a proposé une notation permettant de manipuler plus aisément les plausibilités . alan turing avait fait remarquer en son temps que l&apos; expression des probabilités était beaucoup plus facile à manier en remplaçant une probabilité p variant de 0 à 1 par l&apos; expression ln ( p / ( 1-p ) ) permettant une meilleure discrimination des très petites valeurs ( très proches de 0 ) comme des très grandes ( très proches de 1 ) . en particulier , sous cette forme , un apport d&apos; information par la règle de bayes se traduit par l&apos; ajout d&apos; une quantité algébrique unique à cette expression ( que turing nommait log-odd ) , cela quelle que soit la probabilité a priori de départ avant l&apos; observation &#91; réf. nécessaire &#93; catégorie : article à référence nécessaire . la notation de good utilise , conformément à cette idée , une échelle logarithmique. adopta un facteur 10 afin d&apos; éviter la complication de manier des quantités décimales , là où une précision de 25 % suffisait . il nomma la mesure correspondante , w = 10 log10 ( p / ( 1-p ) ) , weight of evidence parce qu&apos; elle permettait de « peser » le témoignage des faits en fonction des attentes - manifestées par des probabilités « subjectives » antérieures à l&apos; observation - de façon indépendante de ces attentes &#91; réf. insuffisante &#93; . dehaene préfère pour éviter toute connotation parasite parler plutôt de décibans que de décibelstemplate : lien web . . les évidences sont parfois exprimées aussi en bits , en particulier dans les tests de validité de lois scalantes . en effet , quand une loi comme la loi de zipf ou de mandelbrot s&apos; ajuste mieux aux données qu&apos; une autre loi ne nécessitant pas de tri préalable , il faut tenir compte du fait que trier une séquence de n termes sélectionne une permutation parmi n ! possibles . le tri représente un apport d&apos; information ( ou dordre ) de l&apos; ordre de n log2 n. cet apport d&apos; information pourrait suffire au meilleur ajustement . on peut s&apos; attendre à voir une répartition décroissante rendre mieux compte de ce qu&apos; on vient de trier soi-même en ordre décroissant . si le gain d&apos; évidence qu&apos; apporte le tri représente moins de bits que celui qu&apos; a coûté le tri , l&apos; information apportée par la considération d&apos; une loi scalante est nulle . l&apos; ordre apporté est simplement celui que nous venons de mettre : le modèle ne doit donc pas dans ce cas être conservé . dans d&apos; autres , sa validité ressort nettement : voir loi de zipf-mandelbrottemplate : ouvrage template : refins passage ? . . on remarque que l&apos; algèbre de boole est isomorphe à la théorie des probabilités réduite aux seules valeurs 0 et 1 . cette considération conduisit à l&apos; invention dans les années 1970 des calculateurs stochastiques promus par la société alsthom ( qui s&apos; écrivait avec un h à l&apos; époque ) et qui entendaient combiner le faible coût des circuits de commutation avec la puissance de traitement des calculateurs analogiques . quelques-uns furent réalisés à l&apos; époque . myron tribus propose de considérer la probabilité comme la simple traduction numérique d&apos; un état de connaissance et non comme le passage à la limite de la notion de fréquence . à l&apos; appui , il prend l&apos; image classique du dé dont la probabilité de sortie de chaque face est considérée au départ de 1 / 6e même si le dé est en glace , donc ne peut être lancé plus d&apos; un petit nombre de fois , ce qui interdit tout passage à la limite . il imagine alors l&apos; objection d&apos; un interlocuteur : &quot; si je me représente mentalement mille dés , je peux bel et bien envisager un passage à la limite &quot; , à laquelle il répond : &quot; tout à fait . et donc si vous vous les représentez simplement mentalement , c&apos; est qu&apos; il s&apos; agit bien d&apos; un état de connaissance &#91; réf. insuffisante &#93; . les divergences entre approches fréquentistes et bayésiennes ont suscité beaucoup de passions dans les années 1970 , où elles prenaient alors presque l&apos; aspect d&apos; une &quot; guerre de religion &quot; . leur coexistence &quot; pacifique &quot; est aujourd&apos; hui admise , chacune ayant son domaine d&apos; efficacité maximale et les deux approches convergeant de toute façon lorsqu&apos; on passe aux grands nombres d&apos; observationstemplate : article ( il n&apos; y a pas de conflit pour les petits nombres , les méthodes fréquentistes ( statistiques ) ne concernant pas ce domaine d&apos; application ) . edwin thompson jaynes , dans sa reprise et son approfondissement du théorème de cox , utilise celui-ci pour montrer que tout apprentissage y compris automatique devra nécessairement soit utiliser l&apos; inférence bayésienne ( à un homomorphisme près si on le désire , comme un passage par une transformation logarithme simplifiant les calculs pratiques ) , soit donner quelque part des résultats incohérents et être , en conséquence , inadapté . ce résultat extrêmement fort , nécessite l&apos; acceptation de cinq desiderata simples , dont celui de la continuité de méthode ( ne pas changer brusquement d&apos; algorithme simplement parce qu&apos; une donnée est modifiée de façon infinitésimale ) template : harvsp &#91; réf. insuffisante &#93; . voir également l&apos; article logit . les approches sont différentes : la logique dite floue est d&apos; origine pragmatique ( un exemple de &quot; logique floue &quot; est le classement d&apos; élèves à un examen général par emploi de coefficients arbitraires pour chaque matière ) et sans véritables théorèmes : il s&apos; agit d&apos; une simple technique . l&apos; apprentissage bayésien relève d&apos; une théorie solide fondée sur un édifice mathématique et des notions quantitatives , comme la maximisation d&apos; entropie ( maxent ) . il est vrai que les deux approches ont fini par converger ( détection automatique des scènes pour les appareils photo numériques , reconnaissance de voix et de caractères ) , mais uniquement parce que les approches bayésiennes ont largement phagocyté le reste . chaque discipline possède ses mesures favorites : si la thermique s&apos; occupe principalement de températures , la thermodynamique sera plus attachée à des mesures de quantité de chaleur , voire d&apos; entropie . l&apos; électrostatique s&apos; intéresse plus aux tensions qu&apos; aux intensités , tandis que c&apos; est l&apos; inverse pour les courants faibles , et qu&apos; en électrotechnique c&apos; est davantage en termes de puissance qu&apos; on aura tendance à raisonner . selon sa discipline d&apos; origine , chaque expérimentateur tendra donc à effectuer ses estimations sur les unités auxquelles il est habitué &#91; réf. souhaitée &#93; catégorie : article à référence souhaitée . dans le cas d&apos; un montage électrique , un spécialiste d&apos; électrotechnique fera peut-être une estimation de puissance dissipée ( ri ² ) tandis qu&apos; un spécialiste des courants faibles préférera estimer l&apos; intensité elle-même ( i ) . si la convergence à terme des estimations est assurée dans les deux cas , elle ne se fera pas de la même façon , même avec des distributions a priori identiques , car l&apos; espérance mathématique d&apos; un carré n&apos; est pas mathématiquement liée au carré d&apos; une espérance . il s&apos; agit là de la principale pierre d&apos; achoppement des méthodes bayésiennes &#91; réf. insuffisante &#93; . indépendamment des probabilités a priori que nous attribuons aux événements , nos estimations sont également en partie « formatées » par le langage et la « déformation professionnelle » qui s&apos; y attachent . concrètement , cela rappelle qu&apos; il n&apos; existe pas seulement une , mais deux sources d&apos; arbitraire dans les méthodes bayésiennes : celle , de mesure , qui entache les probabilités a priori choisiesle choix d&apos; une distribution d&apos; entropie maximale ( « maxent » ) parmi celles qui satisfont aux contraintes garantit que l&apos; on choisit la moins arbitraire . loi normale de gauss , loi exponentielle décroissante , loi de zipf-mandelbrot sont respectivement d&apos; entropie maximale quand on fixe moyenne et écart-type , moyenne seule ou rang moyen seul et celle , de méthode , qui correspond à notre représentation du problème . en revanche , l&apos; arbitraire se limite à ces deux éléments , et les méthodes bayésiennes sont ensuite totalement impersonnelles . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « cox&apos; s theorem » ( voir la liste des auteurs ) . ( en ) r. t. cox , &quot; probability , frequency , and reasonable expectation , am . jour . phys . , 14 , 1 – 13 , ( 1946 ) . ( en ) r. t. cox , the algebra of probable inference &apos; , johns hopkins university press , baltimore , md , ( 1961 ) . myron tribus ( trad . jacques pézier ) , décisions rationnelles dans l&apos; incertain &#91; « rational descriptions , decisions and designs » &#93; , paris , masson , &lt; time &gt; 1974 &lt; / time &gt; ( 1re éd. 1969 ) . ( de ) niels henrik abel , « untersuchung der functionen zweier unabhängig veränderlichen gröszen x und y , wie f ( x , y ) , welche die eigenschaft haben , dasz f &#91; z , f ( x , y ) &#93; eine symmetrische function von z , x und y ist » , j. reine angew . math . , vol. 1 , ‎ &lt; time &gt; 1826 &lt; / time &gt; , p. 11-15 . ( en ) janos aczél ( en ) catégorie : article contenant un appel à traduction en anglais , lectures on functional equations and their applications , academic press , new york , 1966 . ( en ) terrence l. fine , theories of probability : an examination of foundations , academic press , new york , 1973 . ( en ) myron tribus , rational descriptions , decisions and designs , pergamon press , &lt; time &gt; 1969 &lt; / time &gt; , 479 p. ( en ) kevin s. van horn , « constructing a logic of plausible inference : a guide to cox ’ s theorem » , international journal of approximate reasoning , vol. 34 , no 1 , 2003 , p. 3-24 , doi : 10.1016 / s0888-613x ( 03 ) 00051-3 . ( en ) some key references on the cox proof of bayesianism liste bibliographique plus détaillée , et annotée . p. bessière et al. , fondements mathématiques de l&apos; approche f + d &#91; pdf &#93; : détails sur le théorème de cox , implications et application . université de valenciennes , e-diagnostic : cours de sensibilisation aux techniques de diagnostic ; méthodes déductives , inductives et abductives .
