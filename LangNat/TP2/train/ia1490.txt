en informatique , la programmation dynamique est une méthode algorithmique pour résoudre des problèmes d&apos; optimisation . le concept a été introduit au début des années 1950 par richard bellmantemplate : harvsp . . à l&apos; époque , le terme « programmation » signifie planification et ordonnancement . la programmation dynamique consiste à résoudre un problème en le décomposant en sous-problèmes , puis à résoudre les sous-problèmes , des plus petits aux plus grands en stockant les résultats intermédiaires . elle a d&apos; emblée connu un grand succès , car de nombreuses fonctions économiques de l&apos; industrie étaient de ce type . la programmation dynamique s&apos; appuie sur un principe simple , appelé le principe d&apos; optimalité de bellman : toute solution optimale s&apos; appuie elle-même sur des sous-problèmes résolus localement de façon optimalel&apos; inverse n&apos; est pas vrai en général et un ensemble de sous-solutions chacune optimale ne garantit pas un résultat général lui-même optimal . . concrètement , cela signifie que l&apos; on peut déduire une ou la solution optimale d&apos; un problème en combinant des solutions optimales d&apos; une série de sous-problèmes . les solutions des problèmes sont calculées de manière ascendante , c&apos; est-à-dire qu&apos; on débute par les solutions des sous-problèmes les plus petits pour ensuite déduire progressivement les solutions de l&apos; ensemble . la méthode de programmation , comme la méthode diviser pour régner , résout des problèmes en combinant des solutions de sous-problèmes . alors que les algorithmes diviser-pour-régner partitionnent le problème en sous-problèmes indépendants qu ’ ils résolvent récursivement , puis combinent leurs solutions pour résoudre le problème initial , la programmation dynamique , quant à elle , peut s ’ appliquer même lorsque les sous-problèmes ne sont pas indépendants , c ’ est-à-dire lorsque des sous-problèmes comportent des parties communes . dans ce cas , un algorithme diviser-pour-régner fait du travail inutile en résolvant plusieurs fois le sous-problème commun . un algorithme de programmation dynamique résout chaque sous-problème une seule fois et mémorise la réponse dans un tableau , évitant ainsi le recalcultemplate : harvsp . . un problème d&apos; optimisation peut avoir de nombreuses solutions . chaque solution a une valeur , et on souhaite trouver une solution ayant la valeur optimale . une telle solution optimale au problème n&apos; est pas forcément unique , c&apos; est sa valeur qui l&apos; est . le développement d ’ un algorithme de programmation dynamique peut être découpé en quatre étapes . caractériser la structure d ’ une solution optimale . définir ( souvent de manière récursive ) la valeur d ’ une solution optimale . calculer la valeur d ’ une solution optimale de manière ascendante . construire une solution optimale à partir des informations calculées . la dernière étape n&apos; est utile que si on veut connaître plus que la valeur optimale . dans une pyramide de nombres , en partant du sommet , et en se dirigeant vers le bas à chaque étape , on cherche à maximiser le total des nombres traverséstemplate : harvsp . . on peut voir la pyramide comme un graphe , parcourir les 8 chemins , et choisir celui qui a le plus grand total . quand la pyramide a n niveaux , il y a 2 ^ { n-1 } chemins et 2 ^ { n } -2 calculs à effectuer. c ( x ) = v ( x ) + \ max ( c ( g ( x ) ) , c ( d ( x ) ) . où g ( x ) et d ( x ) sont les sommets à gauche et à droite sous x. mais si on cherche à calculer directement par la définition récursive , on évalue plusieurs fois la même valeur ; dans l&apos; exemple ci-dessus , la valeur du sommet portant 4 du milieu par exemple est calculée deux fois , en venant de 7 et en venant du 4 au-dessus . le nombre de calculs est seulement n ( n-1 ) / 2 . l&apos; important est de conserver dans un tableau les valeurs intermédiaires . on se donne des matrices m _ 1 , m _ 2 , \ ldots , m _ h , et on veut calculer la matrice produit m _ 1 m _ 2 \ cdots m _ htemplate : harvsp . , . les matrices ne sont pas forcément carrées , et l&apos; ordre dans lequel on effectue les multiplications peut influencer le coût . ainsi , le produit d&apos; une matrice ligne par une matrice colonne de taille m ne coûte que m multiplications élémentaires , le produit d&apos; une matrice colonne par une matrice ligne en coûte m ^ 2. c _ { i , j } = \ min _ { i \ le k &lt; j } &#91; c _ { i , k } + c _ { k + 1 , j } + m _ { i } m _ { k + 1 } m _ { j + 1 } &#93; . le calcul des c _ { i , j } se fait dans un tableau , diagonale par diagonale , en temps quadratique en le nombre de matrices . dans cet exemple aussi , il est important de garder le résultat des calculs dans un tableau pour ne pas les recalculer . problème d&apos; affectation des ressources . il s&apos; agit ( par exemple ) de distribuer m skis à n skieurs ( m &gt; n ) en minimisant les écarts de taille entre les skis et les skieurs . la propriété d&apos; optimalité des sous-structures ( si une distribution est optimale , alors toute sous-partie des skis et des skieurs est optimale ) le rend traitable par programmation dynamique . le problème du sac à dos ( knapsack en anglais ) est un problème classique de recherche opérationnelle qui est np-difficile , mais qui est résolu de manière pseudo-polynomiale à l&apos; aide d&apos; un algorithme de programmation dynamique . le problème du rendu de monnaie dans le cas général . des problèmes d&apos; algorithmique du texte comme le calcul de la plus longue sous-suite commune entre deux chaînes , le calcul de la distance de levenshtein ou encore l&apos; alignement de séquences ( avec l&apos; algorithme de smith-waterman par exemple ) . l&apos; algorithme cyk est un algorithme d&apos; analyse syntaxique . la méthode la plus utilisée pour résoudre le problème de détection de ruptures est la programmation dynamique . la programmation dynamique a des liens avec le calcul des variations et la commande optimale . un théorème général énonce que tout algorithme de programmation dynamique peut se ramener à la recherche du plus court chemin dans un graphea . martelli , « template : lang » , comm . acm , 19 ( 2 ) : 73--83 , février 1976 . . or , les techniques de recherche heuristique basées sur l&apos; algorithme a * permettent d&apos; exploiter les propriétés spécifiques d&apos; un problème pour gagner en temps de calcul . autrement dit , il est souvent plus avantageux d&apos; exploiter un algorithme a * que d&apos; utiliser la programmation dynamique . richard bellman , dynamic programming , princeton , princeton university press , &lt; time &gt; 1957 &lt; / time &gt; . — réimpression 2003 , dover publication , mineola , new-york , ( isbn 0-486-42809-5 ) . thomas h. cormen , charles e. leiserson , ronald l. rivest et clifford stein , introduction à l&apos; algorithmique , dunod , &lt; time &gt; 2002 &lt; / time &gt; &#91; détail de l ’ édition &#93; — chapitre 15 , « programmation dynamique » , pages 315-360 . robert cori , « principe de la programmation dynamique » , enseirb ( consulté le 21 février 2015 ) . guillaume carlier , « programmation dynamique - notes de cours » , ensae , ‎ &lt; time &gt; 2007 &lt; / time &gt; ( consulté le 21 février 2015 ) .
