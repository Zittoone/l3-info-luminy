vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants . consultez la liste des tâches à accomplir en page de discussion . un modèle de mélange gaussien ( usuellement abrégé par l&apos; acronyme anglais gmm pour gaussian mixture model ) est un modèle statistique exprimé selon une densité mélange . il sert usuellement à estimer paramétriquement la distribution de variables aléatoires en les modélisant comme une somme de plusieurs gaussiennes ( appelées noyaux ) . il s&apos; agit alors de déterminer la variance , la moyenne et l&apos; amplitude de chaque gaussienne . ces paramètres sont optimisés selon un critère de maximum de vraisemblance pour approcher le plus possible la distribution recherchée . cette procédure se fait le plus souvent itérativement via l&apos; algorithme espérance-maximisation ( em ) . les modèles de mélange gaussien sont réputés reconstruire de manière particulièrement efficace les données manquantes dans un jeu de données expérimentales . dans les modèles de mélanges , fréquemment utilisés en classification automatique , on considère qu&apos; un échantillon de données suit , non pas une loi de probabilité usuelle , mais une loi dont la fonction de densité est une densité mélange . bien que n&apos; importe quelle loi puisse être utilisée , la plus courante est la loi normale dont la fonction de densité est une gaussienne . on parle alors de mélange gaussien . le problème classique de la classification automatique est de considérer qu&apos; un échantillon de données provienne d&apos; un nombre de groupes inconnus a priori qu&apos; il faut retrouver . si en plus , on considère que les lois que suivent les individus sont normales , alors on se place dans le cadre des modèles de mélanges gaussiens. avec f ( \ boldsymbol { x } , \ boldsymbol { \ theta } _ k ) \ , la loi normale multidimensionnelle paramétrée par \ boldsymbol { \ theta } _ k \ , . = p ( \ bigcup _ { k = 1 } ^ g \ { ( \ boldsymbol { x } = \ boldsymbol { x } , \ boldsymbol { \ theta } = \ boldsymbol { \ theta } _ k ) \ } ) où \ boldsymbol { x } est un vecteur gaussien de dimension p et de paramètre \ boldsymbol { \ theta } . l \ left ( \ mathbf { x } ; \ boldsymbol { \ phi } \ right ) = \ sum _ { i = 1 } ^ p \ log \ left ( \ sum _ { k = 1 } ^ g \ pi _ kf ( \ boldsymbol { x } _ i , \ boldsymbol { \ theta } _ k ) \ right ) . l&apos; estimation des paramètres peut être effectuée au moyen de l&apos; algorithme em . p \ left ( \ boldsymbol { x } _ i \ in g _ k \ right ) = \ frac { \ pi _ kf \ left ( \ boldsymbol { x } _ i , \ boldsymbol { \ theta } _ k \ right ) } { \ sum _ { \ ell = 1 } ^ g \ pi _ \ ell f \ left ( \ boldsymbol { x } _ i , \ boldsymbol { \ theta } _ \ ell \ right ) } . il suffit alors d&apos; attribuer chaque individu \ boldsymbol { x } _ i à la classe pour laquelle la probabilité a posteriori p \ left ( \ boldsymbol { x } _ i \ in g _ k \ right ) est la plus grande . un problème qu&apos; on peut rencontrer lors de la mise en œuvre des modèles de mélange concerne la taille du vecteur de paramètres à estimer . dans le cas d&apos; un mélange gaussien de g composantes de dimension p le paramètre est de dimension k ( 1 + p + p ^ 2 ) -1 . la quantité de données nécessaire à une estimation fiable peut alors être trop importante par rapport au coût de leur recueil . une solution couramment employée est de déterminer quelles sont , parmi toutes les variables disponibles , celles qui apporteront le plus d&apos; information à l&apos; analyse et d&apos; éliminer les variables ne présentant que peu d&apos; intérêt . cette technique , très employée dans des problèmes de discrimination l&apos; est moins dans les problèmes de classification . une méthode alternative consiste à considérer des modèles dits parcimonieux dans lesquels on contraint le modèle initial de manière à n&apos; estimer qu&apos; un nombre plus restreint de paramètres . dans le cas gaussien , la paramétrisation synthétique des lois de probabilités grâce à deux ensembles \ boldsymbol { \ mu } _ k et \ boldsymbol { \ sigma } _ k de paramètres permet des ajouts de contraintes relativement simples . le plus souvent , ces contraintes ont une signification géométrique en termes de volumes , d&apos; orientation et de forme . \ boldsymbol { \ sigma } _ k = d _ kb _ kd _ k ^ { -1 } . d&apos; autre part , b _ k peut également être décomposée en b _ k = \ lambda _ ka _ k où \ lambda _ k est un réel et a _ k une matrice dont le déterminant vaut 1 . en utilisant ces notations , on peut considérer que \ lambda _ k représente le volume de la classe , la matrice a _ k représente sa forme et d _ k son orientation . formes quelconques : en fixant des contraintes d&apos; égalité entre les a _ k , les d _ k ou les \ lambda _ k , on peut générer 8 modèles différents . on peut par exemple considérer des volumes et des formes identiques mais orientées différemment , ou encore des formes et orientations identiques avec des volumes différents , etc. formes diagonales : en considérant que les matrices d _ k sont diagonales , on oblige les classes à être alignées sur les axes . il s&apos; agit en fait de l&apos; hypothèse d&apos; indépendance conditionnelle dans laquelle les variables sont indépendantes entre elles à l&apos; intérieur d&apos; une même classe . formes sphériques : en fixant a _ k = i , on se place dans le cas ou les classes sont de formes sphériques , c ’ est-à-dire que les variances de toutes les variables sont égales à l&apos; intérieur d&apos; une même classe . une combinaison linéaire de m mélanges gaussiens indépendants \ sum _ { i = 1 } ^ m \ boldsymbol \ alpha _ i { x } _ i est un mélange gaussien de proportions \ bigotimes _ { i = 1 } ^ m { \ boldsymbol { \ pi } _ i } ( produit de kronecker ) où les \ boldsymbol { \ pi } _ i sont les proportions des mélanges impliqués , de moyenne \ bigoplus _ { i = 1 } ^ m \ alpha _ i \ boldsymbol { \ mu } _ i .
