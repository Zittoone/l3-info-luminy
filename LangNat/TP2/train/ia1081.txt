l&apos; apprentissage par renforcement fait référence à une classe de problèmes d&apos; apprentissage automatique , dont le but est d&apos; apprendre , à partir d&apos; expériences , ce qu&apos; il convient de faire en différentes situations , de façon à optimiser une récompense quantitative au cours du temps . un paradigme classique pour présenter les problèmes d&apos; apprentissage par renforcement consiste à considérer un agent autonome , plongé au sein d&apos; un environnement , et qui doit prendre des décisions en fonction de son état courant . en retour , l&apos; environnement procure à l&apos; agent une récompense , qui peut être positive ou négative . l&apos; agent cherche , au travers d&apos; expériences itérées , un comportement décisionnel ( appelé stratégie ou politique , et qui est une fonction associant à l&apos; état courant l&apos; action à exécuter ) optimal , en ce sens qu&apos; il maximise la somme des récompenses au cours du temps . parmi les premiers algorithmes d&apos; apprentissage par renforcement , on compte le td-learning , proposé par richard sutton en 1988sutton , r.s. ( 1988 ) . learning to predict by the method of temporal differences . machine learning , 3 : 9-44 . , et le q-learningvoir machine learning , pp. 373-380 mis au point essentiellement lors d&apos; une thèse soutenue par chris watkins en 1989 et publié réellement en 1992watkins , c.j.c.h. &amp; dayan , p. ( 1992 ) . q-learning . machine learning , 8 : 279-292 . . toutefois , l&apos; origine de l&apos; apprentissage par renforcement est plus ancienne . elle dérive de formalisations théoriques de méthodes de contrôle optimal , visant à mettre au point un contrôleur permettant de minimiser au cours du temps une mesure donnée du comportement d&apos; un système dynamique . la version discrète et stochastique de ce problème est appelée un processus de décision markovien et fut introduite par bellman en 1957bellman , r.e. ( 1957 ) . a markov decision process . journal of mathematical mech . , 6 : 679-684 . . d&apos; autre part , la formalisation des problèmes d&apos; apprentissage par renforcement s&apos; est aussi beaucoup inspirée de théories de psychologie animale , comme celles analysant comment un animal peut apprendre par essais-erreurs à s&apos; adapter à son environnement . ces théories ont beaucoup inspiré le champ scientifique de l&apos; intelligence artificielle et ont beaucoup contribué à l&apos; émergence d&apos; algorithmes d&apos; apprentissage par renforcement au début des années 1980 . en retour , le raffinement actuel des algorithmes d&apos; apprentissage par renforcement inspire les travaux des neurobiologistes et des psychologues pour la compréhension du fonctionnement du cerveau et du comportement animal . en effet , la collaboration entre neurobiologistes et chercheurs en intelligence artificielle a permis de découvrir qu&apos; une partie du cerveau fonctionnait de façon très similaire aux algorithmes d&apos; apprentissage par renforcement tels que le td-learninghouk , j.c. , adams , j.l. &amp; barto , a.g. ( 1995 ) . a model of how the basal ganglia generate and use neural signals that predict reinforcement . in houk et al. ( eds ) , models of information processing in the basal ganglia . the mit press , cambridge , ma . . il semblerait ainsi que la nature ait découvert , au fil de l&apos; évolution , une façon semblable à celles trouvées par des chercheurs pour optimiser la façon dont un agent ou organisme peut apprendre par essais-erreurs . ou plutôt , les chercheurs en intelligence artificielle ont redécouvert en partie ce que la nature avait mis des millions d&apos; années à mettre en place . en effet , la zone du cerveau qui montre des analogies avec les algorithmes d&apos; apprentissage par renforcement s&apos; appelle les ganglions de la base , dont une sous-partie appelée la substance noire émet un neuromodulateur , la dopamine , qui renforce chimiquement les connexions synaptiques entre les neurones . ce fonctionnement des ganglions de la base a été identifié comme existant chez l&apos; ensemble des vertébrésredgrave , p. , prescott , t.j. &amp; gurney , k. ( 1999 ) . the basal ganglia : a vertebrate solution to the selection problem ? neuroscience , 89 , 1009-1023 . , et on retrouve le même genre de résultats en imagerie médicale chez l&apos; hommeo ’ doherty , j. , dayan , p. , schultz , j. , deichmann , r. , friston , k. &amp; dolan , r. ( 2004 ) . dissociable roles of dorsal and ventral striatum in instrumental conditioning . science , 304 : 452-454 . . enfin , la boucle d&apos; échange scientifique entre neurobiologistes , psychologues et chercheurs en intelligence artificielle n&apos; est pas terminée puisque actuellement , des chercheurs prennent inspiration du cerveau pour raffiner les algorithmes d&apos; apprentissage par renforcement et essayer ainsi de mettre au point des robots plus autonomes et adaptatifs que ceux existantskhamassi , m. , lachèze , l. , girard , b. , berthoz , a. &amp; guillot , a. ( 2005 ) . actor-critic models of reinforcement learning in the basal ganglia : from natural to artificial rats . adaptive behavior , special issue towards artificial rodents , 13 ( 2 ) : 131-148 . . en effet , même si la nature et les chercheurs semblent avoir trouvé séparément une même solution pour résoudre certains types de problèmes tels que ceux décrits au paragraphe précédent , on se rend bien compte que l&apos; intelligence des robots actuels est encore bien loin de celle de l&apos; homme ou même de celle de nombreux animaux tels que les singes ou les rongeurs . une voie prometteuse pour pallier cela est d&apos; analyser plus en détail comment le cerveau biologique paramétrise et structure anatomiquement des processus tels que l&apos; apprentissage par renforcement , et comment il intègre ces processus avec d&apos; autres fonctions cognitives telles que la perception , l&apos; orientation spatiale , la planification , la mémoire , et d&apos; autres afin de reproduire cette intégration dans le cerveau artificiel d&apos; un robotmeyer , j.-a. , guillot , a. , girard , b. , khamassi , m. , pirim , p. &amp; berthoz , a. ( 2005 ) . the psikharpax project : towards building an artificial rat . robotics and autonomous systems , 50 ( 4 ) : 211-223 . . 3. un ensemble de valeurs scalaires &quot; récompenses &quot; r que l&apos; agent peut obtenir . à chaque pas de temps t de l&apos; algorithme , l&apos; agent perçoit son état s _ t \ in s et l&apos; ensemble des actions possibles a ( s _ t ) . il choisit une action a \ in a ( s _ t ) et reçoit de l&apos; environnement un nouvel état s _ { t + 1 } et une récompense r _ { t + 1 } ( qui est nulle la plupart du temps et vaut classiquement 1 dans certains états clefs de l&apos; environnement ) . fondé sur ces interactions , l&apos; algorithme d&apos; apprentissage par renforcement doit permettre à l&apos; agent de développer une politique \ pi : s \ rightarrow a qui lui permette de maximiser la quantité de récompenses . cette dernière s&apos; écrit r = r _ 0 + r _ 1 + \ cdots + r _ n dans le cas des processus de décision markoviens ( mdp ) qui ont un état terminal , et r = \ sigma _ t \ text { } \ gamma ^ t \ text { } r _ t pour les mdps sans état terminal ( où \ gamma est un facteur de dévaluation compris entre 0 et 1 et permettant , selon sa valeur , de prendre en compte les récompenses plus ou moins loin dans le futur pour le choix des actions de l&apos; agent ) . ainsi , la méthode de l&apos; apprentissage par renforcement est particulièrement adaptée aux problèmes nécessitant un compromis entre la quête de récompenses à court terme et celle de récompenses à long terme . cette méthode a été appliquée avec succès à des problèmes variés , tels que le contrôle robotiquetemplate : article , ng , a and al. ( 2004 ) . autonomous helicopter flight via reinforcement learning . in nips 16 , le pendule inversétemplate : ouvrage , la planification de tâches , les télécommunications , le backgammontemplate : ouvrage et les échecstemplate : article , template : article . on donne ici une version algorithmique basique de l&apos; apprentissage par renforcement , particulièrement du td-learning . cette version a pour but de permettre au lecteur intéressé de faire une rapide implémentation informatique de l&apos; algorithme afin de mieux comprendre son fonctionnement itératif . on se place ici dans le cas le plus simple où on ne cherche pas à améliorer le comportement ( ou politique ) de l&apos; agent , mais on cherche à évaluer une politique donnée lorsqu&apos; elle est mise en œuvre par un agent dans un environnement donné . on initialise v ( s ) aléatoirement , qui est la valeur que l&apos; agent attribuera à chaque état s. on initialise la politique π à évaluer . pour plus de détails concernant l&apos; implémentation de cet algorithme , ou concernant la version algorithmique du q-learning , se référer au livre de sutton et barto publié en 1998sutton , r.s. &amp; barto , a.g. ( 1998 ) . reinforcement learning : an introduction . the mit press cambridge , ma . . reinforcement learning : an introduction version complète en html de l&apos; ouvrage de référence de r.s. sutton et a.g. barto en 1998 .
