l&apos; optimisation quadratique successive est un algorithme de résolution d&apos; un problème d&apos; optimisation non linéaire . un tel problème consiste à déterminer des paramètres qui minimisent une fonction , tout en respectant des contraintes d&apos; égalité et d&apos; inégalité sur ces paramètres . on parle aussi de l&apos; algorithme oqs pour optimisation quadratique successive ou de l&apos; algorithme sqp pour sequential quadratic programming , en anglais . c&apos; est un algorithme newtonien appliqué aux conditions d&apos; optimalité du premier ordre du problème . plus précisément , on peut le voir comme un algorithme de josephy-newton appliqué à ces conditions d&apos; optimalité , écrites comme un problème d&apos; inclusion fonctionnelle ou comme un problème de complémentarité . de ce fait , l&apos; algorithme bénéficie d&apos; une convergence locale rapide , mais chaque itération pourra demander beaucoup de temps de calcul ( c&apos; est surtout vrai dans les premières itérations ) . par ailleurs , l&apos; algorithme ne fait pas de distinction entre minima et maxima ( comme l&apos; algorithme de newton pour minimiser une fonction sans contrainte ) , mais ses itérés sont attirés par tout point stationnaire « régulier » . l&apos; algorithme se globalise facilement , ce qui veut dire que l&apos; on connait des techniques permettant la plupart du temps de forcer la convergence des itérés , même si le premier itéré n&apos; est pas proche d&apos; une solution du problème d&apos; optimisation . l&apos; algorithme requiert que les fonctions définissant le problème d&apos; optimisation soient « suffisamment » différentiables . il se définit naturellement en utilisant les dérivées secondes des fonctions définissant le problème , mais il se décline aussi sous une forme quasi-newtonienne , qui ne requiert donc que l&apos; évaluation des dérivées premières . connaissances supposées : le calcul différentiel ( on linéarise des fonctions ) et les conditions d&apos; optimalité des problèmes d&apos; optimisation avec contraintes ( qui est le système linéarisé ) ; l&apos; approche utilisée pour introduire l&apos; algorithme sera mieux comprise si l&apos; on a pris connaissance auparavant de l&apos; algorithme de josephy-newton , mais ce dernier point n&apos; est pas essentiel ; bien sûr , l&apos; algorithme a un lien étroit avec l&apos; algorithme de newton . \ end { array } \ right . e \ cup i = &#91; 1 : m &#93; \ qquad \ mbox { et } \ qquad e \ cap i = \ varnothing. v _ i ^ + : = \ max ( 0 , v _ i ) &amp; \ mbox { si } ~ i \ in i. \ end { array } \ right . \ ell ( x , \ lambda ) : = f ( x ) + \ lambda ^ { \ ! \ top \ ! } c ( x ) = f ( x ) + \ sum _ { i = 1 } ^ m \ lambda _ ic _ i ( x ) . le vecteur \ lambda porte le nom de multiplicateur ( de karush , kuhn et tucker ou de lagrange ) ou variable duale . l&apos; algorithme oqs est une méthode primale-duale de résolution de ( p _ { ei } ) procédant par linéarisation des conditions d&apos; optimalité du premier ordre de ce problème , celles de karush , kuhn et tucker ( kkt ) . l&apos; algorithme oqs peut être vu comme l&apos; algorithme de josephy-newton appliqué au système d&apos; optimalité écrit sous la forme de problème d&apos; inclusion fonctionnelle , même si ce dernier a été conçu après l&apos; introduction de l&apos; algorithme oqs , comme une généralisation élégante de celui-ci . l&apos; algorithme oqs est primal-dual car il génère une suite de couples ( x _ k , \ lambda _ k ) \ in \ mathbb { e } \ times \ r ^ m , où x _ k approche une solution x _ * de ( p _ { ei } ) ( dite solution primale car appartenant à \ mathbb { e } ) et \ lambda _ k approche un multiplicateur optimal \ lambda _ * \ in \ r ^ m de ( p _ { ei } ) ( aussi appelé solution duale ) . on peut énoncer l&apos; algorithme oqs sans explication sur sa conception et c&apos; est souvent comme cela qu&apos; il est présenté , mais nous préférons l&apos; introduire ici comme une application de l&apos; algorithme de josephy-newton aux conditions d&apos; optimalité du premier ordre ou conditions de karush , kuhn et tucker ( kkt ) de ( p _ { ei } ) . c&apos; est aussi en adoptant ce point de vue que l&apos; on obtient les meilleurs résultats de convergence . 0 \ leqslant ( \ lambda _ * ) _ i \ perp c _ i ( x _ * ) \ leqslant 0 . \ end { array } \ right. où k ^ + est le cône dual de k. on voit alors aisément l&apos; équivalence de ce système avec ( kkt ) en notant que k ^ + : = \ { 0 _ \ mathbb { e } \ } \ times ( \ { 0 _ { \ r ^ { m _ e } } \ } \ times \ r ^ { m _ i } _ + ) . k \ ni z \ perp f ( z _ k ) + f &apos; ( z _ k ) ( z-z _ k ) \ in k ^ + . \ end { array } \ right. où l _ k est la hessienne \ nabla ^ 2 _ { xx } \ ell ( x _ k , \ lambda _ k ) du lagrangien \ ell par rapport à x ( voir ci-dessus ) . c _ i ( x _ k ) + c _ i &apos; ( x _ k ) d \ leqslant 0 . \ end { array } \ right . \ lambda _ { k + 1 } = \ lambda _ k ^ { \ text { pq } } . on peut à présent définir l&apos; algorithme oqs . algorithme oqs — une itération passe d&apos; un couple primal-dual ( x _ k , \ lambda _ k ) \ in \ mathbb { e } \ times \ r ^ m au suivant ( x _ { k + 1 } , \ lambda _ { k + 1 } ) \ in \ mathbb { e } \ times \ r ^ m comme suit . \ lambda _ { k + 1 } = \ lambda _ k ^ { \ text { pq } } . quelques remarques s&apos; imposent. le pqo est réalisable mais n&apos; est pas borné ( sa valeur optimale vaut alors - \ infty ) . ces deux situations peuvent très bien se produire même si ( x _ k , \ lambda _ k ) est proche d&apos; une solution primale-duale ( x _ * , \ lambda _ * ) de ( p _ { ei } ) . nous verrons ci-dessous des conditions pour qu&apos; elles n&apos; aient pas lieu . il existe des techniques pour faire face aux deux situations signalées ci-dessus . clairement , le pqo représente la partie la plus coûteuse de l&apos; algorithme . le temps de calcul est nettement plus élevé que celui de la résolution d&apos; un système linéaire , requis par l&apos; algorithme de newton . ceci est surtout vrai lorsque les itérés sont éloignés d&apos; une solution , car lorsqu&apos; ils sont proches d&apos; une solution primale-duale satisfaisant la complémentarité stricte , le problème quadratique osculateur se ramène à un problème quadratique avec seulement des contraintes d&apos; égalité , dont l&apos; équation d&apos; optimalité est un système linéaire . mais en toute généralité , le pqo est np-ardu . il devient résoluble en temps polynomial si l _ k est semi-définie positive ( le pqo est convexe dans ce cas ) . c&apos; est une des raisons pour lesquelles on préfère parfois approcher l _ k par une matrice définie positive ( version quasi-newtonienne de l&apos; algorithme ) . rien n&apos; est fait dans cet algorithme pour forcer sa convergence si le premier itéré est éloigné d&apos; une solution ( on parle de globalisation de l&apos; algorithme quand des moyens sont mis en œuvre pour obtenir cette propriété ) . comme pour l&apos; algorithme de newton , l&apos; algorithme oqs ne convergera que si le premier itéré est pris suffisamment proche d&apos; une solution et que certaines conditions sont remplies : lissité des fonctions f et c et régularité de la solution cherchée ( x _ * , \ lambda _ * ) . le résultat suivant est dû à bonnans ( 1994voir bonnans ( 1994 ) . ) . on l&apos; obtient en appliquant le résultat de convergence locale de l&apos; algorithme de josephy-newton . la convergence locale est donc garantie si f et c sont suffisamment lisses et si une condition de régularité du point limite ( x _ * , \ lambda _ * ) est vérifiée , exprimée par le couple : unicité du multiplicateur et conditions suffisantes d&apos; optimalité du second ordre . l&apos; algorithme oqs est une méthode locale , conçue , on l&apos; a dit , en linéarisant les conditions d&apos; optimalité du premier ordre ( kkt ) du problème ( p _ { ei } ) , aux propriétés de convergence locale remarquables . lorsque le premier itéré n&apos; est pas dans le voisinage d&apos; une solution assurant la convergence de l&apos; algorithme , celui-ci a tendance à générer des itérés au comportement erratique , qui ne convergent pas . globaliser l&apos; algorithme signifie donner une technique améliorant sa convergence lorsque le premier itéré n&apos; est pas proche d&apos; une solution ( cela n&apos; a donc rien à voir avec la recherche d&apos; un minimum global ) . il n&apos; y a pas de méthode algorithmique permettant de trouver à coup sûr une solution d&apos; un système d&apos; équations non linéaires de la forme f ( x ) = 0 , quelle que soit la fonction f ( opérant sur \ r ^ mpar exemple ) . il n&apos; y a donc pas non plus de méthode permettant de trouver à coup sûr une solution de ( p _ { ei } ) car en l&apos; appliquant au problème { \ min } _ x \ , \ { 0 : f ( x ) = 0 \ } on serait alors assuré de trouver une solution du système non linéaire f ( x ) = 0 . les techniques de globalisation de l&apos; algorithme oqs ont donc la tâche plus modeste d&apos; améliorer sa convergence lorsque le premier itéré est éloigné d&apos; une solution de ( p _ { ei } ) . ( en ) j.f. bonnans ( 1994 ) . local analysis of newton-type methods for variational inequalities and nonlinear programming . applied mathematics and optimization , 29 , 161 – 186 . ( en ) j. f. bonnans , j. ch . gilbert , c. lemaréchal , c. sagastizábal ( 2006 ) , numerical optimization - theoretical and numerical aspects &#91; détail des éditions &#93; . ( en ) a.f. izmailov , m.v. solodov ( 2014 ) . newton-type methods for optimization and variational problems , springer series in operations research and financial engineering , springer . ( en ) j. nocedal , s. j. wright ( 2006 ) , numerical optimization , springer . ( isbn 0-387-30303-0 ) .
