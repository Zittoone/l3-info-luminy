vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants . consultez la liste des tâches à accomplir en page de discussion . en théorie de la décision et de la théorie des probabilités , un processus de décision markovien ( mdp ) est un modèle stochastique où un agent prend des décisions et où les résultats de ses actions sont aléatoires . les mdps sont utilisés pour étudier des problèmes d&apos; optimisation à l&apos; aide d&apos; algorithmes de programmation dynamique ou d&apos; apprentissage par renforcement . les mdps sont connus depuis 1950s ( cf. bellman 1957 ) . une grande contribution provient du travail de ronald a. howard avec son livre de 1960 , &quot; dynamic programming and markov processes &quot; . ils sont utilisés dans de nombreuses disciplines , incluant la robotique , l&apos; automatisation , l&apos; économie , et l&apos; industrie manufacturière . un processus de décision markovien est un processus de contrôle stochastique discret . à chaque étape , le processus est dans un certain état s , et l&apos; agent choisit une action a. la probabilité que le processus arrive à l&apos; état s &apos; est déterminée par l&apos; action choisie . plus précisément , elle est décrite par la fonction de transition d&apos; états t ( s , a , s &apos; ) . donc , l&apos; état s &apos; dépend de l&apos; état actuel s et de l&apos; action a sélectionnée par le décideur . cependant , pour un s et un a , le prochain état est indépendant des actions et états précédents . on dit alors que le processus satisfait la propriété de markov . quand le processus passe de l&apos; état s à l&apos; état s &apos; avec l&apos; action a , l&apos; agent gagne une récompense r ( s , a , s &apos; ) . les mdps sont une extension des chaînes de markov . la différence est l&apos; addition des actions choisies par l&apos; agent et des récompenses gagnées par l&apos; agent . s&apos; il n&apos; y a qu&apos; une seule action à tirer dans chaque état et que les récompenses sont égales , le processus de décision markovien est une chaîne de markov . afin de comprendre ce qu&apos; est un mdp , supposons que l&apos; on ait un système évoluant dans le temps comme un automate probabiliste . à chaque instant le système est dans un état donné et il existe une certaine probabilité pour que le système évolue vers tel ou tel autre état à l&apos; instant suivant en effectuant une transition . supposons maintenant que l&apos; on doive contrôler ce système boite noire de la meilleure façon possible . l&apos; objectif est de l&apos; amener dans un état considéré comme bénéfique , en évitant de lui faire traverser des états néfastes . pour cela , on dispose d&apos; un ensemble d&apos; actions possibles sur le système . pour compliquer la chose , on supposera que l&apos; effet de ces actions sur le système est probabiliste : l&apos; action entreprise peut avoir l&apos; effet escompté ou un tout autre effet . l&apos; efficacité du contrôle est mesurée relativement au gain ou à la pénalité reçue au long de l&apos; expérience . ainsi , un raisonnement à base de mdp peut se ramener au discours suivant : étant dans tel cas et choisissant telle action , il y a tant de chance que je me retrouve dans tel nouveau cas avec tel gain . pour illustrer les mdp , on prend souvent des exemples issus de la robotique mobile ( avec les positions pour états , les commandes comme actions , les mouvements comme transitions et l&apos; accomplissement / échec de tâches comme gains / pénalités ) . dans les mdp , l&apos; évolution du système est supposée correspondre à un processus markovien . autrement dit , le système suit une succession d&apos; états distincts dans le temps et ceci en fonction de probabilités de transitions . l&apos; hypothèse de markov consiste à dire que les probabilités de transitions ne dépendent que des n états précédents . en général , on se place à l&apos; ordre n = 1 , ce qui permet de ne considérer que l&apos; état courant et l&apos; état suivant. une fonction de transition t : s \ times a \ times s \ to &#91; 0 ; 1 &#93; ; cette fonction définit l&apos; effet des actions de l&apos; agent sur l&apos; environnement : t ( s , a , s &apos; ) représente la probabilité de se retrouver dans l&apos; état s &apos; en effectuant l&apos; action a , sachant que l&apos; on était à l&apos; instant d&apos; avant dans l&apos; état s. t ainsi définie représente le cas le plus général ; dans un environnement déterministe , on aura plutôt t : s \ times a \ to s. r : s \ to \ r ( récompense déterministe rattachée à un état donné ) . dans la littérature , la fonction de récompense est des fois remplacée par une fonction de coût &#91; réf. nécessaire &#93; catégorie : article à référence nécessaire . nb : nous ne considérons ici que les modèles dans lesquels le temps est discrétisé , c&apos; est-à-dire que la « trajectoire » de l&apos; agent dans l&apos; environnement est décrite par une suite d&apos; états s _ t ( t \ in \ n ) , et non par une fonction s ( t ) avec t \ in \ r. de même on notera a _ t la suite des actions prises par l&apos; agent. pour une description des mdp à temps continu . l&apos; exemple donné ci-contre représente un processus de décision markovien à trois états distincts \ { s _ 0 , s _ 1 , s _ 2 \ } représentés en vert . depuis chacun des états , on peut effectuer une action de l&apos; ensemble \ { a _ 0 , a _ 1 \ } . les nœuds rouges représentent donc une décision possible ( le choix d&apos; une action dans un état donné ) . les nombres indiqués sur les flèches sont les probabilités d&apos; effectuer la transition à partir du nœud de décision . enfin , les transitions peuvent générer des récompenses ( dessinées ici en jaune ) . le modèle mdp présenté ici est supposé stable dans le temps , c&apos; est-à-dire que les composants du quadruplet sont supposés invariants . il n&apos; est donc pas applicable en l&apos; état pour un système qui évolue , par exemple pour modéliser un système qui apprend contre un autre agent . une politique décrit les choix des actions à jouer par l&apos; agent dans chaque état . formellement , il s&apos; agit donc d&apos; une fonction \ pi : s \ to a dans le cas d&apos; une politique déterministe ou \ pi : s \ times a \ to &#91; 0 ; 1 &#93; dans le cas stochastique . nous nous plaçons dans le cas déterministe . l&apos; agent choisit une politique à l&apos; aide de la fonction de récompense r. notons r _ t = r ( s _ t , \ pi ( s _ t ) , s _ { t + 1 } ) la récompense effective obtenue après avoir effectué l&apos; action \ pi ( s _ t ) par l&apos; agent qui suit la politique \ pi . e \ left ( \ sum _ { t = 0 } ^ \ infty \ gamma ^ t r _ t \ right ) : récompense escomptée ( ou amortie ) à horizon infini où 0 \ leq \ gamma &lt; 1 . le dernier critère est courant et c&apos; est celui que nous adoptons dans la suite . la valeur de \ gamma permet de définir l&apos; importance que l&apos; on donne au futur . quand \ gamma = 0 nous sommes face à un agent « pessimiste » qui ne cherche qu&apos; à optimiser son gain immédiat . à l&apos; opposé si \ gamma \ to 1 , l&apos; agent est « optimiste » puisqu&apos; il tient de plus en plus sérieusement compte du futur lointain . v ^ \ pi : s \ to \ r : c&apos; est la fonction de valeur des états ; v ^ \ pi ( s ) représente le gain ( selon le critère adopté ) engrengé par l&apos; agent s&apos; il démarre à l&apos; état s et applique ensuite la politique \ pi ad infinitum . q ^ \ pi : s \ times a \ to \ r : c&apos; est la fonction de valeur des états-actions ; q ^ \ pi ( s , a ) représente le gain engrengé par l&apos; agent s&apos; il démarre à l&apos; état s et commence par effectuer l&apos; action a , avant d&apos; appliquer ensuite la politique \ pi ad infinitum . les deux fonctions sont intimement liées . q ^ \ pi ( s , a ) = \ sum _ { s &apos; \ in s } &#91; r ( s , a , s &apos; ) + \ gamma v ^ \ pi ( s &apos; ) &#93; t ( s , a , s &apos; ) . v ^ \ pi ( s ) = \ sum _ { s &apos; \ in s } &#91; r ( s , \ pi ( s ) , s &apos; ) + \ gamma v ^ \ pi ( s &apos; ) &#93; t ( s , \ pi ( s ) , s &apos; ) . planification : étant donné un mdp \ { s , a , t , r \ } , trouver quelle est une politique \ pi qui maximise l&apos; espérance de la récompense . améliorer une politique connue : étant donné une politique \ pi _ 0 , trouver une meilleure politique . ce problème est notamment au cœur des algorithmes de recherche de la politique optimale. à partir de traces d&apos; exécution : c&apos; est le problème de l&apos; apprentissage par renforcement hors ligne. au cours d&apos; expériences sur le modèle , on parle alors d&apos; apprentissage par renforcement en ligne . une politique étant fixée , l&apos; équation de bellman peut se résoudre d&apos; au moins deux manières , permettant donc de déterminer les valeurs de v ^ \ pi , et par suite , celles de q ^ \ pi également. on peut déjà remarquer que , dans le cas où le nombre d&apos; états n est fini , l&apos; équation de bellman cache en fait un système linéaire de n équations à n inconnues . on peut donc le résoudre , une fois traduit en une équation matricielle , par une technique telle que le pivot de gauss. on définit un opérateur k , appelé opérateur de bellman , pour lequel v ^ \ pi est un point fixe . on peut montrer que k est une contraction , ce qui garantit d&apos; une part l&apos; existence d&apos; un unique point fixe , et d&apos; autre part que la suite récurrence v _ { n + 1 } = k ( v _ n ) converge vers ce point fixe exponentiellement vite . le but de l&apos; agent est de trouver la politique optimale \ pi ^ * qui lui permet de maximiser son gain , c&apos; est-à-dire celle qui vérifie , pour tout état s \ in s , v ^ { \ pi ^ * } ( s ) \ geq v ^ { \ pi } ( s ) quelle que soit l&apos; autre politique \ pi . v ^ * ( s ) = \ max _ { a \ in a } \ sum _ { s &apos; \ in s } &#91; r ( s , \ pi ( s ) , s &apos; ) + \ gamma v ^ * ( s &apos; ) &#93; t ( s , a , s &apos; ) . q ^ * ( s , a ) = \ sum _ { s &apos; \ in s } &#91; r ( s , a , s &apos; ) + \ gamma \ max _ { a &apos; \ in a } q ^ * ( s &apos; , a &apos; ) &#93; t ( s , a , s &apos; ) . les équations d&apos; optimalité de bellman ne sont pas linéaires , il faut donc abandonner l&apos; idée de les résoudre algébriquement. définit encore une contraction dont v ^ * est un point fixe . la fonction de valeurs optimale peut donc à nouveau s&apos; approcher par un processus itératif à convergence exponentielle . la méthode itérative que nous venons de voir pour les équations d&apos; optimalité de bellman fournit un premier algorithme , appelé itération sur la valeur ( vi : value-iteration ) permettant de déterminer \ pi ^ * . = \ arg \ max _ { a \ in a } \ sum _ { s &apos; \ in s } &#91; r ( s , a , s &apos; ) + \ gamma v ^ * ( s &apos; ) &#93; t ( s , a , s &apos; ) . une difficulté dans cet algorithme est de déterminer la précision avec laquelle calculer v ^ * de manière à être sûr d&apos; en déduire effectivement la politique optimale . un autre algorithme , appelé itération de la politique ( pi : policy-iteration ) essaye d&apos; obtenir la politique optimale sans nécessairement calculer « jusqu&apos; au bout » les valeurs de v ^ * . \ pi _ { n + 1 } ( s ) = \ arg \ max _ { a \ in a } \ sum _ { s &apos; \ in s } &#91; r ( s , a , s &apos; ) + \ gamma v ^ { \ pi _ n } ( s &apos; ) &#93; t ( s , a , s &apos; ) . . cet algorithme prend fin lorsqu&apos; aucune évolution de la politique n&apos; est observée , ie , lorsque \ pi _ { n + 1 } ( s ) = \ pi _ n ( s ) pour tout s. si dans l&apos; algorithme précédent l&apos; on utilise une méthode itérative pour évaluer v ^ \ pi , alors se pose la question de savoir à quelle précision s&apos; arrêter . ce problème n&apos; en est en réalité pas un , car on peut montrer que même si l&apos; on tronque l&apos; évaluation de v ^ \ pi , l&apos; algorithme converge tout de même vers l&apos; optimal . à l&apos; extrême , c&apos; est-à-dire lorsqu&apos; une seule itération est utilisée pour évaluer v ^ \ pi , et après avoir réuni en une seule étape de calcul la phase d&apos; amélioration et la phase d&apos; évaluation , on retombe sur l&apos; algorithme vi . l&apos; algorithme pi peut également se formuler dans les termes de la fonction d&apos; états-actions q plutôt que v. on voit donc qu&apos; un grand nombre de variantes peuvent être imaginées , mais tournant toutes autour d&apos; un même principe général qui est schématisé à la figure ci-contre . l&apos; apprentissage par renforcement , méthode permettant de résoudre des processus de décision markoviens . les métaheuristiques , méthodes utilisant parfois des processus markoviens . ( putterman 1994 ) putterman m. l. , markov decision processes . discrete stochastic dynamic programming . wiley-interscience , new york 1994 , 2005 . ( sutton et barto 1998 ) sutton r. s. et barto a.g. reinforcement learning : an introduction . mit press , cambridge , ma , 1998 .
