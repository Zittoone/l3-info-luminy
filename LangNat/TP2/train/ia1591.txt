le surapprentissage ou sur-ajustement ( en anglais « overfitting » ) est un problème pouvant survenir dans les méthodes mathématiques et informatiques d&apos; apprentissage automatique . il est en général provoqué par un mauvais dimensionnement de la structure utilisée pour classifier . de par sa trop grande capacité à stocker des informations , une structure dans une situation de surapprentissage aura de la peine à généraliser les caractéristiques des données . elle se comporte alors comme une table contenant tous les échantillons utilisés lors de l&apos; apprentissage et perd ses pouvoirs de prédiction sur de nouveaux échantillons . le surapprentissage s&apos; interprète comme un apprentissage &quot; par cœur &quot; des données . il résulte souvent d&apos; une trop grande liberté dans le choix du modèle . le figure ci-dessous illustre ce phénomène dans le cas d&apos; une régression dans \ mathbb { r } ^ 2 . les points verts sont correctement décrits par une régression linéaire . si l&apos; on autorise un ensemble de fonctions d&apos; apprentissage plus grand , par exemple l&apos; ensemble des fonctions polynomiales à coefficients réels , il est possible de trouver un modèle décrivant parfaitement les données d&apos; apprentissage ( erreur d&apos; apprentissage nulle ) . c&apos; est le cas du polynôme d&apos; interpolation de lagrange : il passe bien par tous les points verts mais n&apos; a visiblement aucune capacité de généralisation . en rouge , les points de l&apos; ensemble de test . en bleu , le polynôme d&apos; interpolation de lagrange a une erreur d&apos; apprentissage nulle mais est fortement affecté par le bruit de l&apos; ensemble d&apos; apprentissage et échoue à en dégager les caractéristiques . pour limiter ce genre de problèmes dans le cas des réseaux de neurones , on doit veiller à utiliser un nombre adéquat de neurones et de couches cachées . cependant , ces paramètres sont difficiles à déterminer à l&apos; avance . pour détecter un surapprentissage , on sépare les données en deux sous-ensembles : l&apos; ensemble d&apos; apprentissage et l&apos; ensemble de validation . l&apos; ensemble d&apos; apprentissage comme son nom l&apos; indique permet de faire évoluer les poids du réseau de neurones avec par exemple une rétropropagation . l&apos; ensemble de validation n&apos; est pas utilisé pour l&apos; apprentissage mais permet de vérifier la pertinence du réseau avec des échantillons qu&apos; il ne connait pas . on peut vraisemblablement parler de surapprentissage si l&apos; erreur de prédiction du réseau sur l&apos; ensemble d&apos; apprentissage diminue alors que l&apos; erreur sur la validation augmente de manière significative . cela signifie que le réseau continue à améliorer ses performances sur les échantillons d&apos; apprentissage mais perd son pouvoir de prédiction sur ceux provenant de la validation . pour avoir un réseau qui généralise bien , on arrête l&apos; apprentissage dès que l&apos; on observe cette divergence entre les deux courbes . on peut aussi diminuer la taille du réseau et recommencer l&apos; apprentissage . les méthodes de régularisation comme le weight decay permettent également de limiter la spécialisation . une autre méthode permettant d&apos; éviter le surapprentissage est d&apos; utiliser une forme de régularisation . durant l&apos; apprentissage , on pénalise les valeurs extrêmes des paramètres , car ces valeurs correspondent souvent à un surapprentissage . voir notamment la page rasoir d&apos; ockham pour des phénomènes semblables au surapprentissage dans d&apos; autres domaines .
