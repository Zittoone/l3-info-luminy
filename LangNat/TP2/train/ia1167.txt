le codage parcimonieux ( ou sparse coding en anglais ) est un type de réseau de neurones à apprentissage non supervisé . il est basé sur le principe qu&apos; une entrée est approximativement la somme de plein de petits motifs élémentaires . plus rigoureusement , considérons une entrée vectorielle x. alors , considérant qu&apos; on a à notre disposition un dictionnaire de motifs élémentaires d , matrice contenant dans chaque colonne un motif de la même taille que x ( x \ in \ r ^ m et d \ in \ r ^ { m \ times k } ) , il existe ( \ alpha _ i ) _ { i \ in &#91; 1 .. k &#93; } tel que x \ simeq d . \ alpha . plus généralement , pour un jeu de données de taille d , le problème revient à décomposer la matrice x \ in \ r ^ { m \ times d } en produit de deux autres matrices plus simple : la propriété de parcimonie impose qu&apos; il y ait le plus de 0 possible dans le vecteur de poids alpha ainsi que dans le dictionnaire . cette condition se traduit par le fait que l&apos; on veut reproduire l&apos; entrée avec le moins de motif possiblewiki de stanford sur le sparse coding : http : / / ufldl.stanford.edu / wiki / index.php / sparse _ coding . la pratique courante est de découper l&apos; entrée en petites fenêtres de taille fixe afin d&apos; avoir à traiter des entrées de taille constante , quitte à faire du zéro-padding pour compléter une fenêtre qui dépasse de l&apos; entrée . le premier terme correspond à la distance de l&apos; entrée reconstruite par rapport à l&apos; entrée originale tandis que le second terme correspond à la condition de parcimonie . toute la difficulté de l&apos; apprentissage pour le codage parcimonieux est d&apos; apprendre à la fois un dictionnaire ainsi que les poids . pour ce faire , on fixe alternativement un des paramètres pour entrainer l&apos; autre à l&apos; aide d&apos; une descente de gradient stochastiquetemplate : chapitre . . des améliorations peuvent être apportées à la construction précédente en rajoutant des conditions sur le dictionnaire ou sur les poids . on peut notamment montrer que l&apos; ajout de l&apos; invariance par translation via une convolution apporte des résultats meilleurs et permettent d&apos; obtenir un dictionnaire plus complexe car il y a moins d&apos; éléments redondants dans celui-cisite pour obtenir l&apos; article : http : / / www.mitpressjournals.org / doi / abs / 10.1162 / neco _ a _ 00670 ? journalcode = neco # .v34 _ qbo _ 1uq .
