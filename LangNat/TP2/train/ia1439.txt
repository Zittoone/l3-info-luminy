si vous disposez d&apos; ouvrages ou d&apos; articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici , merci de compléter l&apos; article en donnant les références utiles à sa vérifiabilité et en les liant à la section « notes et références » ( modifier l&apos; article , comment ajouter mes sources ? ) . pour les articles homonymes , voir classification spectrale . en informatique théorique , le partitionnement spectral ou spectral clustering en anglais , est un type de partitionnement de données prenant en compte les propriétés spectrales de l&apos; entrée . le partitionnement spectral utilise le plus souvent les vecteurs propres d&apos; une matrice de similarités . par rapport à des algorithmes classiques comme celui des k-moyennes , cette technique offre l&apos; avantage de classer des ensembles de données de structure « non-globulaire » dans un espace de représentation adéquat . ce partitionnement est notamment utilisé en intelligence artificielle , où le terme classification spectrale renvoie au fait de faire de la classification non-supervisée en utilisant ce type de partitionnement . le partitionnement spectral est une méthode de partitionnement en k groupes reposant sur la minimisation d&apos; un critère de type « coupe » ( coupe simple à k = 2 , ou coupe multiple à k ≥ 2 ) . ces deux mesures expriment la cohésion interne des groupes , relativement à leur dissociation les uns des autres . elles sont directement fonctions d&apos; une matrice de similarités entre objets , notée s. étant donné un ensemble de n objets , il est possible d&apos; obtenir une représentation détaillée de cet ensemble sous la forme d&apos; un graphe pondéré ( cf. théorie des graphes ) , noté g ( v , e , s ) . v désigne l&apos; ensemble des n nœuds du graphe , correspondant aux objets ; e est l&apos; ensemble des arcs inter-nœuds et s est la matrice de poids des arcs ( matrice de similarités ) , symétrique et non négative ( sij indiquant la similarité entre les objets xi et xj ) . dans le but de résoudre le problème d&apos; optimisation du critère de coupe défini précédemment , le partionnement spectral s&apos; appuie sur l&apos; utilisation du spectre de la matrice de similarités des données en vue du partitionnement de l&apos; ensemble des objets dans un espace de plus faible dimension . on a alors un problème de partitionnement de graphe pour lequel les nœuds d&apos; un même groupe sont similaires et les nœuds appartenant à des groupes différents sont dissimilaires « von luxburg u. , a tutorial on spectral clustering . statistics and computing , vol. 17 ( 4 ) , p. 395-416 , 200 7 » . graphe des k plus proches voisins : connexion du ie nœud avec le je nœud si et seulement si l&apos; objet xj fait partie des k plus proches voisins de l&apos; objet xi . graphe totalement connecté : connexion de tous les nœuds entre eux et pondération des arcs de liaison par la valeur de sij. avec d étant une mesure de distance ( de type euclidienne , manhattan , minkowski , etc. ) , et σ , un paramètre d&apos; échelle dont la valeur est fixé par l&apos; utilisateur. avec b désignant la matrice de données ( constituée de n objets décrits chacun par m attributs ) normalisée en ligne . les algorithmes de partitionnement spectral utilisent l&apos; information contenue dans les vecteurs propres d&apos; une matrice laplacienne ( construite à partir de la matrice de similarités s ) afin de détecter une structure des données . ( avec dmax désignant le degré maximum de d et i étant la matrice identité ) . l&apos; étape suivante consiste à extraire les vecteurs propres de la matrice laplacienne . il est démontré que l&apos; utilisation des k ( nombre de groupes souhaité ) premiers vecteurs propres orthogonaux de l ( correspondant aux k plus grandes valeurs propres ) , permet d&apos; obtenir un espace de projection de plus faible dimension ( en k dimensions ) que l&apos; espace original ( en m dimensions ) . la matrice x est alors construite en stockant ces vecteurs propres ( en colonnes ) : { x } = &#91; x _ 1 , x _ 2 , ... , x _ k &#93; . parmi les algorithme de calcul des valeurs propres ( en ) catégorie : article contenant un appel à traduction en anglais , on peut citer la méthode de la puissance itérée . le partitionnement des données est alors effectué sur la matrice x. en effet , la première étape consiste à considérer chaque ligne de cette matrice comme représentant un objet dans l&apos; espace spectral ( en k dimensions ) . la seconde étape est alors d&apos; appliquer un algorithme de classification non-supervisée sur cette matrice . le partitionnement des données en k groupes se résume alors à l&apos; affectation de l&apos; objet original xi au groupe k si et seulement si la ie ligne de x a été affectée au groupe k. bien que l&apos; algorithme de partitionnement utilisé le plus souvent dans la littérature soit l&apos; algorithme des k-moyennes , il est tout à fait envisageable d&apos; utiliser n&apos; importe quelle autre méthode non-supervisée afin de classer les différents objets « lang k. , fixing two weaknesses of the spectral method . advances in neural information processing systems , p. 715-722 , 2006 » , « bach f. , jordan m. , learning spectral clustering . advances in neural information processing systems , p. 305-312 , 2004 » . classification non-supervisée .
