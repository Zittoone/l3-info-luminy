en statistiques , le lasso est une méthode de contraction des coefficients de la régression développée par robert tibshirani dans un article publié en 1996 intitulé regression shrinkage and selection via the lassotemplate : article . le nom est un acronyme anglais : least absolute shrinkage and selection operator , template : lien web . bien que cette méthode fût utilisée à l&apos; origine pour des modèles utilisant l&apos; estimateur usuel des moindres carrés , la pénalisation lasso s&apos; étend facilement à de nombreux modèles statistiques tels que les modèles linéaires généralisés , les modèles à risque proportionnel , et les m-estimateurs . la capacité du lasso à sélectionner un sous-ensemble de variables est due à la nature de la contrainte exercée sur les coefficients et peut s&apos; interpréter de manière géométrique , en statistique bayésienne ou analyse convexe . soit x _ i = ( x _ { i , 1 } , \ dots , x _ { i , p } ) ^ t , le vecteur contenant les variables explicatives associées à l&apos; individu i , y _ i la réponse associée et \ beta = ( \ beta _ 1 , \ dots , \ beta _ p ) ^ t les coefficients à estimer . dans le cadre d&apos; un modèle linéaire standard , les coefficients sont obtenus par minimisation de la somme des carrés des résidus . le paramètre t contrôle le niveau de régularisation des coefficients estimés . il s&apos; agit d&apos; une pénalisation de la norme \ ell _ 1 des coefficients \ beta _ j , j = 1 , \ dots , p. cette contrainte va contracter la valeur des coefficients ( tout comme la régression ridge ) mais la forme de la pénalité \ ell _ 1 va permettre à certains coefficients de valoir exactement zéro ( à l&apos; inverse de la régression ridge ) . de plus , dans des cas où le nombre de variables est supérieur au nombre d&apos; individus n &lt; p , le lasso en sélectionnera au plus ntemplate : article. avec \ lambda \ geq 0 le paramètre de régularisation . ce paramètre \ lambda est relié au paramètre t par une relation dépendante des données . \ end { array } \ right . \ end { array } \ right . le lasso n&apos; est pas uniquement restreint à la régression linéaire , il peut être également utilisé avec les modèles linéaires généralisés permettant ainsi de faire de la régression logistique pénalisée. avec f _ \ beta une fonction objectif. f _ \ beta ( x _ { i , . } , y _ i ) = \ frac { 1 } { n } \ sum _ { i = 1 } ^ n y _ i ( \ beta _ 0 + x _ { i , . } ^ t \ beta ) - \ log ( 1 + e ^ { ( \ beta _ 0 + x _ { i , . } ^ t \ beta ) } ) . grande dimension : le lasso fonctionne dans les cas où le nombre d&apos; individus est inférieur au nombre de variables ( n &lt; p ) , si toutefois un faible nombre de ces variables a une influence sur les observations ( hypothèse de parcimonie ) . cette propriété n&apos; est pas vraie dans le cas de la régression linéaire classique avec un risque associé qui augmente comme la dimension de l&apos; espace des variables même si l&apos; hypothèse de parcimonie est vérifiée . sélection parcimonieuse : le lasso permet de sélectionner un sous-ensemble restreint de variables ( dépendant du paramètre \ lambda ) . cette sélection restreinte permet souvent de mieux interpréter un modèle ( rasoir d&apos; ockham ) . les fortes corrélations : si des variables sont fortement corrélées entre elles et qu&apos; elles sont importantes pour la prédiction , le lasso en privilégiera une au détriment des autres . un autre cas , où les corrélations posent problème , est quand les variables d&apos; intérêts sont corrélées avec d&apos; autres variables . dans ce cas , la consistance de la sélection du lasso n&apos; est plus assurée . la très grande dimension : lorsque notamment la dimension est trop élevée ( p très grand comparé à n ) ou le vrai vecteur \ beta n&apos; est pas suffisamment creux ( trop de variables d&apos; intérêts ) , le lasso ne pourra pas retrouver l&apos; ensemble de ces variables d&apos; intérêtstemplate : article . comme la fonction objectif du lasso n&apos; est pas différentiable ( car la norme \ ell _ 1 n&apos; est pas différentiable en 0 ) , différents algorithmes ont été développés afin d&apos; en trouver les solutions . parmi ces algorithmes , on retrouve notamment le least-angle regression ( lars ) template : article et la descente de coordonnées circulaire template : article . le lasso est utilisé dans des problèmes de grande dimension ( n &lt; &lt; p ) , un cas où des méthodes plus classiques ne fonctionnent pas . le lasso dispose d&apos; algorithmes peu coûteux en temps de calcul et de stockage , ce qui le rend d&apos; autant plus populaire , comme en génomique où l&apos; on peut être amené à traiter des jeux de données avec plusieurs centaines de milliers de variables . en pratique , le lasso est testé pour différentes valeurs de \ lambda . un chemin solution représentant l&apos; évolution des coefficients en fonction de \ lambda est ainsi obtenu . la courbe d&apos; un coefficient estimé en fonction de \ lambda est linéaire par morceaux . une fois ce chemin solution obtenu , une valeur de \ lambda est choisie par des méthodes comme la validation croisée ou un critère d&apos; information ( aic par exemple ) . un certain nombre de variantes du lasso ont été créées pour étendre la méthode à différents cas pratiques ou pour palier certaines limitations du lasso . sont présentées ici les variantes les plus courantes. avec \ lambda _ 1 \ geq 0 et \ lambda _ 2 \ geq 0. avec \ lambda _ 1 \ geq 0 et \ lambda _ 2 \ geq 0. avec \ lambda \ geq 0 , le paramètre de régularisation et w _ j &gt; 0 , un poids associé au groupe g _ j ( généralement \ sqrt { \ mathrm { card } ( g _ j ) } ) .
