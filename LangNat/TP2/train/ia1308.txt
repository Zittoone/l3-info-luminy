une intelligence artificielle amicale ( aussi appelé ia amicale ou iaa ) est une intelligence artificielle hypothétique qui aurait un effet positif plutôt que négatif sur l&apos; humanité . il fait partie de l&apos; éthique de l&apos; intelligence artificielle et est étroitement lié à l&apos; éthique des machines . alors que l&apos; éthique des machines se préoccupe de la façon dont un agent artificiellement intelligent doitse comporter , la recherche de l&apos; intelligence artificielle amicale est axée sur la façon de provoquer ce comportement et de s&apos; assurer qu&apos; il est suffisamment restreint . yudkowsky ( 2008 ) va dans le détails sur la façon de concevoir une ia amical . il affirme que la gentillesse ( le désir de ne pas nuire aux humains ) doit être conçue dès le début , mais que les concepteurs doivent reconnaître que cette conception peut être défectueuse , et que le robot va apprendre et évoluer avec le temps . le défi est donc celui de la conception d&apos; un mécanisme — définir un mécanisme pour l&apos; évolution des systèmes d&apos; ia qui resteront amicales face à de tels changements . dans ce contexte , l&apos; expression « amicale » est utilisée commeterminologie technique et choisit des agents sûrs et utiles , pas forcément ceux qui sont « amicaux » au sens familier . le concept est principalement utitlisé dans le contexte des discussions sur cette technologie hypothétique aurait un impact important , rapide et difficile à contrôler sur la société humainetemplate : ouvrage . les bases de l&apos; inquiétude concernant l&apos; intelligence artificielle sont très anciennes . kevin lagrandeur a montré que les dangers spécifiques de l&apos; ia peuvent être observés dans la littérature ancienne concernant les humanoïdes artificiels tels que le golem , ou les proto-robots degerbert d&apos; aurillac et roger bacon . dans ces histoires , l&apos; intelligence extrême et la puissance de ces créations humanoïdes s&apos; opposent à leur statut d&apos; esclaves ( qui , par nature , sont considérés comme sous-humains ) et provoquent des conflits désastreux . template : lien web en 1942 , ces thèmes incitent isaac asimov à créer les « trois lois de la robotique » - des principes intégrés dans tous les robots de sa fiction , à savoir qu&apos; ils ne peuvent pas se retourner vers leurs créateurs ou leur permettre de nuiretemplate : ouvrage . steve omohundro affirme que tous les systèmes d&apos; ia avancés , à moins qu&apos; ils ne soient explicitement contrecarrés , présentent un certain nombre de pulsions / tendances / désirs de base en raison de la nature intrinsèque des systèmes axés sur les objectifs sans précautions particulières , car l&apos; ia agit d&apos; une manière qui va de la désobéissance au contraire à l&apos; éthique . alexander wissner-gross affirme que les ias qui cherchent à maximiser leur liberté d&apos; action peuvent être considérées comme amicales , si leur horizon de planification est plus long qu&apos; un certain seuil , et peu amical si leur horizon de planification est plus court que ce seuil&apos; how skynet might emerge from simple physics , io9 , published 2013-04-26 . , template : article . luke muehlhauser , rédacteur en chef de la machine intelligence research institute , recommande que les chercheurs en éthique des machines adoptent ce que bruce schneier appelle la « mentalité de la sécurité » : plutôt que de penser comment un système fonctionnerait , imaginez comment il pourrait échouertemplate : lien web . ben goertzel , un chercheur en intelligence artificielle , estime que l&apos; ia amicale ne peut pas être créé avec les connaissances humaines actuelles . goertzel suggère que les humains peuvent plutôt décider de créer un « ai nanny » avec des pouvoirs « légèrement surhumains d&apos; intelligence et de surveillance » pour protéger la race humaine desrisques de catastrophes , tel que la nanotechnologie et retarder le développement d&apos; autres intelligences artificielles ( hostiles ) jusqu&apos; à ce que des solutions de sécurité soit trouvéesgoertzel , ben . . james barrat , auteur de our final invention , a suggéré qu &apos; « un partenariat public-privé doit être créé afin de rassembler les responsables de recherche et développement dans le but de de partager des idées concernant la sécurité — quelque chose comme linternational atomic energy agency , mais en partenariat avec les entreprises » . il exhorte les chercheurs d&apos; ia à convoquer une réunion similaire à la conférence d&apos; asilomar sur l&apos; adn recombinant , qui a traité des risques de la biotechnologietemplate : article . selongary marcus , le montant annuel d&apos; argent dépensé pour développer la morale des machines est très petittemplate : article . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « friendly artificial intelligence » ( voir la liste des auteurs ) . yudkowsky , e. artificial intelligence as a positive and negative factor in global risk . in global catastrophic risks , oxford university press , 2008 . discusses artificial intelligence from the perspective of existential risk , introducing the term &quot; friendly ai &quot; . in particular , sections 1-4 give background to the definition of friendly ai in section 5 . section 6 gives two classes of mistakes ( technical and philosophical ) which would both lead to the accidental creation of non-friendly ais . sections 7-13 discuss further related issues . what is friendly ai ? — une brève description de l&apos; ia amicale par la machine intelligence research institute . commentary on miri&apos; s guidelines on friendly ai — par peter voss . the problem with ‘ friendly ’ artificial intelligence — par adam keiper et ari n. schulman .
