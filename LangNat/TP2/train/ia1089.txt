l ’ apprentissage par arbre de décision désigne une méthode basée sur l&apos; utilisation d&apos; un arbre de décision comme modèle prédictif . on l&apos; utilise notamment en fouille de données et en apprentissage automatique . dans ces structures d&apos; arbre , les feuilles représentent les valeurs de la variable-cible et les embranchements correspondent à des combinaisons de variables d&apos; entrée qui mènent à ces valeurs . en analyse de décision , un arbre de décision peut être utilisé pour représenter de manière explicite les décisions réalisées et les processus qui les amènent . en apprentissage et en fouille de données , un arbre de décision décrit les données mais pas les décisions elles-mêmes , l&apos; arbre serait utilisé comme point de départ au processus de décision . c&apos; est une technique d&apos; apprentissage supervisé : on utilise un ensemble de données pour lesquelles on connaît la valeur de la variable-cible afin de construire l&apos; arbre ( données dites étiquetées ) , puis on extrapole les résultats à l&apos; ensemble des données de test . template : ouvrage . son but est de créer un modèle qui prédit la valeur d&apos; une variable-cible depuis la valeur de plusieurs variables d&apos; entrée . une des variables d&apos; entrée est sélectionnée à chaque nœud intérieur ( ou interne , nœud qui n&apos; est pas terminal ) de l&apos; arbre selon une méthode qui dépend de l&apos; algorithme et qui sera discutée plus loin . chaque arête vers un nœud-fils correspond à un ensemble de valeurs d&apos; une variable d&apos; entrée , de manière à ce que l&apos; ensemble des arêtes vers les nœuds-fils couvrent toutes les valeurs possibles de la variable d&apos; entrée . chaque feuille ( ou nœud terminal de l&apos; arbre ) représente soit une valeur de la variable-cible , soit une distribution de probabilité des diverses valeurs possibles de la variable-cible . la combinaison des valeurs des variables d&apos; entrée est représentée par le chemin de la racine jusqu&apos; à la feuille . l&apos; arbre est en général construit en séparant l&apos; ensemble des données en sous-ensembles en fonction de la valeur d&apos; une caractéristique d&apos; entrée . ce processus est répété sur chaque sous-ensemble obtenu de manière récursive , il s&apos; agit donc d&apos; un partitionnement récursif . la récursion est achevée à un nœud soit lorsque tous les sous-ensembles ont la même valeur de la caractéristique-cible , ou lorsque la séparation n&apos; améliore plus la prédiction . ce processus est appelé induction descendante d&apos; arbres de décision ( top-down induction of decision trees ou tdidt ) quinlan , j. r. , ( 1986 ) . induction of decision trees . machine learning 1 : 81-106 , kluwer academic publishers , c&apos; est un algorithme glouton puisqu&apos; on recherche à chaque nœud de l&apos; arbre le partage optimal , dans le but d&apos; obtenir le meilleur partage possible sur l&apos; ensemble de l&apos; arbre de décision . c&apos; est la stratégie la plus commune pour apprendre les arbres de décision depuis les données . en fouille de données , les arbres de décision peuvent aider à la description , la catégorisation ou la généralisation d&apos; un jeu de données fixé . la variable y désigne la variable-cible que l&apos; on cherche à prédire , classer ou généraliser . le vecteur \ textbf { x } est constitué des variables d&apos; entrée x _ 1 , x _ 2 , x _ 3 etc. qui sont utilisées dans ce but . les arbres de régression ( regression tree ) permettent de prédire une quantité réelle ( par exemple , le prix d&apos; une maison ou la durée de séjour d&apos; un patient dans un hôpital ) , dans ce cas la prédiction est une valeur numérique . le terme d&apos; analyse d&apos; arbre de classification et de régression ( cart , d&apos; après l&apos; acronyme anglais ) est un terme générique se référant aux procédures décrites précédemment et introduites par breiman et al. template : ouvrage . les arbres utilisés dans le cas de la régression et dans le cas de la classification présentent des similarités mais aussi des différences , en particulier en ce qui concerne la procédure utilisée pour déterminer les séparations des branches . l&apos; apprentissage par arbre de décision consiste à construire un arbre depuis un ensemble d&apos; apprentissage constitué de n-uplets étiquetés . un arbre de décision peut être décrit comme un diagramme de flux de données ( ou flowchart ) où chaque nœud interne décrit un test sur une variable d&apos; apprentissage , chaque branche représente un résultat du test , et chaque feuille contient la valeur de la variable cible ( une étiquette de classe pour les arbres de classification , une valeur numérique pour les arbres de régression ) . usuellement , les algorithmes pour construire les arbres de décision sont construits en divisant l&apos; arbre du sommet vers les feuilles en choisissant à chaque étape une variable d&apos; entrée qui réalise le meilleur partage de l&apos; ensemble d&apos; objets , comme décrit précédemmenttemplate : article . pour choisir la variable de séparation sur un nœud , les algorithmes testent les différentes variables d&apos; entrée possibles et sélectionnent celle qui maximise un critère donné . dans le cas des arbres de classification , il s&apos; agit d&apos; un problème de classification automatique . le critère d ’ évaluation des partitions caractérise l&apos; homogénéité ( ou le gain en homogénéité ) des sous-ensembles obtenus par division de l&apos; ensemble . ces métriques sont appliquées à chaque sous-ensemble candidat et les résultats sont combinés ( par exemple , moyennés ) pour produire une mesure de la qualité de la séparation . il existe un grand nombre de critères de ce type , les plus utilisés sont l ’ entropie de shannon , le coefficient de gini et leurs variantes . dans le cas des arbres de régression , le même schéma de séparation peut être appliqué , mais au lieu de minimiser le taux d ’ erreur de classification , on cherche à maximiser la variance inter-classes ( avoir des sous-ensembles dont les valeurs de la variable-cible soient les plus dispersées possibles ) . en général , le critère utilise le test du chi carré . certains critères permettent de prendre en compte le fait que la variable-cible prend des valeurs ordonnées , en utilisant des mesures ou des heuristiques appropriéesdes heuristiques sont notamment utilisées lorsque l&apos; on cherche à réduire la complexité de l&apos; arbre en agrégeant les modalités des variables utilisées comme prédicteurs de la cible . par exemple , pour le cas des modalités d&apos; une variable de classes d&apos; âge , on ne va autoriser que des regroupements de classes d&apos; âge contiguës . . chaque ensemble de valeurs de la variable de segmentation permet de produire un nœud-fils . les algorithmes d ’ apprentissage peuvent différer sur le nombre de nœud-fils produits : certains ( tels que cart ) produisent systématiquement des arbres binaires , et cherchent donc la partition binaire qui optimise le critère de segmentation . d ’ autres ( comme chaid ) cherchent à effectuer les regroupements les plus pertinents en s ’ appuyant sur des critères statistiques . selon la technique , nous obtiendrons des arbres plus ou moins larges . pour que la méthode soit efficace , il faut éviter de fractionner exagérément les données afin de ne pas produire des groupes d ’ effectifs trop faibles , ne correspondant à aucune réalité statistique . dans le cas de variables de segmentation continues , le critère de segmentation choisi doit être adéquate . en général , on trie les données selon la variable à traiter , puis on teste les différents points de coupure possibles en évaluant le critère pour chaque cas , le point de coupure optimal sera celui qui maximise le critère de segmentation . il n&apos; est pas toujours souhaitable en pratique de construire un arbre dont les feuilles correspondent à des sous-ensembles parfaitement homogènes du point de vue de la variable-cible . en effet , l&apos; apprentissage est réalisé sur un échantillon que l&apos; on espère représentatif d ’ une population . l&apos; enjeu de toute technique d&apos; apprentissage est d&apos; arriver à saisir l&apos; information utile sur la structure statistique de la population , en excluant les caractéristiques spécifiques au jeu de données étudié . plus le modèle est complexe ( plus l&apos; arbre est grand , plus il a de branches , plus il a de feuilles ) , plus l&apos; on court le risque de voir ce modèle incapable d&apos; être extrapolé à de nouvelles données , c&apos; est-à-dire de rendre compte de la réalité que l&apos; on cherche à appréhender . en particulier , dans le cas extrême où l&apos; arbre a autant de feuilles qu&apos; il y a d&apos; individus dans la population ( d&apos; enregistrements dans le jeu de données ) , l&apos; arbre ne commet alors aucune erreur sur cet échantillon puisqu&apos; il en épouse toutes les caractéristiques , mais il n&apos; est pas généralisable à un autre échantillon . ce problème , nommé surapprentissage ou surajustement ( overfitting ) , est un sujet classique de l&apos; apprentissage automatique et de la fouille de données . on cherche donc à construire un arbre qui soit le plus petit possible en assurant la meilleure performance possible . suivant le principe de parcimonie , plus un arbre sera petit , plus il sera stable dans ses prévisions futures . il faut réaliser un arbitrage entre performance et complexité dans les modèles utilisés . à performance comparable , on préférera toujours le modèle le plus simple , si l&apos; on souhaite pouvoir utiliser ce modèle sur de nouveaux échantillons . pour réaliser l&apos; arbitrage performance / complexité des modèles utilisés , on évalue la performance d&apos; un ou de plusieurs modèles sur les données qui ont servi à sa construction ( le ou les échantillons d&apos; apprentissage ) , mais également sur un ( ou plusieurs ) échantillon ( s ) de validation : des données étiquetées à disposition mais que l&apos; on décide volontairement de ne pas utiliser dans la construction des modèles . ces données sont traitées comme les données de test , la stabilité de la performance des modèles sur ces deux types d&apos; échantillon permettra de juger de son surajustement et donc de sa capacité à être utilisé avec un risque d&apos; erreur maîtrisé dans des conditions réelles où les données ne sont pas connues à l&apos; avance . dans le graphique ci-contre , nous observons l ’ évolution de l ’ erreur d ’ ajustement d&apos; un arbre de décision en fonction du nombre de feuilles de l ’ arbre ( qui mesure ici la complexité ) . nous constatons que si l ’ erreur décroît constamment sur l ’ échantillon d ’ apprentissage , à partir d&apos; un certain niveau de complexité , le modèle s&apos; éloigne de la réalité , réalité que l ’ on cherche à estimer sur l&apos; échantillon de validation ( appelé échantillon de test dans le graphique ) . dans le cas des arbres de décisions , plusieurs types de solutions algorithmiques ont été envisagées pour tenter d&apos; éviter autant que possible le surapprentissage des modèles : les techniques de pré- ou de post-élagage des arbres . certaines théories statistiques cherchent à trouver l&apos; optimum entre l&apos; erreur commise sur l&apos; échantillon d&apos; apprentissage et celle commise sur l&apos; échantillon de test . la théorie de vapnik-chervonenkis structured risk minimization ( ou srm ) , utilise une variable appelée dimension vc , pour déterminer l ’ optimum d ’ un modèle . elle est utilisable par conséquent pour générer des modèles qui assurent le meilleur compromis entre qualité et robustesse du modèle . ces solutions algorithmiques sont complémentaires des analyses de performances comparées et de stabilité effectuées sur les échantillons d&apos; apprentissage et de validation . la première stratégie utilisable pour éviter un surapprentissage des arbres de décision consiste à proposer des critères d ’ arrêt lors de la phase d ’ expansion . c ’ est le principe du pré-élagage . lorsque le groupe est d ’ effectif trop faible , ou lorsque l&apos; homogénéité d&apos; un sous-ensemble a atteint un niveau suffisant , on considère qu ’ il n ’ est plus nécessaire de séparer l&apos; échantillon . un autre critère souvent rencontré dans ce cadre est l ’ utilisation d ’ un test statistique pour évaluer si la segmentation introduit un apport d ’ informations significatif pour la prédiction de la variable-cible . la seconde stratégie consiste à construire l ’ arbre en deux temps : on produit d&apos; abord l ’ arbre dont les feuilles sont le plus homogènes possibles dans une phase d ’ expansion , en utilisant une première fraction de l ’ échantillon de données ( échantillon d ’ apprentissage à ne pas confondre avec la totalité de l ’ échantillon , appelé en anglais growing set pour lever l&apos; ambiguïté ) , puis on réduit l ’ arbre , en s ’ appuyant sur une autre fraction des données de manière à optimiser les performances de l ’ arbre , c ’ est la phase de post-élagage . selon les cas , cette seconde portion des données est désignée par le terme d ’ échantillon de validation ou échantillon de test , introduisant une confusion avec l ’ échantillon utilisé pour mesurer les performances des modèles . le terme d&apos; échantillon d ’ élagage permet de le désigner sans ambiguïté , c&apos; est la traduction directe de l ’ appellation anglaise pruning set . les ignorer : cela n&apos; est possible que si l&apos; échantillon de données est suffisamment grand pour supprimer des individus ( c&apos; est-à-dire des lignes d&apos; enregistrements ) du jeu de données , et que si l&apos; on est sûr que lorsque l&apos; arbre de décision sera utilisé en pratique , toutes les données seront toujours disponibles pour tous les individus . les remplacer par une valeur calculée jugée adéquate ( on parle d&apos; imputation de valeurs manquantes ) : cette technique est parfois utilisée en statistique mais au-delà des problèmes purement mathématiques , elle est contestable du point de vue méthodologique . utiliser des variables substituts : cela consiste , pour un individu qui aurait une donnée manquante pour une variable sélectionnée par l&apos; arbre comme étant discriminante , à utiliser la variable qui parmi l&apos; ensemble des variables disponibles dans la base de données produit localement les feuilles les plus similaires aux feuilles produites par la variable dont la donnée est manquante , on appelle alors cette variable un substitut . si un individu a une valeur manquante pour la variable initiale , mais aussi pour la variable substitut , on peut utiliser une deuxième variable substitut . et ainsi de suite , jusqu&apos; à la limite d&apos; un critère de qualité du substitut . cette technique a l&apos; avantage d&apos; exploiter l&apos; ensemble de l&apos; information disponible ( cela est donc très utile lorsque cette information est complexe à récupérer ) pour chaque individu . dans le cas des arbres de classification , il faut préciser la règle d ’ affectation dans les feuilles une fois l ’ arbre construit . si les feuilles sont homogènes , il n&apos; y a pas d&apos; ambiguïté . si ce n ’ est pas le cas , une règle simple consiste à décider de la classe de la feuille en fonction de la classe majoritaire , celle qui est la plus représentée . cette technique très simple est optimale dans le cas où les données sont issues d ’ un tirage aléatoire non-biasé dans la population ; la matrice des coûts de mauvaise affectation est unitaire ( symétrique ) : affecter à bon escient à un coût nul , et affecter à tort coûte 1 quel que soit le cas de figure . en dehors de ce cadre , la règle de la majorité n ’ est pas nécessairement justifiée mais est facile à utiliser dans la pratique . l&apos; ensachage ( bagging ou bootstrap aggregating ) , une des premières méthodes historiquement , selon laquelle on construit plusieurs arbres de décision en ré-échantillonnant l&apos; ensemble d&apos; apprentissage , puis en construisant les arbres par une procédure de consensusbreiman , l. ( 1996 ) . bagging predictors . &quot; machine learning , 24 &quot; : template : p. . . la classification par forêts d&apos; arbres aléatoires de breiman . le boosting d&apos; arbre de classification et de régressionfriedman , j. h. ( 1999 ) . stochastic gradient boosting . stanford university . , hastie , t. , tibshirani , r. , friedman , j. h. ( 2001 ) . the elements of statistical learning : data mining , inference , and prediction . new york : springer verlag . . la classification par rotation de forêts d&apos; arbres de décision , dans laquelle on applique d&apos; abord une analyse en composantes principales ( pca ) sur un ensemble aléatoire des variables d&apos; entréerodriguez , j.j. and kuncheva , l.i. and alonso , c.j. ( 2006 ) , rotation forest : a new classifier ensemble method , ieee transactions on pattern analysis and machine intelligence , 28 ( 10 ) : 1619-1630 . . les arbres de décision sont parfois combinés entre eux ou à d&apos; autres techniques d&apos; apprentissage : analyse discriminante , régressions logistiques , régressions linéaires , réseaux de neurones ( perceptron multicouche , radial basis function network ) ou autres . des procédures d&apos; agrégation des performances des différents modèles utilisés ( telles que les décisions par consensus ) , sont mises en place pour obtenir une performance maximale , tout en contrôlant le niveau de complexité des modèles utilisés . la simplicité de compréhension et d&apos; interprétation . c&apos; est un modèle boîte blanche : si l&apos; on observe une certaine situation sur un modèle , celle-ci peut-être facilement expliquée à l&apos; aide de la logique booléenne , au contraire de modèles boîte noire comme les réseaux neuronaux , dont l&apos; explication des résultats est difficile à comprendre . peu de préparation des données ( pas de normalisation , de valeurs vides à supprimer , ou de variable muette ) . le modèle peut gérer à la fois des valeurs numériques et des catégories . d&apos; autres techniques sont souvent spécialisées sur un certain type de variables ( les réseaux neuronaux ne sont utilisables que sur des variables numériques ) . il est possible de valider un modèle à l&apos; aide de tests statistiques , et ainsi de rendre compte de la fiabilité du modèle . performant sur de grands jeux de données : la méthode est relativement économique en termes de ressources de calcul . &quot; efficient construction of decision trees by the dual information distance method &quot; . quality technology &amp; quantitative management ( qtqm ) , 11 ( 1 ) , 133-147 . ( disponible en ligne en anglais pdf ) . l&apos; apprentissage par arbre de décision peut amener des arbres de décision très complexes , qui généralisent mal l&apos; ensemble d&apos; apprentissage ( il s&apos; agit du problème de surapprentissage précédemment évoquétemplate : doi ) . on utilise des procédures d&apos; élagage pour contourner ce problème , certaines approches comme l&apos; inférence conditionnelle permettent de s&apos; en affranchirtemplate : article , template : article . certains concepts sont difficiles à exprimer à l&apos; aide d&apos; arbres de décision ( comme xor ou la parité ) . dans ces cas , les arbres de décision deviennent extrêmement larges . pour résoudre ce problème , plusieurs moyens existent , tels que la proportionnalisationtemplate : doi , ou l&apos; utilisation d&apos; algorithmes d&apos; apprentissage utilisant des représentations plus expressives ( par exemple la programmation logique inductive ) . lorsque les données incluent des attributs ayant plusieurs niveaux , le gain d&apos; information dans l&apos; arbre est biaisé en faveur de ces attributstemplate : lien conférence . cependant , le problème de la sélection de prédicteurs biaisés peut être contourné par des méthodes telles que l&apos; inférence conditionnelle . dans un arbre de décision , tous les chemins depuis la racine jusqu&apos; aux feuilles utilisent le connecteur and . dans un graphe de décision , on peut également utiliser le connecteur or pour connecter plusieurs chemins à l&apos; aide du minimum message length ( mml ) http : / / citeseer.ist.psu.edu / oliver93decision.html. en général les graphes de décision produisent des graphes avec moins de feuilles que les arbres de décision . des algorithmes évolutionnistes sont utilisés pour éviter les séparations amenant à des optimum locauxpapagelis a. , kalles d. ( 2001 ) . breeding decision trees using evolutionary techniques , proceedings of the eighteenth international conference on machine learning , template : p. , june 28-july 01 , 2001 , barros , rodrigo c. , basgalupp , m. p. , carvalho , a. c. p. l. f. , freitas , alex a. ( 2011 ) . a survey of evolutionary algorithms for decision-tree induction . ieee transactions on systems , man and cybernetics , part c : applications and reviews , vol. 42 , n. 3 , template : p. , may 2012 . . on peut également échantillonner l&apos; arbre en utilisant des méthodes mcmc dans un paradigme bayésienchipman , hugh a. , edward i. george , and robert e. mcculloch . &quot; bayesian cart model search . &quot; journal of the american statistical association 93.443 ( 1998 ) : 935-948 . . l&apos; arbre peut être construit par une approche ascendante ( du bas vers le haut ) barros r. c. , cerri r. , jaskowiak p. a. , carvalho , a. c. p. l. f. , a bottom-up oblique decision tree induction algorithm . proceedings of the template : 11th international conference on intelligent systems design and applications ( isda 2011 ) . . conditional inference trees . une méthode statistique basée sur l&apos; utilisation de tests non-paramétriques comme critère de séparation , . id3 et cart ont été inventées de manière indépendante dans les décennies 1970-1980 , mais utilisent des approches similaires pour apprendre des arbres de décision depuis l&apos; ensemble d&apos; apprentissage . tous ces algorithmes se distinguent par le ou les critères de segmentation utilisés , par les méthodes d&apos; élagages implémentées , par leur manière de gérer les données manquantes dans les prédicteurs . beaucoup de logiciels de fouille de données proposent des bibliothèques permettant d&apos; implémenter un ou plusieurs algorithmes d&apos; apprentissage par arbre de décision . par exemple , le logiciel open source r contient plusieurs implémentations de cart , telles que rpart , party et randomforest , les logiciels libres weka et orange ( et son module orngtree ) ou encore la bibliothèque libre python scikit-learn ; mais également salford systems cart , ibm spss modeler , rapidminer , sas enterprise miner , knime , microsoft sql server &#91; 1 &#93; . cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé decision tree learning . l. breiman , j. friedman , r. olshen , c. stone : cart : classification and regression trees , wadsworth international , 1984 . r. quinlan : c4.5 : programs for machine learning , morgan kaufmann publishers inc . , 1993 . d. zighed , r. rakotomalala : graphes d&apos; induction -- apprentissage et data mining , hermes , 2000 . daniel t. larose ( adaptation française t. vallaud ) : des données à la connaissance : une introduction au data-mining ( 1cédérom ) , vuibert , 2005 . manuel de statistiques en ligne , ( en anglais ) . une introduction aux arbres de décision .
