vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants . en statistique , l&apos; algorithme de métropolis-hastings est une méthode mcmc . étant donnée une distribution de probabilité \ pi sur un univers \ omega , cet algorithme définit une chaîne de markov dont la distribution stationnaire est \ pi . il permet ainsi de tirer aléatoirement un élément de \ omega selon la loi \ pi ( on parle d&apos; échantillonnage ) . un point essentiel de l&apos; algorithme de métropolis-hasting est qu&apos; il ne nécessite que la connaissance de \ pi à constante multiplicative près . en particulier , il n&apos; est pas nécessaire de calculer la fonction de partition de \ pi , tâche souvent difficile . pour cette raison , cette méthode est très utilisée en physique statistique . la première version de l&apos; algorithme est décrite dans un article de 1953 par nicholas metropolis , arianna w. rosenbluth , marshall rosenbluth ( en ) catégorie : article contenant un appel à traduction en anglais , augusta h. teller , et edward tellertemplate : article . . cette première version considérait le cas particulier de la distribution de boltzmann , une des distributions les plus utilisées en physique statistique . en 1970 , w. keith hastings ( 1930- ) a étendu l&apos; algorithme au cas de n&apos; importe quelle distributiontemplate : article . . nous voulons obtenir des tirages aléatoires d&apos; un vecteur x , ayant un grand nombre de dimensions — pour une distribution à une dimension , on utilise d&apos; autres algorithmes plus directs comme la méthode de rejet — , avec une distribution de probabilité π . nous sommes dans le cas où il n&apos; est pas simple de générer directement une suite de vecteurs suivant cette distribution π . par ailleurs , on ne connaît pas nécessairement cette distribution π , il suffit de connaître une fonction ƒ ( x ) qui est proportionnelle à π ( x ) . on part d&apos; une valeur x0 . à partir de cette valeur , on détermine une valeur x &apos; avec un générateur pseudo-aléatoire utilisant une distribution de probabilité q. dans le cas de l&apos; algorithme original de metropolis , q est symétrique ( on prend par exemple une distribution normale centrée sur x0 ) ; hastings a généralisé cet algorithme à une distribution q dissymétrique. α = ƒ ( x &apos; ) / ƒ ( x0 ) = p ( x &apos; ) / p ( x0 ) . et l&apos; on recommence de manière récursive . on a donc une chaîne de markov , puisque l&apos; état de xi ne dépend que de xi - 1 ; et après un « grand nombre » d&apos; itérations , les xi suivent la distribution π . calculer la probabilité d ’ acceptation \ alpha ( x _ t , y _ { t + 1 } ) = \ min \ left \ { \ frac { \ pi ( y _ { t + 1 } ) q ( y _ { t + 1 } , x _ t ) } { \ pi ( x _ t ) q ( x _ t , y _ { t + 1 } ) } , 1 \ right \ } \ , \ ! . en recommençant ces étapes pour t allant de 0 à nvanessa bergeron laperrière ( été 2010 ) , ( supervisée par mylène bédard ) , l ’ algorithme metropolis-hastings projet de recherche crsng , département de mathématiques et statistique université de montréal . . un cas particulier courant de l&apos; algorithme est celui où q est symétrique ( i.e. , q ( x , y ) = q ( y , x ) ) . avec probabilité \ frac { \ pi ( x ) } { \ pi ( x _ t ) } sinon ( et reste en x _ t avec la probabilité restante ) .
