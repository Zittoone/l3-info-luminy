l&apos; algorithme du gradient stochastique est une méthode de descente de gradient ( itérative ) utilisée pour la minimisation d&apos; une fonction objectif qui est écrite comme une somme de fonctions différentiables . article connexe : m-estimateur. où le paramètre w ^ * qui minimise q ( w ) doit être estimé . chacune des fonctions q _ i est généralement associée avec la i-ème observation de l&apos; ensemble des données ( utilisées pour l&apos; apprentissage ) . en statistique classique , les problèmes de minimisation de sommes apparaissent notamment dans la méthode des moindres carrés et dans la méthode de maximum de vraisemblance ( pour des observations indépendantes ) . les estimateurs qui apparaissent alors comme minimiseurs de sommes sont appelés m-estimateur . cependant , en statistique , il est su depuis longtemps qu&apos; exiger ne serait-ce qu&apos; une minimisation locale est trop restrictive pour certains problèmes d&apos; estimation de maximum de vraisemblance , comme montré dans le célèbre exemple de thomas fergusontemplate : article . ainsi , les théoriciens des statistiques modernes considèrent souvent les points stationnaires de la fonction de vraisemblance ( ou bien les zéros de sa dérivée , la fonction score , et d&apos; autres équations d&apos; estimation ) . le problème de la minimisation d&apos; une somme se retrouve aussi dans la minimisation du risque empirique : dans ce cas , q _ i ( w ) est la valeur de la fonction objectif pour le i-ème exemple , et q ( w ) est le risque empirique. où \ eta est le pas de l&apos; itération ( parfois appelé le taux d&apos; apprentissage en apprentissage automatique ) . très souvent , les fonctions élémentaires constituant la somme revêtent une forme simple qui permet le calcul efficace de la fonction somme et de son gradient ( qui n&apos; est autre que la somme des gradients des fonctions élémentaires ) . par exemple , en statistique , les familles exponentielle ( à un paramètre ) permettent une évaluation efficace de la fonction et de son gradient . cependant , dans d&apos; autres cas , évaluer le gradient entier peut devenir très couteux , lorsque le calcul des gradients de chaque morceau n&apos; est pas simple . et si l&apos; ensemble d&apos; apprentissage est très large ( big data ) et qu&apos; aucune formule simple n&apos; est disponible pour les gradients , calculer la somme des gradients peut rapidement devenir très couteux . c&apos; est dans le but d&apos; améliorer le coût de calcul de chaque étape que la méthode de descente de gradient stochastique a été mise au point . en effet , à chaque étape , cette méthode tire un échantillon aléatoire de l&apos; ensemble des fonctions q _ i constituant la somme . cette astuce devient très efficace dans le cas de problème d&apos; apprentissage à grande ou très grande échellestemplate : lien conférence . alors que l&apos; algorithme parcourt l&apos; ensemble d&apos; apprentissage , il réalise cette étape de mise-à-jour pour chaque exemple . plusieurs parcours de l&apos; ensemble d&apos; apprentissage peuvent être nécessaires avant la convergence de la méthode . si cela est nécessaire , les données sont habituellement mélangées à chaque parcours , afin d&apos; éviter les cycles . une autre astuce souvent mise en place en pratique consiste à utiliser un taux d&apos; apprentissage adaptatif afin d&apos; améliorer ou d&apos; assurer la convergence de l&apos; algorithme . un compromis entre les deux formes est appelé &quot; mini-lots &quot; : au lieu de calculer le gradient d&apos; un seul exemple ou le gradient de tous les exemples , la méthode calcul à chaque étape le gradient sur plusieurs exemples ( des petits lots , d&apos; où le nom ) . cette variante peut être bien plus efficace que la méthode simple de descente de gradient stochastique , parce qu&apos; une implémentation bien écrite peut utiliser des bibliothèques de calcul vectoriel , au lieu de calculer chaque étape séparément . la variante &quot; mini-lots &quot; peut aussi présenter une convergence plus lisse , comme le gradient calculé à chaque étape utilise plus d&apos; exemples d&apos; apprentissage . la convergence de l&apos; algorithme de descente de gradient stochastique a été analysée via les théories de l&apos; optimisation convexe et de l&apos; approximation stochastique . en bref , lorsque l&apos; on peut assurer que le taux d&apos; apprentissage \ eta est décroissant ( avec une vitesse suffisante ) , et vérifie certaines hypothèses de régularité , alors l&apos; algorithme converge presque sûrement vers un minimum global , si la fonction coût est convexe ou pseudo-convexe , ou sinon converge presque sûrement vers un minimum localtemplate : ouvrage , template : article . ce résultat peut aussi être obtenu comme une application du théorème de robbins-siegmundtemplate : ouvrage . q ( w ) = \ sum _ { i = 1 } ^ n q _ i ( w ) = \ sum _ { i = 1 } ^ n \ left ( w _ 1 + w _ 2 x _ i - y _ i \ right ) ^ 2 . la descente de gradient stochastique est un algorithme très utilisé pour l&apos; entraînement de nombreuses familles de modèles en apprentissage automatique , notamment les machines à vecteurs support ( linéaires ) , la régression logistique ( voir par exemple vowpal wabbit ) et les modèles graphiquesjenny rose finkel , alex kleeman , christopher d. manning ( 2008 ) . . lorsqu&apos; elle est combinée à l&apos; algorithme de propagation inverse , la méthode obtenue est l&apos; algorithme standard de facto , le plus utilisé pour l&apos; entraînement des réseaux neuronaux artificiels . la méthode dgs ( sgd en anglais ) est en compétition directe avec l&apos; algorithme l-bfgs , &#91; citation nécessaire &#93; catégorie : article à citation nécessaire qui lui aussi est très utilisé . sgd a été utilisé depuis les années 60 ( au moins ) , pour entraîner des modèles de régression linéaire , initialement sous le nom adalinetemplate : lien web . un autre algorithme populaire de descente de gradient stochastique est le filtre moyen des moindres carrés ( lms ) adaptatif . de nombreuses améliorations sur l&apos; algorithme sgd initial ont été proposées et utilisées . en particulier , en apprentissage automatique , la nécessité de fixer un taux d&apos; apprentissage ( le pas de l&apos; algorithme ) a souvent été décrite comme problématique . un choix de paramètre trop grand peut rendre l&apos; algorithme divergent tandis qu&apos; un choix trop petit rend la convergence trop lente . une extension de sgd relativement simple consiste à choisir comme taux d&apos; apprentissage une fonction \ eta _ t décroissante en fonction du nombre d&apos; itération t , donnant une progression du taux d&apos; apprentissage , de sorte que les premières itérations permettent de forts changements dans les paramètres , tandis que les itérations tardives ne feront que les peaufiner légèrement . de telles progressions décroissantes sont connues depuis les travaux de macqueen sur les k-moyennescited by template : lien conférence . parmi les autres propositions , on notera notamment la méthode du moment , qui apparaît dans un papier de rumelhart , hinton et williams sur l&apos; apprentissage par rétro-propagationtemplate : article . le nom moment vient d&apos; une analogie avec le moment en physique : le vecteur de paramètres w , considéré comme une particule qui voyage au travers de l&apos; espace des paramètres ( souvent &lt; math &gt; \ mathbb { r } ^ d &lt; / math &gt; en grande dimension ) r , subit une accélération via le gradient de la perte ( qui agit comme une &quot; force &quot; ) . contrairement à la méthode sgd classique , cette variante a tendance à continuer de voyager dans la même direction , en empêchant les oscillations . la méthode sgd moment a été utilisé avec succès depuis plusieurs dizaines d&apos; années template : lien arxiv . \ bar { w } = \ frac { 1 } { t } \ sum _ { i = 0 } ^ { t-1 } w. lorsque l&apos; optimisation est terminée , ce vecteur moyenné de paramètres sera utilisé à la place de w. où g _ \ tau = \ nabla q _ i ( w ) est le gradient à l&apos; étape \ tau . la diagonale est donnée par : g _ { j , j } = \ sum _ { \ tau = 1 } ^ t g _ { \ tau , j } ^ 2 . ce vecteur est mis-à-jour à chaque itération. w : = w - \ eta \ , \ mathrm { diag } ( g ) ^ { - \ frac { 1 } { 2 } } \ circ g \ circ est le produit matriciel. w _ j : = w _ j - \ frac { \ eta } \ sqrt { g _ { j , j } } g _ j. chaque g _ { j , j } donne un facteur multiplicatif appliqué au taux d&apos; apprentissage correspondant à l&apos; unique paramètre w _ i. et comme le dénominateur de ce facteur ( \ sqrt { g _ i } = \ sqrt { \ sum _ { \ tau = 1 } ^ t g _ \ tau ^ 2 } ) est la norme ℓ2 des dérivées précédentes , les mises-à-jour trop importantes des paramètres sont atténuées tandis que les petites modifications sont faites avec un taux d&apos; apprentissage plus grand ( et donc sont amplifiées ) . alors qu&apos; elle fut initialement pensée pour des problèmes convexes , adagrad a été appliquée avec succès à l&apos; optimisation non-convexetemplate : article . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « stochastic gradient descent » ( voir la liste des auteurs ) . krzysztof c. kiwiel , « convergence of approximate and incremental subgradient methods for convex optimization » , siam journal of optimization , vol. 14 , no 3 , ‎ &lt; time &gt; 2003 &lt; / time &gt; , p. 807 – 840 ( doi 10.1137 / s1052623400376366 ) ( contient une liste de références très fournie ) . sgd : une bibliothèque c + + ( sous licence lgpl ) qui utilise la descente de gradient stochastique pour entraîner des mvs et des modèles conditional random field . crf-adf : une bibliothèque c # implémentant la méthode de descente de gradient stochastique et sa variation adagrad pour entraîner des modèles conditional random field . vowpal wabbit : une solution d&apos; apprentissage rapide et scalable ( sous licence bsd ) par john langford et d&apos; autres contributeurs . contient notamment quelques variantes de l&apos; algorithme de descente de gradient stochastique . le dépôt source est sur github .
