dans le domaine des mathématiques et des statistiques , et plus particulièrement dans le domaine de l&apos; apprentissage automatique , la régularisation fait référence à un processus consistant à ajouter de l&apos; information à un problème pour éviter le surapprentissage . cette information prend généralement la forme d&apos; une pénalité envers la complexité du modèle . on peut relier cette méthode au principe du rasoir d&apos; occam . d&apos; un point de vue bayésien , l&apos; utilisation de la régularisation revient à imposer une distribution a priori sur les paramètres du modèle . une méthode généralement utilisée est de pénaliser les valeurs extrêmes des paramètres , qui correspondent souvent à un surapprentissage . pour cela , on va utiliser une norme sur ces paramètres , qu&apos; on va rajouter à la fonction qu&apos; on cherche à minimiser . les normes les plus couramment employées pour cela sont l ₁ et l ₂ . l ₁ offre l&apos; avantage de revenir à faire une sélection de paramètres , mais elle n&apos; est pas différentiable , ce qui peut être un inconvénient pour les algorithmes utilisant un calcul de gradient pour l&apos; optimisationandrew , galen ; gao , jianfeng ( 2007 ) . &quot; scalable training of l ₁ -regularized log-linear models &quot; . proceedings of the 24th international conference on machine learning. doi : 10.1145 / 1273496.1273501 . template : isbn . , tsuruoka , y. ; tsujii , j. ; ananiadou , s. ( 2009 ) . &quot; stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty &quot; . proceedings of the afnlp / acl . .
