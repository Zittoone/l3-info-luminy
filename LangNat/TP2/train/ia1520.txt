en statistiques , la technique de rétropropagation du gradient est une méthode qui permet de calculer le gradient de l&apos; erreur pour chaque neurone d&apos; un réseau de neurones , de la dernière couche vers la première . de façon abusive , on appelle souvent technique de rétropropagation du gradient l&apos; algorithme classique de correction des erreurs basé sur le calcul du gradient grâce à la rétropropagation et c&apos; est cette méthode qui est présentée ici . en vérité , la correction des erreurs peut se faire selon d&apos; autres méthodes , en particulier le calcul de la dérivée seconde . cette technique consiste à corriger les erreurs selon l&apos; importance des éléments qui ont justement participé à la réalisation de ces erreurs . dans le cas des réseaux de neurones , les poids synaptiques qui contribuent à engendrer une erreur importante se verront modifiés de manière plus significative que les poids qui ont engendré une erreur marginale . ce principe fonde les méthodes de type algorithme du gradient , qui sont efficacement utilisées dans des réseaux de neurones multicouches comme les perceptrons multicouches . l&apos; algorithme du gradient a pour but de converger de manière itérative vers une configuration optimisée des poids synaptiques . cet état peut être un minimum local de la fonction à optimiser et idéalement , un minimum global de cette fonction ( dite fonction de coût ) . normalement , la fonction de coût est non linéaire au regard des poids synaptiques . elle dispose également d&apos; une borne inférieure et moyennant quelques précautions lors de l&apos; apprentissage , les procédures d&apos; optimisation finissent par aboutir à une configuration stable au sein du réseau de neurones . les méthodes de rétropropagation du gradient firent l&apos; objet de communications dès 1975 ( werbos ) , puis 1985 ( parker et lecun ) , mais ce sont les travaux de rumelhart , hinton et williams en 1986 qui suscitèrent le véritable début de l&apos; engouement pour cette méthodepatrick van der smagt , an introduction to neural networks , 1996 , page 33 . . dans le cas d&apos; un apprentissage supervisé , des données sont présentées à l&apos; entrée du réseau de neurones et celui-ci produit des sorties . la valeur des sorties dépend des paramètres liés à la structure du réseau de neurones : connectique entre neurones , fonctions d&apos; agrégation et d&apos; activation ainsi que les poids synaptiques . les différences entre ces sorties et les sorties désirées forment des erreurs qui sont corrigées via la rétropropagation , les poids du réseau de neurones sont alors changés . la manière de quantifier cette erreur peut varier selon le type d&apos; apprentissage à effectuer . en appliquant cette étape plusieurs fois , l&apos; erreur tend à diminuer et le réseau offre une meilleure prédiction . il se peut toutefois qu&apos; il ne parvienne pas à échapper à un minimum local , c&apos; est pourquoi on ajoute en général un terme d&apos; inertie ( momentum ) à la formule de la rétropropagation pour aider l&apos; algorithme du gradient à sortir de ces minimums locaux . la propagation vers l&apos; avant se calcule à l&apos; aide de la fonction d&apos; activation g , de la fonction d&apos; agrégation h ( souvent un produit scalaire entre les poids et les entrées du neurone ) et des poids synaptiques \ vec { w } _ { jk } entre le neurone x _ k ^ { ( n-1 ) } et le neurone x _ j ^ { ( n ) } . attention au passage à cette notation qui est inversée , \ vec { w } _ { jk } indique bien un poids de k vers j. on voit qu&apos; en définissant la suite des e ^ { ( n ) } _ i comme nous l&apos; avons fait , on peut facilement obtenir la dérivée de l&apos; énergie par rapport aux poids synaptiques d&apos; un neurone à distance n-l de la sortie . l&apos; algorithme présenté ici est de type « online » , c&apos; est-à-dire que l&apos; on met à jour les poids pour chaque échantillon d&apos; apprentissage présenté dans le réseau de neurones . une autre méthode est dite en « batch » , c&apos; est-à-dire que l&apos; on calcule d&apos; abord les erreurs pour tous les échantillons sans mettre à jour les poids ( on additionne les erreurs ) et lorsque l&apos; ensemble des données est passé une fois dans le réseau , on applique la rétropropagation en utilisant l&apos; erreur totale . cette façon de faire est préférée pour des raisons de rapidité et de convergence . l&apos; algorithme est itératif et la correction s&apos; applique autant de fois que nécessaire pour obtenir une bonne prédiction . il faut cependant veiller aux problèmes de surapprentissage liés à un mauvais dimensionnement du réseau ou un apprentissage trop poussé. avec \ alpha un paramètre compris entre 0 et 1.0 . il a été découvert un phénomène biologique équivalent à la rétropropagation d&apos; information dans les réseaux de neurones : c&apos; est la rétropropagation neuronale .
