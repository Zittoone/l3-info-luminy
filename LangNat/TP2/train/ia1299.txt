améliorez sa vérifiabilité en les associant par des références à l&apos; aide d&apos; appels de notes . pour les articles homonymes , voir dss et bi . l ’ informatique décisionnelle ( en anglais business intelligence ou bibusiness intelligence ne signifie pas intelligence économique , contrairement à ce que laisserait croire une traduction littérale , voir la section intelligence économique et business intelligence de l&apos; article intelligence économique ) est l&apos; informatique à l&apos; usage des décideurs et des dirigeants d&apos; entreprises . elle désigne les moyens , les outils et les méthodes qui permettent de collecter , consolider , modéliser et restituer les données , matérielles ou immatérielles , d&apos; une entreprise en vue d&apos; offrir une aide à la décision et de permettre à un décideur d ’ avoir une vue d ’ ensemble de l ’ activité traitée . ce type d ’ application repose sur une architecture commune dont les bases théoriques viennent principalement de r. kimball , b. inmon et d. linstedt . les données opérationnelles sont extraites périodiquement de sources hétérogènes : fichiers plats , fichiers excel , base de données ( db2 , oracle , sql server , etc. ) , service web , données massives et stockées dans un entrepôt de données . les données sont restructurées , enrichies , agrégées , reformatées , nomenclaturées pour être présentées à l&apos; utilisateur sous une forme sémantique ( vues métiers ayant du sens ) qui permettent aux décideurs d&apos; interagir avec les données sans avoir à connaître leur structure de stockage physique , de schémas en étoile qui permettent de répartir les faits et mesures selon des dimensions hiérarchisées , de rapports pré-préparés paramétrables , de tableaux de bords plus synthétiques et interactifs . ces données sont livrées aux divers domaines fonctionnels ( direction stratégique , finance , production , comptabilité , ressources humaines , etc. ) à travers un système de sécurité ou de datamart spécialisés à des fins de consultations , d&apos; analyse , d&apos; alertes prédéfinies , d&apos; exploration de données , etc. l ’ informatique décisionnelle s ’ insère dans l ’ architecture plus large d ’ un système d&apos; information mais n&apos; est pas un concept concurrent du management du système d&apos; information . au même titre que le management relève de la sociologie et de l&apos; économie , la gestion par l&apos; informatique est constitutive de deux domaines radicalement différents que sont le management et l&apos; informatique . afin d&apos; enrichir le concept avec ces deux modes de pensées , il est possible d&apos; envisager un versant orienté ingénierie de l&apos; informatique portant le nom dinformatique décisionnelle , et un autre versant servant plus particulièrement les approches de gestion appelé management du système d&apos; information . actuellement , les données applicatives métier sont stockées dans une ( ou plusieurs ) base ( s ) de données relationnelle ( s ) ou non relationnelles . ces données sont extraites , transformées et chargées dans un entrepôt de données généralement par un outil de type etl ( extract-transform-load ) ou en français etc ( extraction-transformation-chargement ) . un entrepôt de données peut prendre la forme d ’ un data warehouse ou d ’ un datamart . en règle générale , le data warehouse globalise toutes les données applicatives de l ’ entreprise , tandis que les datamarts ( généralement alimentés depuis les données du data warehouse ) sont des sous-ensembles d ’ informations concernant un métier particulier de l ’ entreprise ( marketing , risque , contrôle de gestion … ) , des usages spécifiques ( analyse , reporting ... ) , ou encore répondent à des exigences ou contraintes particulières ( cloisonnement des données , volumétrie ... ) . le terme comptoir de données ou magasin de données est aussi utilisé pour désigner un datamart . les entrepôts de données permettent de produire des rapports qui répondent à la question « que s ’ est-il passé ? » , mais ils peuvent être également conçus pour répondre à la question analytique « pourquoi est-ce que cela s ’ est passé ? » et à la question pronostique « que va-t-il se passer ? » . dans un contexte opérationnel , ils répondent également à la question « que se passe-t-il en ce moment ? » , voire dans le cas d ’ une solution d ’ entrepôt de données actif « que devrait-il se passer ? » . de sélectionner des données relatives à telle période , telle production , tel secteur de clientèle , etc. les programmes utilisés pour le reporting permettent bien sûr de reproduire de période en période les mêmes sélections et les mêmes traitements et de faire varier certains critères pour affiner l ’ analyse . mais le reporting n&apos; est pas à proprement parler une application d&apos; aide à la décision . l&apos; avenir appartient plutôt aux instruments de type tableau de bord équipés de fonctions d&apos; analyses multidimensionnelles de type olap . fonction olap qui peut être obtenue de différentes façons par exemple via une base de données relationnelle r-olap , ou multidimensionnelle m-olap , voire aussi en h-olap . les datamart et / ou les datawarehouses peuvent ainsi permettre via l&apos; olap l ’ analyse très approfondie de l ’ activité de l ’ entreprise , grâce à des statistiques recoupant des informations relatives à des activités apparemment très différentes ou très éloignées les unes des autres , mais dont l ’ étude fait souvent apparaître des dysfonctionnements , des corrélations ou des possibilités d ’ améliorations très sensibles . l&apos; interopérabilité entre les systèmes d&apos; entrepôt de données , les applications informatiques ou de gestion de contenu , et les systèmes de reporting est réalisée grâce à une gestion des métadonnées . à titre d&apos; illustration , les tableaux croisés des principaux tableurs permettent de construire ce type de tableau de bord depuis une base de données . on obtient une dimension de plus et on passe ainsi au cube . les tableaux croisés dynamiques d&apos; excel permettent de représenter ce type de cube avec le champ &quot; page &quot; . il représente les données agrégées pour chaque niveau hiérarchique et pour chaque dimension . on obtient alors un cube à plus de 3 dimensions , appelé hypercube . le terme cube est souvent utilisé en lieu et place d &apos; hypercube . chacune de ces vues partielles du cube se traduit finalement , soit par un tableau à double entrée ( tri croisé ) , soit par un graphique le plus souvent bidimensionnel . ainsi , bien que la navigation dans le cube soit multidimensionnelle , le décideur n ’ a pas , en réalité , accès à une synthèse , mais à une multitude de tris croisés ou de vues bidimensionnelles dont l ’ exploration , longue et fastidieuse , est parfois court-circuitée faute de temps . cela peut conduire à de coûteuses erreurs de décision . aussi peut-il être utile d ’ associer à cette démarche une iconographie des corrélations , qui permet une vue d ’ ensemble réellement multidimensionnelle , débarrassée des redondances . un système d&apos; information décisionnel ( sid ) doit être capable d&apos; assurer quatre fonctions fondamentales : la collecte , l&apos; intégration , la diffusion et la présentation des données . à ces quatre fonctions s&apos; ajoute une fonction d&apos; administration , soit le contrôle du sid lui-même . la fonction collecte ( parfois appelée datapumping ) recouvre l&apos; ensemble des tâches consistant à détecter , sélectionner , extraire et filtrer les données brutes issues des environnements pertinents compte tenu du périmètre couvert par le sid . comme il est fréquent que les sources de données internes et / ou externes soient hétérogènes — tant sur le plan technique que sur le plan sémantique — cette fonction est la plus délicate à mettre en place dans un système décisionnel complexe . elle s&apos; appuie notamment sur des outils d&apos; etl ( extract-transform-load pour extraction-transformation-chargement ) . la fonction de collecte joue également , au besoin , un rôle de recodage . une donnée représentée différemment d&apos; une source à une autre impose le choix d&apos; une représentation unique et donc d&apos; une mise en équivalence utile pour les futures analyses . la fonction d&apos; intégration consiste à concentrer les données collectées dans un espace unifié , dont le socle informatique essentiel est l&apos; entrepôt de données . élément central du dispositif , il permet aux applications décisionnelles de masquer la diversité de l&apos; origine des données et de bénéficier d&apos; une source d&apos; information commune , homogène , normalisée et fiable , au sein d&apos; un système unique et si possible normalisé . c&apos; est également dans cette fonction que sont effectués éventuellement les calculs et les agrégations ( cumuls ) communs à l&apos; ensemble du projet . la fonction d&apos; intégration est généralement assurée par la gestion de métadonnées , qui assurent l&apos; interopérabilité entre toutes les ressources informatiques , qu&apos; il s&apos; agisse de données structurées ( bases de données accédées par des progiciels ou applications ) , ou des données non structurées ( documents et autres ressources non structurées , manipulés par les systèmes de gestion de contenu ) . la fonction de diffusion met les données à la disposition des utilisateurs , selon des schémas correspondant aux profils ou aux métiers de chacun , sachant que l&apos; accès direct à l&apos; entrepôt de données ne correspond généralement pas aux besoins spécifiques d&apos; un décideur ou d&apos; un analyste . l&apos; objectif prioritaire est à ce titre de segmenter les données en contextes informationnels fortement cohérents , simples à utiliser et correspondant à une activité décisionnelle particulière . alors qu&apos; un entrepôt de données peut héberger des centaines ou des milliers de variables ou indicateurs , un contexte de diffusion raisonnable n&apos; en présente que quelques dizaines au maximum . chaque contexte peut correspondre à un datamart , bien qu&apos; il n&apos; y ait pas de règles générales concernant le stockage physique . très souvent , un contexte de diffusion est multidimensionnel , c&apos; est-à-dire modélisable sous la forme d&apos; un hypercube ; il peut alors être mis à disposition à l&apos; aide d&apos; un outil olap . les différents contextes d&apos; un même système décisionnel n&apos; ont pas tous besoin du même niveau de détail . de nombreux agrégats ou cumuls , n&apos; intéressent que certaines applications et n&apos; ont donc pas lieu d&apos; être gérés en tant qu&apos; agrégats communs par la fonction d&apos; intégration : la gestion de ce type de spécificité peut être prise en charge par la fonction de diffusion . ces agrégats pouvant au choix , être stockés de manière persistante ou calculés dynamiquement à la demande . cette quatrième fonction , la plus visible pour l&apos; utilisateur , régit les conditions d&apos; accès de l&apos; utilisateur aux informations , dans le cadre d&apos; une interface homme-machine déterminé ( ihm ) . elle assure le contrôle d&apos; accès et le fonctionnement du poste de travail , la prise en charge des requêtes , la visualisation des résultats sous une forme ou une autre . elle utilise toutes les techniques de communication possibles : outils bureautiques , requêteurs et générateurs d&apos; états spécialisés , infrastructure web , télécommunications mobiles , etc. c&apos; est la fonction transversale qui supervise la bonne exécution de toutes les autres . elle pilote le processus de mise à jour des données , la documentation sur les données ( les méta-données ) , la sécurité , les sauvegardes , et la gestion des incidents . dans une entreprise , le volume de données traitées croît rapidement avec le temps . ces données peuvent provenir des fournisseurs , des clients , de l ’ environnement etc. cette quantité de données augmente en fonction du secteur et de l&apos; activité de l ’ entreprise . par exemple , dans la grande distribution , les quantités de données collectées chaque jour sont énormes ( notamment lorsque les magasins collectent les tickets des caisses ) . le projet décisionnel correspond à cette dernière option . il s ’ agit de traiter les données et de les stocker de manière cohérente au fur et à mesure qu ’ elles se présentent . c ’ est pour cela que le projet décisionnel est un projet sans limite dans le temps . c&apos; est-à-dire que dès que l ’ entreprise commence ce projet , elle ne s ’ arrête pas ( sauf cas exceptionnel ) . wal-mart ( une chaîne de la grande distribution ) est l ’ une des entreprises qui stockent le plus de données ( elle a multiplié par 100 ses données en quelques années ) et va atteindre dans les années à venir &#91; quand ? &#93; le pétaoctet ( 1 000 téraoctets ) . pour mener à bien ces projets décisionnels , il existe une multitude d&apos; outils , chacun étant plus ou moins adapté à la taille de l&apos; entreprise , à la structure des données existantes et au type d&apos; analyse désiré . des sgbd relationnels et d&apos; autres systèmes qui contiennent les données d&apos; exploitation . article détaillé : gestion des exigences . cela conduit au choix de technologies précises et à un modèle particulier .
