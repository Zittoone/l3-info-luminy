vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants. choisir de toutes les distributions répondant à ces contraintes celle ayant la plus grande entropie au sens de shannon . ce choix n&apos; a rien d&apos; arbitraire : de toutes ces distributions , c&apos; est - par définition de l&apos; entropie - celle d&apos; entropie maximale qui contient le moins d&apos; information , et elle est donc pour cette raison la moins arbitraire de toutes celles que l&apos; on pourrait utiliser . la distribution de probabilité obtenue sert ensuite de probabilité a priori dans un processus classique d&apos; inférence bayésienne . le principe d&apos; entropie maximale considère un principe d&apos; équidistribution ( principe d&apos; indifférence de laplace ) et d&apos; indépendance entre événements élémentaires ayant donné lieu à la distribution de probabilité . il s&apos; agit donc d&apos; un a priori extrêmement « neutre » , si toutefois l&apos; espace d&apos; hypothèses est bien choisi . comme la mesure d&apos; entropie de shannon considère un ensemble d&apos; états équiprobables , il peut être utile d&apos; adapter l&apos; espace d&apos; hypothèses pour rendre les différents états équiprobables ou alors utiliser l&apos; entropie relative pour normaliser l&apos; expression par rapport à leur probabilités respectives a priori .
