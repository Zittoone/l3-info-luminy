en optimisation , vue comme branche des mathématiques , l&apos; optimisation non linéaire ( en anglais : nonlinear programming – nlp ) s&apos; occupe principalement des problèmes d&apos; optimisation dont les données , i.e. , les fonctions et ensembles définissant ces problèmes , sont non linéaires ( bien sûr ) , mais sont aussi différentiables autant de fois que nécessaire pour l&apos; établissement des outils théoriques , comme les conditions d&apos; optimalité , ou pour la bonne marche des algorithmes de résolution qui y sont introduits et analysés . cette sous-discipline de l&apos; optimisation , à la frontière mal définie et dont l&apos; introduction est un peu artificielle , a aussi son existence liée à la communauté de chercheurs qui se sont spécialisés sur ces sujets et au type de résultats qui ont pu être obtenus . elle complémente l&apos; optimisation non lisse ( ou non différentiable ) , elle aussi liée à une communauté de chercheurs spécialisés . ces deux disciplines se rassemblent pour former ce que l&apos; on appelle l&apos; optimisation continue , qui jouxte , quant à elle , d&apos; autres sous-disciplines telles que l&apos; optimisation combinatoire ( ou discrète ) , l&apos; optimisation stochastique , etc. x \ in x ; \ , f ( x ) = \ min _ { y \ in x } f ( y ) . x \ in x ; \ , f ( x ) = \ max _ { y \ in x } f ( y ) . si la fonction est convexe ou concave , et l&apos; ensemble des contraintes est convexe , alors il existe des méthodes spécialisées , appelées méthodes d&apos; optimisation convexe . sinon , il existe plusieurs solutions . par exemple , utilisant le principe de séparation et évaluation pour diviser et traiter séparément plusieurs paramètres . l&apos; algorithme peut également être arrêté avant d&apos; aboutir , si on peut prouver qu&apos; aucune solution ultérieure ne sera meilleure à un certain seuil de tolérance près . les conditions de karush-kuhn-tucker ( kkt ) garantissent qu&apos; une solution ainsi obtenue est optimale. ainsi , lorsque x se rapproche de la frontière , la valeur de ln ( h ) tend vers - ∞ , ce qui pénalise la zone . on effectue plusieurs recherches en faisant tendre μ vers 0 . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « nonlinear programming » ( voir la liste des auteurs ) . ( en ) avriel , mordecai ( 2003 ) , nonlinear programming : analysis and methods . dover publishing . ( isbn 0-486-43227-0 ) . ( en ) bazaraa , mokhtar et shetty ( 1979 ) , nonlinear programming . theory and algorithms . john wiley &amp; sons . ( isbn 0-471-78610-1 ) . ( en ) bertsekas , dimitri ( 1999 ) , nonlinear programming : 2d edition . athena scientific . ( isbn 1-886529-00-0 ) . ( en ) bonnans , j. f et shapiro , a. ( 2000 ) , perturbation analysis of optimization problems . springer . ( isbn 978-0-387-98705-7 ) . ( en ) nocedal , jorge et wright , stephen ( 1999 ) , numerical optimization . springer . ( isbn 0-387-98793-2 ) . ( en ) gams general algebraic modeling system .
