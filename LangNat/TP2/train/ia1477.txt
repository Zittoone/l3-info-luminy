en mathématiques , un processus de markov est un processus stochastique possédant la propriété de markov . dans un tel processus , la prédiction du futur à partir du présent n&apos; est pas rendue plus précise par des éléments d&apos; information concernant le passé . les processus de markov portent le nom de leur inventeur , andreï markov . un processus de markov en temps discret est une séquence \ scriptstyle \ x _ 1 , \ scriptstyle \ x _ 2 , \ scriptstyle \ x _ 3 , \ \ dots de variables aléatoires . l&apos; ensemble de leurs valeurs possibles est appelé l ’ espace d&apos; états , la valeur \ scriptstyle \ x _ n \ étant l&apos; état du processus à l&apos; instant \ scriptstyle \ n. selon les auteurs , le vocable « chaîne de markov » désigne les processus de markov à temps discret ou uniquement les processus de markov à temps discret et à espace d&apos; états discret , i.e. les processus de markov à temps discret dont l&apos; espace d&apos; états est fini ou dénombrable. où x est un état quelconque du processus . l&apos; identité ci-dessus identifie la probabilité markovienne . andreï markov a publié les premiers résultats de ces processus en 1906 . une généralisation à un espace d&apos; états infini dénombrable a été donnée par kolmogorov en 1936 . les processus de markov sont liées au mouvement brownien et à l&apos; hypothèse ergodique , deux sujets de physique statistique qui ont été très importants au début du xxe siècle . lorsque les variables aléatoires successives sont des variables discrètes munies d&apos; une fonction de probabilité , on parle de chaîne de markov . elle est donc également définie par la probabilité au second ordre . enfin , elle peut être définie par l&apos; état initial et la probabilité de transition . les chaînes de markov trouvent des applications dans les domaines les plus divers mais les processus considérés dans les problèmes dynamiques , en particulier en vibrations , portent généralement sur des variables aléatoires continues . les considérations qui précèdent restent valables si les intervalles de temps deviennent infiniment petits . cette remarque est particulièrement intéressante dans le cas d&apos; une équation différentielle . si elle est du premier ordre , la mise en différences finies fait apparaître un mécanisme markovien . pour les ordres supérieurs et les systèmes différentiels , la décomposition en équations du premier ordre conduit à un système markovien à plusieurs dimensions . ces formules se généralisent à un futur arbitrairement lointain n + k en multipliant les probabilités de transition et en intégrant k-1 fois. où y est un nom arbitraire pour la variable d&apos; intégration . une telle distribution \ pi est appelée une distribution stationnaire . une distribution stationnaire est une fonction propre de la loi de distribution conditionnelle , associée à la valeur propre 1 . dans le cas des chaînes de markov à espace d&apos; états discret , certaines propriétés du processus déterminent s&apos; il existe ou non une distribution stationnaire , et si elle est unique ou non. une chaîne de markov est irréductible si tout état est accessible à partir de n&apos; importe quel autre état. un état est récurrent positif si l&apos; espérance du temps de premier retour en cet état , partant de cet état , est finie . quand l&apos; espace des états d&apos; une chaîne de markov n&apos; est pas irréductible , il peut être partitionné en un ensemble de classes communicantes irréductibles . le problème de la classification a son importance dans l&apos; étude mathématique des chaînes de markov et des processus stochastiques . si une chaîne de markov possède au moins un état récurrent positif , alors il existe une distribution stationnaire. et le processus construit en prenant la distribution stationnaire comme distribution initiale est ergodique . c&apos; est vrai en particulier lorsque f est la fonction identité . la moyenne de la valeur des instances est donc , sur le long terme , égale à l&apos; espérance de la distribution stationnaire. où \ mu _ { \ pi } est la mesure induite par \ pi . cela permet d&apos; approximer la distribution stationnaire par un histogramme d&apos; une séquence particulière . les systèmes markoviens sont très présents en physique particulièrement en physique statistique . ils interviennent en particulier à travers l&apos; équation de fokker-planck . plus généralement l&apos; hypothèse markovienne est souvent invoquée lorsque des probabilités sont utilisées pour modéliser l&apos; état d&apos; un système , en supposant toutefois que l&apos; état futur du système peut être déduit du passé avec un historique assez faible . le célèbre article de 1948 de claude shannon , a mathematical theory of communication , qui fonde la théorie de l&apos; information , commence en introduisant la notion d&apos; entropie à partir d&apos; une modélisation markovienne de la langue anglaise . il montre ainsi le degré de prédictibilité de la langue anglaise , muni d&apos; un simple modèle d&apos; ordre 1 . bien que simples , de tels modèles permettent de bien représenter les propriétés statistiques des systèmes et de réaliser des prédictions efficaces sans décrire complètement la structure complète des systèmes . en compression , la modélisation markovienne permet la réalisation de techniques de codage entropique très efficaces , comme le codage arithmétique . de très nombreux algorithmes en reconnaissance des formes ou en intelligence artificielle comme l&apos; algorithme de viterbi , utilisé dans la grande majorité des systèmes de téléphonie mobile pour la correction d&apos; erreurs , font l&apos; hypothèse d&apos; un processus markovien sous-jacent . l&apos; indice de popularité d&apos; une page web ( pagerank ) tel qu&apos; il est utilisé par google est défini par une chaîne de markov . il est défini par la probabilité d&apos; être dans cette page à partir d&apos; un état quelconque de la chaine de markov représentant le web . si n est le nombre de pages web connues , et une page i a k _ i liens , alors sa probabilité de transition vers une page liée ( vers laquelle elle pointe ) est p _ i ^ l = \ tfrac { 1-q } { k _ i } + \ tfrac qn et p _ i ^ { nl } = \ tfrac qn pour toutes les autres ( pages non liées ) . notons qu&apos; on a bien k _ i p _ i ^ l + ( n-k _ i ) p _ i ^ { nl } = 1 . le paramètre q vaut environ 0,15 . les chaînes de markov sont un outil fondamental pour modéliser les processus en théorie des files d&apos; attente et en statistiques . les chaînes de markov sont couramment employées en sûreté de fonctionnement pour les calculs de fiabilité et de disponibilité des systèmes techniques , en particulier pour modéliser des séquences d&apos; évènements de type pannes , réparations , changements de configuration . les chaînes de markov sont également utilisées en bio-informatique pour modéliser les relations entre symboles successifs d&apos; une même séquence ( de nucléotides par exemple ) , en allant au-delà du modèle polynomial . les modèles markoviens cachés ont également diverses utilisations , telles que la segmentation ( définition de frontières de régions au sein de séquences de gènes ou de protéines dont les propriétés chimiques varient ) , l&apos; alignement multiple , la prédiction de fonction , ou la découverte de gènes ( les modèles markoviens cachés sont plus « flexibles » que les définitions strictes de type codon start + multiples codons + codons stop et ils sont donc plus adaptés pour les eucaryotes ( à cause de la présence d&apos; introns dans le génome de ceux-ci ) ou pour la découverte de pseudo-gènes ) .
