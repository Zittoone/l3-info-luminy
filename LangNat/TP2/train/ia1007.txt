adaboost ( ou adaptive boosting ) est une méthode de boosting ( intelligence artificielle , apprentissage automatique ) introduite par yoav freund et robert schapiretemplate : harv . adaboost repose sur la sélection itérative de classifieur faible en fonction d&apos; une distribution des exemples d&apos; apprentissage . chaque exemple est pondéré en fonction de sa difficulté avec le classifieur courant . c&apos; est un exemple de la méthode générale multiplicative weights updatetemplate : lien web . , template : lien web . . soit un ensemble d&apos; apprentissage annoté : ( x _ { 1 } , y _ { 1 } ) , \ ldots , ( x _ { m } , y _ { m } ) où x _ { i } \ in x , sont les exemples et \ , y _ { i } \ in y = \ { -1 , + 1 \ } les annotations . on initialise la distribution des exemples par d _ { 1 } ( i ) = \ frac { 1 } { m } , i = 1 , \ ldots , m. des variantes ont été introduites , et dont les modifications portent essentiellement sur la manière dont les poids sont mis à jour . parmi ces variantes , gentle adaboost et real adaboost sont fréquemment utilisées . citons aussi rankboost . ce fut l&apos; une des premières méthodes pleinement fonctionnelles permettant de mettre en œuvre le principe de boosting . les auteurs ont reçu le prestigieux prix gödel en 2003 pour leur découvertepage officielle du prix gödel 2003 .
