pour les articles homonymes , voir svm . les machines à vecteurs de support ou séparateurs à vaste marge ( en anglais support vector machine , svm ) sont un ensemble de techniques d&apos; apprentissage supervisé destinées à résoudre des problèmes de discriminationle terme anglais pour discrimination est template : langue , qui a un sens différent en français ( se rapporte au template : langue ) . on utilise aussi le terme classement à la place de discrimination , plus proche du terme anglais , et plus compréhensible. et de régression . les svm sont une généralisation des classifieurs linéaires . les svm ont été développés dans les années 1990 à partir des considérations théoriques de vladimir vapnik sur le développement d&apos; une théorie statistique de l&apos; apprentissage : la théorie de vapnik-chervonenkis . les svm ont rapidement été adoptés pour leur capacité à travailler avec des données de grandes dimensions , le faible nombre d&apos; hyper paramètres , leurs garanties théoriques , et leurs bons résultats en pratique . les svm ont été appliqués à de très nombreux domaines ( bio-informatique , recherche d&apos; information , vision par ordinateur , financetemplate : en bernhard schölkopf , alexander j. smola , template : langue , 2002 , mit press . … ) . selon les données , la performance des machines à vecteurs de support est de même ordre , ou même supérieure , à celle d&apos; un réseau de neurones ou d&apos; un modèle de mélanges gaussiens . les séparateurs à vastes marges reposent sur deux idées clés : la notion de marge maximale et la notion de fonction noyau . ces deux notions existaient depuis plusieurs années avant qu&apos; elles ne soient mises en commun pour construire les svm . l&apos; idée des hyperplans à marge maximale a été explorée dès 1963 par vladimir vapnik et a. lernertemplate : en vladimir vapnik et a. lerner , pattern recognition using generalized portrait method , automation and remote control , 1963 . , et en 1973 par richard duda et peter hart dans leur livre pattern classificationtemplate : dudahartstork . . les fondations théoriques des svm ont été explorées par vapnik et ses collègues dans les années 70 avec le développement de la théorie de vapnik-chervonenkis , et par valiant et la théorie de l&apos; apprentissage pactemplate : en l.g. valiant . template : langue , template : langue , 27 ( 11 ) , template : p. , novembre 1984 . . l&apos; idée des fonctions noyaux n&apos; est pas non plus nouvelle : le théorème de mercer date de 1909template : en j. mercer , template : langue . philos . trans . roy . soc . london , a 209 : 415--446 , 1909 . , et l&apos; utilité des fonctions noyaux dans le contexte de l&apos; apprentissage artificiel a été montré dès 1964 par aizermann , bravermann et rozoenerm . aizerman , e. braverman , and l. rozonoer , template : langue , automation and remote control 25 : 821--837 , 1964 . . ce n&apos; est toutefois qu&apos; en 1992 que ces idées seront bien comprises et rassemblées par boser , guyon et vapnik dans un article , qui est l&apos; article fondateur des séparateurs à vaste margetemplate : en bernhard e. boser , isabelle m. guyon , vladimir n. vapnik , a training algorithm for optimal margin classifiers in fifth annual workshop on computational learning theory , pages 144--152 , pittsburgh , acm . 1992 . l&apos; idée des variables ressorts , qui permet de résoudre certaines limitations pratiques importantes , ne sera introduite qu&apos; en 1995 . à partir de cette date , qui correspond à la publication du livre de vapnikv . vapnik , template : langue , n-y , springer-verlag , 1995 . , les svm gagnent en popularité et sont utilisés dans de nombreuses applications . un brevet américain sur les svm est déposé en 1997 par les inventeurs originelstemplate : en template : langue . b. boser , i. guyon , and v. vapnik , us patent 5,649,068 1997 . . les séparateurs à vastes marges sont des classificateurs qui reposent sur deux idées clés , qui permettent de traiter des problèmes de discrimination non linéaire , et de reformuler le problème de classement comme un problème d&apos; optimisation quadratique . la première idée clé est la notion de marge maximale . la marge est la distance entre la frontière de séparation et les échantillons les plus proches . ces derniers sont appelés vecteurs supports . dans les svm , la frontière de séparation est choisie comme celle qui maximise la marge . ce choix est justifié par la théorie de vapnik-chervonenkis ( ou théorie statistique de l&apos; apprentissage ) , qui montre que la frontière de séparation de marge maximale possède la plus petite capacitétemplate : en marti a. hearst , support vector machines , ieee intelligent systems , vol. 13 , no . 4 , template : p. , jul / aug , 1998 . le problème est de trouver cette frontière séparatrice optimale , à partir d&apos; un ensemble d&apos; apprentissage . ceci est fait en formulant le problème comme un problème d&apos; optimisation quadratique , pour lequel il existe des algorithmes connus . afin de pouvoir traiter des cas où les données ne sont pas linéairement séparables , la deuxième idée clé des svm est de transformer l&apos; espace de représentation des données d&apos; entrées en un espace de plus grande dimension ( possiblement de dimension infinie ) , dans lequel il est probable qu&apos; il existe une séparatrice linéaire . ceci est réalisé grâce à une fonction noyau , qui doit respecter les conditions du théorème de mercer , et qui a l&apos; avantage de ne pas nécessiter la connaissance explicite de la transformation à appliquer pour le changement d&apos; espace . les fonctions noyau permettent de transformer un produit scalaire dans un espace de grande dimension , ce qui est coûteux , en une simple évaluation ponctuelle d&apos; une fonction . cette technique est connue sous le nom de kernel trick . on se limite pour l&apos; instant à un problème de discrimination à deux classes ( discrimination binaire ) , c&apos; est-à-dire y \ in \ { -1,1 \ } , le vecteur d&apos; entrée x étant dans un espace x muni d&apos; un produit scalaire . on peut prendre par exemple x = \ mathbb { r } ^ n. il est alors décidé que x est de classe 1 si h ( x ) \ ge 0 et de classe -1 sinon . c&apos; est un classifieur linéaire. l _ k h ( x _ k ) \ ge 0 \ quad 1 \ le k \ le p , \ quad \ mbox { autrement dit } \ quad l _ k ( w ^ t x _ k + w _ 0 ) \ ge 0 \ quad 1 \ le k \ le p. imaginons un plan ( espace à deux dimensions ) dans lequel sont répartis deux groupes de points . ces points sont associés à un groupe : les points ( + ) pour y &gt; x et les points ( - ) pour y &lt; x. on peut trouver un séparateur linéaire évident dans cet exemple , la droite d&apos; équation y = x. le problème est dit linéairement séparable . pour des problèmes plus compliqués , il n&apos; existe en général pas de séparatrice linéaire . imaginons par exemple un plan dans lequel les points ( - ) sont regroupés à l&apos; intérieur d&apos; un cercle , avec des points ( + ) tout autour : aucun séparateur linéaire ne peut correctement séparer les groupes : le problème n&apos; est pas linéairement séparable . il n&apos; existe pas d&apos; hyperplan séparateur . on se place désormais dans le cas où le problème est linéairement séparable . même dans ce cas simple , le choix de l&apos; hyperplan séparateur n&apos; est pas évident . il existe en effet une infinité d&apos; hyperplans séparateurs , dont les performances en apprentissage sont identiques ( le risque empirique est le même ) , mais dont les performances en généralisation peuvent être très différentes . pour résoudre ce problème , il a été montrév . vapnik , et s. kotz , estimation of dependences based on empirical data , springer series in statistics , 1982 , template : isbn . , qu&apos; il existe un unique hyperplan optimal , défini comme l&apos; hyperplan qui maximise la marge entre les échantillons et l&apos; hyperplan séparateur . il existe des raisons théoriques à ce choix . vapnik a montré que la capacité des classes d&apos; hyperplans séparateurs diminue lorsque leur marge augmente . cette normalisation est parfois appelée la forme canonique de l&apos; hyperplan , ou hyperplan canoniquetemplate : en bernhard schölkopf , alexander j. smola , learning with kernels : support vector machines , regularization , optimization and beyond , 2002 , mit press , template : p. . le lagrangien doit être minimisé par rapport à w et w0 , et maximisé par rapport à α . ce qui donne les multiplicateurs de lagrange optimaux \ alpha _ k ^ * . \ alpha _ k &#91; l _ k h ( x _ k ) -1 &#93; = 0 \ quad 1 \ le k \ le p. \ begin { cases } \ alpha _ k &amp; = 0 \ \ l _ k h ( x _ k ) &amp; = 1 \ end { cases } . les seuls points pour lesquels les contraintes du lagrangien sont actives sont donc les points tels que l _ k h ( x _ k ) = 1 , qui sont les points situés sur les hyperplans de marges maximales . en d&apos; autres termes , seuls les vecteurs supports participent à la définition de l&apos; hyperplan optimal . la deuxième remarque découle de la première . seul un sous-ensemble restreint de points est nécessaire pour le calcul de la solution , les autres échantillons ne participant pas du tout à sa définition . ceci est donc efficace au niveau de la complexité . d&apos; autre part , le changement ou l&apos; agrandissement de l&apos; ensemble d&apos; apprentissage a moins d&apos; influence que dans un modèle de mélanges gaussiens par exemple , où tous les points participent à la solution . en particulier , le fait d&apos; ajouter des échantillons à l&apos; ensemble d&apos; apprentissage qui ne sont pas des vecteurs supports n&apos; a aucune influence sur la solution finale . la dernière remarque est que l&apos; hyperplan solution ne dépend que du produit scalaire entre le vecteur d&apos; entrée et les vecteurs supports . cette remarque est l&apos; origine de la deuxième innovation majeure des svm : le passage par un espace de redescription grâce à une fonction noyau . article détaillé : kernel trick . la notion de marge maximale et la procédure de recherche de l&apos; hyperplan séparateur telles que présentées pour l&apos; instant ne permettent de résoudre que des problèmes de discrimination linéairement séparables . c&apos; est une limitation sévère qui condamne à ne pouvoir résoudre que des problèmes jouets , ou très particuliers . afin de remédier au problème de l&apos; absence de séparateur linéaire , l&apos; idée des svm est de reconsidérer le problème dans un espace de dimension supérieure , éventuellement de dimension infinie . dans ce nouvel espace , il est alors probable qu&apos; il existe une séparatrice linéaire . plus formellement , on applique aux vecteurs d&apos; entrée x une transformation non-linéaire \ phi . l&apos; espace d&apos; arrivée \ phi ( x ) est appelé espace de redescription. l _ kh ( x _ k ) &gt; 0 , pour tous les points x _ k de l&apos; ensemble d&apos; apprentissage , c&apos; est-à-dire l&apos; hyperplan séparateur dans l&apos; espace de redescription . le calcul se fait dans l&apos; espace d&apos; origine , ceci est beaucoup moins coûteux qu&apos; un produit scalaire en grande dimension . la transformation \ phi n&apos; a pas besoin d&apos; être connue explicitement , seule la fonction noyau intervient dans les calculs . on peut donc envisager des transformations complexes , et même des espaces de redescription de dimension infinie . en pratique , on ne connaît pas la transformation \ phi , on construit plutôt directement une fonction noyau . celle-ci doit respecter certaines conditions , elle doit correspondre à un produit scalaire dans un espace de grande dimension . le théorème de mercer explicite les conditions que k doit satisfaire pour être une fonction noyau : elle doit être symétrique , semi-définie positive . on se ramène donc au cas d&apos; un classifieur linéaire , sans changement d&apos; espace . l&apos; approche par kernel trick généralise ainsi l&apos; approche linéaire . le noyau linéaire est parfois employé pour évaluer la difficulté d&apos; un problème . en général , il n&apos; est pas non plus possible de trouver une séparatrice linéaire dans l&apos; espace de redescription . il se peut aussi que des échantillons soient mal étiquetés , et que l&apos; hyperplan séparateur ne soit pas la meilleure solution au problème de classement. où c est une constante qui permet de contrôler le compromis entre nombre d&apos; erreurs de classement , et la largeur de la marge . elle doit être choisie par l&apos; utilisateur , en général par une recherche exhaustive dans l&apos; espace des paramètres , en utilisant par exemple la validation croisée sur l&apos; ensemble d&apos; apprentissage . le choix automatique de ce paramètre de régularisation est un problème statistique majeur . plusieurs méthodes ont été proposées pour étendre le schéma ci-dessus au cas où plus de deux classes sont à séparer . ces schémas sont applicables à tout classifieur binaire , et ne sont donc pas spécifiques aux svmtemplate : bishop06 , template : p. . les deux plus connues sont appelées one versus all et one versus one . formellement , les échantillons d&apos; apprentissage et de test peuvent ici être classés dans m classes \ { c _ 1 , c _ 2 , ... , c _ m \ } . la méthode one-versus-all ( appelée parfois one-versus-the-rest ) consiste à construire m classifieurs binaires en attribuant le label 1 aux échantillons de l&apos; une des classes et le label -1 à toutes les autres . en phase de test , le classifieur donnant la valeur de confiance ( e.g la marge ) la plus élevée remporte le vote . une généralisation de ces méthodes a été proposée en 1995dietterich , t.g. and bakiri , g. , solving multiclass learning problems via error-correcting output codes . journal of artificial intelligence research. v2 . 263-286. sous le nom decoc , consistant à représenter les ensembles de classifieurs binaires comme des codes sur lesquels peuvent être appliqués les techniques de correction d&apos; erreur . ces méthodes souffrent toutes de deux défauts . dans la version one-versus-all , rien n&apos; indique que les valeurs du résultat de classification des m classifieurs soient comparables ( pas de normalisation , donc possibles problèmes d&apos; échelle ) template : bishop06 , template : p. . de plus le problème n&apos; est plus équilibré , par exemple avec m = 10 , on utilise seulement 10 % d&apos; exemples positifs pour 90 % d&apos; exemples négatifs . vladimir vapnik , harris drucker , chris burges , linda kaufman et alex smola ont proposé en 1996 une méthode pour utiliser des svm afin de résoudre des problèmes de régressiontemplate : en harris drucker , chris j. c. burges , linda kaufman , alex smola and vladimir vapnik ( 1997 ) . « template : langue » . template : langue , 155-161 , template : langue . . ( en ) bernhard schölkopf , alexander j. smola , learning with kernels : support vector machines , regularization , optimization and beyond , 2002 , mit press . ( en ) john shawe-taylor , nello cristianini , support vector machines and other kernel-based learning methods , cambridge university press , 2000 . jean beney , classification supervisée de documents : théorie et pratique , hermes science , février 2008 , 184p .
