en intelligence artificielle , la classification sous contrainte désigne une famille d&apos; algorithmes d&apos; apprentissage semi-supervisée . cette dernière occupe une place intermédiaire entre l&apos; apprentissage supervisé ( pour laquelle les étiquettes de classes sont connues ) et l&apos; apprentissage non-supervisé ( où aucune étiquette de classe n&apos; est fournie ) . elle est de plus en plus usitée , en raison du besoin croissant d&apos; automatiser des tâches requérant une expertise humaine . ces dernières années , l&apos; intérêt porté sur l&apos; incorporation de connaissances a priori dans des processus de classification , a pris de l&apos; ampleur dans un nombre important d&apos; applications telles que l&apos; indexation et la recherche par le contenu , la segmentation d&apos; images ou l&apos; analyse de documents « han j. , kamber m. , data mining : concepts and techniques . morgan kaufmann publishers , template : 3rd edition , 2006 » . cependant , un des problèmes les plus fréquemment traités par la classification sous contrainte , est la régionalisation : l&apos; objectif est alors de trouver des groupes de régions géographiques similaires respectant certaines contraintes . dans la littérature , la majorité des auteurs traitent ces problèmes en adaptant des procédures de partitionnement classiques ( par exemple , les algorithmes de classification hiérarchique ) et / ou des procédures d&apos; optimisation. étiquetage partiel et contraintes de comparaisons entre objets ( contraintes d&apos; objets ) . cette distance est donc égale à la norme du vecteur résultant de la soustraction de xi et xj , et peut s&apos; écrire d ( xi , xj ) = dij . la façon la plus évidente d&apos; ajouter une notion de localisation spatiale dans un ensemble de données , est d&apos; ajouter des attributs de position pour chacun des objets . par exemple , pour une image à deux dimensions , ces attributs peuvent être définis comme étant la ligne et la colonne de chaque pixel de l&apos; image . la fonction d ( xi , xj ) , calculant la distance entre deux objets , permet alors d&apos; affecter un degré de similarité important aux pixels proches et un degré faible à ceux qui sont éloignés . l&apos; avantage de cette méthode réside dans le fait qu&apos; elle ne requiert pas de modification de l&apos; algorithme de classification utilisé . on appelle voisin de xi , l&apos; objet xj le plus proche de l&apos; objet xi en termes de distance dij dans l&apos; espace en d dimensions ( avec i ≠ j ) . la technique de duplication proposée consiste en l&apos; augmentation de la taille du vecteur attributs d&apos; un objet en ajoutant un ou plusieurs ensembles d&apos; attributs selon le nombre de voisins considéré ( cf. recherche des plus proches voisins ) . en effet , on affecte à chaque objet , un duplicata des caractéristiques de chacun de ses voisins ( au sens de la distance calculée ) « roberts s. , gisler g. , theiler j. , spatio-spectral image analysis using classical and neural algorithms . in dagli c.h. , akay m. , chen c.l.p. , fernandez b.r. , ghosh j. editors , smart engineering systems : neural networks , fuzzy logic and evolutionary programming , vol. 6 of intelligent engineering systems through artificial neural networks , p. 425-430 , 1996 » . cependant , il est aisé de voir que cette approche est souvent impraticable car elle engendre une augmentation importante de la dimension des données et donc un coût non négligeable aussi bien en termes de temps de calcul qu&apos; en termes d&apos; espace mémoire nécessaire à l&apos; exécution des algorithmes de classification utilisés . contrairement aux méthodes précédentes qui modifient l&apos; ensemble des données ( et plus particulièrement , le vecteur attribut de chaque objet par ajout de nouveaux attributs ) , certaines méthodes permettent d&apos; intégrer des contraintes de manière moins directe . en effet , il est possible d&apos; incorporer des informations spatiales en modifiant la manière de calculer la distance entre deux objets . cette approche consiste à modifier la distance originale séparant deux objets par une fonction non-linéaire ( une des fonctions les plus souvent utilisées dans la littérature est la fonction exponentielle ) . avec dij * représentant la distance modifiée incorporant l&apos; information spatiale ( grâce au voisinage de xi et xj ) , dij étant la distance originale entre les deux objets , uij s&apos; exprimant comme une mesure de distance égale au ratio des distances moyennes des voisins de chacun des objets xi et xj , et enfin w étant un poids ( de valeur arbitraire ou fixée par l&apos; utilisateur ) . cette méthode permet donc d&apos; associer les caractéristiques des objets aux connaissances a priori afin d&apos; obtenir une unique source d&apos; information . l&apos; objectif est alors de trouver des groupes d&apos; objets de compacité équitable et homogènes . la dernière catégorie de méthodes intégrant des contraintes globales , et plus particulièrement des informations de voisinage , est celle modifiant la fonction objectif ( et donc le critère à optimiser ) d&apos; un algorithme de classification quelconque , utilisant une procédure d&apos; optimisation . afin de prendre en compte les contraintes définies , il est nécessaire de remplacer cette fonction par une somme pondérée de la fonction originale avec l&apos; information de voisinage à disposition. avec foriginal * la nouvelle fonction objective ( données originales et contraintes ) , foriginal la fonction originale ( données originales ) et fcontraintes la fonction à optimiser pour l&apos; information a priori ( contraintes ) . contraintes de capacité minimum : des méthodes permettant d&apos; utiliser des contraintes sur des groupes d&apos; objets ont été développées récemment « bradley p.s. , bennett k.p. , demiriz a. , constrained k-means clustering . technical report msr-tr-2000-65 , microsoft research , redmond , wa . » , afin d&apos; éviter d&apos; obtenir des solutions contenant des groupes vides ( solution obtenue en particulier , pour l&apos; algorithme des k-moyennes appliqué à des données de dimensions importantes ou lorsque le nombre de groupes défini est grand ) . cette approche impose des contraintes sur la structure des groupes . en effet , il est alors possible de spécifier un nombre minimum d&apos; objets pour chaque groupe . contraintes de capacité maximum : afin d&apos; essayer de faire respecter ce genre de contraintes , il est souvent pratique d&apos; utiliser un algorithme de classification non-supervisée du type regroupement hiérarchique . en effet , à partir de la hiérarchie créée , il est possible de sélectionner des groupes d&apos; objets adaptés permettant de faire respecter la contrainte définie . la connaissance a priori peut également être interprétée comme de l&apos; information dépendante des caractéristiques des objets . ces contraintes permettent d&apos; orienter la classification des objets selon leurs valeurs pour un attribut donné . un algorithme de classification hiérarchique contraint et incorporant des contraintes d&apos; attributs a été développé en 1998 « bejar j. , cortes u. , experiments with domain knowledge in unsupervised learning : using and revising theories . computacion y sistemas , vol. 1 ( 3 ) , p. 136-143 , 1998 » par bejar et cortes. les contraintes de comparaison par paires d&apos; objets. initialisation intelligente des algorithmes de partitionnement utilisés : l&apos; initialisation de certains algorithmes de classification non-supervisée est une étape importante et peut se révéler cruciale dans la validation et l&apos; interprétation des résultats de partitionnement obtenus « basu s. , bannerjee a. , mooney r. , semi-supervised clustering by seeding . proceedings of the nineteenth international conference on machine learning , 2002 » . c&apos; est pourquoi , il semble intéressant d&apos; utiliser l&apos; information contenue dans ces étiquettes de classes afin d&apos; orienter le choix des paramètres initiaux . les systèmes de classification interactifs adoptent une approche itérative , dans lesquels le système produit une partition des données puis la présente à un expert afin de l&apos; évaluer et la valider . cet expert peut alors indiquer clairement les erreurs de partitionnement ( induits par le système ) , puis cette information peut être utilisée lors de l&apos; itération suivante de l&apos; algorithme . cependant , le désavantage de cette méthode réside dans le fait qu&apos; un expert peut se retrouver en difficulté lors de la validation des résultats si l&apos; ensemble des données est de dimensions importantes . si l&apos; étiquetage des objets se révèle être une tâche longue et complexe , les contraintes par paires , attestant simplement que deux objets doivent être dans la même classe ( must-link ) ou non ( cannot-link ) , sont par contre plus aisées à recueillir auprès d&apos; experts « wagstaff k. , cardiec . , clustering with instance-level constraints . international conference on machine learning , p. 1103-1110 , 2000 » . de plus , cette manière de représenter l&apos; information relationnelle est souvent considérée comme la plus générale , de par le fait qu&apos; elle puisse reprendre une grande partie des contraintes précédemment citées . en effet , les contraintes globales ainsi que les contraintes d&apos; attributs peuvent facilement se mettre sous la forme de contraintes de paires d&apos; objets. les contraintes souples : elles peuvent être ou ne pas être respectées dans la partition en sortie de l&apos; algorithme ( elles sont considérées comme des &quot; préférences &quot; ) . comme démontré dans ce tableau , les contraintes de comparaison must-link définissant une relation d&apos; équivalence entre objets , sont transitives , alors que les contraintes cannot-link sont bien symétriques mais non transitives . en effet , il est impossible de prédire la relation liant deux objets xi et xk , sachant que les paires de points ( xi , xj ) et ( xj , xk ) sont liées par deux contraintes cannot-link indépendantes . apprentissage spectral « kamvar s. , klein d. , manning c. , spectral learning . 18th international joint conference on artificial intelligence , p. 561-566 , 2003 » ( algorithme de classification spectrale avec modification des valeurs d&apos; une matrice de similarités : sij = 1 si ml ( xi , xj ) ( les objets xi et xj sont appariés en must-link ) et sij = 0 si cl ( xi , xj ) ) ( les objets xi et xj sont appariés en cannot-link ) . classification spectrale contrainte flexible « wang x. , davidson i. , flexible constrained spectral clustering . 16th international conference on knowledge discovery and data mining , p. 563-572 , 2010 » ( formalisation lagrangienne du problème d&apos; optimisation du critère de classification spectrale et résolution par un système de valeurs propres généralisées ) .
