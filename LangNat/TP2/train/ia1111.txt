en mathématiques , la méthode de broyden-fletcher-goldfarb-shanno ( bfgs ) est une méthode permettant de résoudre un problème d&apos; optimisation non linéaire sans contraintes . la méthode bfgs est une solution souvent utilisée lorsque l&apos; on veut un algorithme à directions de descente . l&apos; idée principale de cette méthode est d&apos; éviter de construire explicitement la matrice hessienne et de construire à la place une approximation de l&apos; inverse de la dérivée seconde de la fonction à minimiser , en analysant les différents gradients successifs . cette approximation des dérivées de la fonction permet l&apos; application de la méthode de quasi-newton ( une variante de la méthode de newton ) de manière à trouver le minimum dans l&apos; espace des paramètres . la matrice hessienne n&apos; a pas besoin d&apos; être recalculée à chaque itération de l&apos; algorithme . cependant , la méthode suppose que la fonction peut être approchée localement par un développement limité quadratique autour de l&apos; optimum . b _ k \ mathbf { p } _ k = - \ nabla f ( \ mathbf { x } _ k ) . une recherche linéaire dans la direction \ mathbf { p } _ k est alors utilisée pour trouver le prochain point \ mathbf { x } _ { k + 1 } . plutôt que d&apos; imposer de calculer b _ { k + 1 } ~ comme la matrice hessienne au point \ mathbf { x } _ { k + 1 } ~ , la hessienne approchée à l&apos; itération k est mise à jour en ajoutant deux matrices . u _ k et v _ k sont des matrices symétriques de rang 1 mais ont des bases différentes . une matrice est symétrique de rang 1 si et seulement si elle peut s&apos; écrire sous la forme c.a.at , où a est une matrice colonne et c un scalaire . b _ { k + 1 } ( \ mathbf { x } _ { k + 1 } - \ mathbf { x } _ k ) = \ nabla f ( \ mathbf { x } _ { k + 1 } ) - \ nabla f ( \ mathbf { x } _ k ) . à partir d&apos; une valeur initiale \ mathbf { x } _ 0 et une matrice hessienne approchée b _ 0 les itérations suivantes sont répétées jusqu&apos; à ce que x converge vers la solution . trouver \ mathbf { p } _ k en résolvant : b _ k \ mathbf { p } _ k = - \ nabla f ( \ mathbf { x } _ k ) . effectuer une recherche linéaire pour trouver le pas optimal \ alpha _ k dans la direction trouvée dans la première partie , et ensuite mettre à jour \ mathbf { x } _ { k + 1 } = \ mathbf { x } _ k + \ alpha _ k \ mathbf { p } _ k = \ mathbf { x } _ k + \ mathbf { s } _ k. \ mathbf { y } _ k = \ nabla f ( \ mathbf { x } _ { k + 1 } ) - \ nabla f ( \ mathbf { x } _ k ) . b _ { k + 1 } = b _ k + ( \ mathbf { y } _ k \ mathbf { y } _ k ^ { \ top } ) / ( \ mathbf { y } _ k ^ { \ top } \ mathbf { s } _ k ) - ( b _ k \ mathbf { s } _ k \ mathbf { s } _ k ^ { \ top } b _ k ) / ( \ mathbf { s } _ k ^ { \ top } b _ k \ mathbf { s } _ k ) . on peut calculer l&apos; intervalle de confiance de la solution à partir de l&apos; inverse de la matrice hessienne finale . avriel , mordecai 2003 . nonlinear programming : analysis and methods . dover publishing . ( isbn 0-486-43227-0 ) . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « bfgs method » ( voir la liste des auteurs ) .
