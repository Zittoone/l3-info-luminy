vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants . en théorie de la décision et de la théorie des probabilités , un processus de décision markovien partiellement observable ( pomdp ) est une généralisation d&apos; un processus de décision markoviens ( mdp ) . comme dans un mdp , l&apos; effet des actions est incertaine mais , contrairement au mdp , l&apos; agent n&apos; a qu&apos; une information partielle de l&apos; état courant . les pomdp sont des modèles de markov cachés ( hmm ) particuliers , dans lesquels on dispose d&apos; actions probabilistes . les modèles de cette famille sont , entre autres , utilisés en intelligence artificielle pour le contrôle de systèmes complexes comme des agents intelligents . s est l&apos; ensemble fini des états possibles du système à contrôler ( il s&apos; agit des états cachés du processus ) . r : s \ times a \ times s \ to \ mathbb { r } est la fonction de récompense . elle indique la valeur réelle obtenue lorsque l&apos; on effectue l&apos; action a dans l&apos; état s et que l&apos; on arrive dans l&apos; état s &apos; . \ omega est un ensemble de symboles observables . note : il existe des variantes dans lesquelles les récompenses peuvent dépendre des actions ou des observations . les observations peuvent également dépendre des actions effectuées . il existe deux grands types d&apos; approches pour s&apos; attaquer à un problème pomdp . on peut travailler directement sur les observations &#91; 2 &#93; de \ omega sans considérer l&apos; état caché . cela n&apos; est pas sans poser de problèmes car des observations similaires peuvent être obtenues dans des états différents ( par exemple , avec l&apos; observation locale des carrefours dans un labyrinthe , on peut très bien tomber sur deux carrefours en forme de t ) . une approche possible pour discriminer ces observations consiste à garder une mémoire des observations rencontrées par le passé ( dans ce cas , on perd la propriété markovienne ) . kaebling l. p. , littman m. l. , cassandra a. r. , planning and acting in partially observable stochastic domains , artificial intelligence , vol. 101 , num . 1 – 2 , pp. 99-134 , 1998 . mccallum a. k. , reinforcement learning with selective perception and hidden state , phd thesis , university of rochester , computer science dept . , 1996 .
