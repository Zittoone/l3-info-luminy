ne pas confondre avec l&apos; algorithme du simplexe qui s&apos; applique aux problèmes d&apos; optimisation linéaires . la méthode de nelder-mead est un algorithme d&apos; optimisation non-linéaire qui a été publiéetemplate : article par john nelder et mead en 1965 . c&apos; est une méthode numérique heuristique qui cherche à minimiser une fonction continue dans un espace à plusieurs dimensions . appelée également downhill simplex method , l ’ algorithme exploite le concept de simplexe qui est un polytope de n + 1 sommets dans un espace à n dimensions . partant initialement d ’ un tel simplexe , celui-ci subit des transformations simples au cours des itérations : il se déforme , se déplace et se réduit progressivement jusqu ’ à ce que ses sommets se rapprochent d ’ un point où la fonction est localement minimale . la méthode de nelder-mead avec recuit simulé est issue du couplage entre l ’ algorithme d ’ origine et le mécanisme empirique du recuit simulé . soit n la dimension de l&apos; espace où f prend ses valeurs . l&apos; algorithme débute par la définition d&apos; un simplexe non dégénéré choisi dans cet espace . par itérations successives , le processus consiste à déterminer le point du simplexe où la fonction est maximale afin de le remplacer par la réflexion ( c&apos; est-à-dire le symétrique ) de ce point par rapport au centre de gravité des n points restants . si la valeur de la fonction en ce nouveau point est inférieure aux valeurs prises sur les autres points , le simplexe est étiré dans cette direction . sinon , il est supposé que l&apos; allure locale de la fonction est une vallée , et le simplexe est réduit par une similitude centrée sur le point du simplexe où la fonction est minimale . calcul des valeurs de la fonction f en ces points , réindexation des points de façon à avoir f ( x _ 1 ) \ leq f ( x _ 2 ) \ leq ... \ leq f ( x _ { n + 1 } ) . il suffit en fait de connaître le premier et les deux derniers . calcul de x _ 0 , centre de gravité de tous les points sauf x _ { n + 1 } . calcul de x _ r = x _ 0 + ( x _ 0-x _ { n + 1 } ) ( réflexion de x _ { n + 1 } par rapport à x _ 0 ) . si f ( x _ r ) &lt; f ( x _ n ) , calcul de x _ e = x _ 0 + 2 ( x _ 0-x _ { n + 1 } ) ( étirement du simplexe ) . si f ( x _ e ) &lt; f ( x _ r ) , remplacement de x _ { n + 1 } par x _ e , sinon , remplacement de x _ { n + 1 } par x _ r. retour à l&apos; étape 2 . si f ( x _ n ) &lt; f ( x _ r ) , calcul de x _ c = x _ { n + 1 } + 1 / 2 ( x _ 0-x _ { n + 1 } ) ( contraction du simplexe ) . si f ( x _ c ) \ leq f ( x _ n ) , remplacement de x _ { n + 1 } par x _ c et retour à l&apos; étape 2 , sinon aller à l&apos; étape 7 . homothétie de rapport 1 / 2 et de centre x _ 1 : remplacement de x _ i par x _ 1 + 1 / 2 ( x _ i-x _ 1 ) pour i &gt; = 2 . retour à l&apos; étape 2 . la généralité : une fonction continue ( sans évaluer ses dérivées ) . la simplicité de la mise en œuvre . l ’ efficacité pour une fonction non dérivable . l ’ interprétation géométrique sous-jacente . l ’ assurance d ’ obtenir une série décroissante de valeurs . 1 ) s ’ applique mal ( ou difficilement ) lorsque le domaine de définition de la fonction est complexe ou que le minimum recherché se situe dans un voisinage de la frontière . 2 ) la donnée « arbitraire » d ’ un simplexe de départ . 3 ) une dégradation des performances lorsque la dimension n augmente . 4 ) le risque que les simplexes obtenus successivement aient tendance à dégénérer ( bien que l ’ expérience montre que ce soit rarement le cas ) . pour pallier les inconvénients 1 ) et 4 ) , ainsi que d&apos; autres , le fait de redémarrer l&apos; algorithme de nelder-mead à partir de la dernière solution obtenue ( et continuer de le redémarrer jusqu&apos; à ce qu&apos; il n&apos; y ait plus d&apos; amélioration , jusqu&apos; à une précision donnée ) ne peut qu&apos; améliorer ( parfois très fortement ) la solution finale ( voir par exemple http : / / www.mathworks.com / matlabcentral / fileexchange / 33328 et http : / / arxiv.org / abs / 1104.5369 ) . de même , il est souvent conseillé de faire plusieurs exécutions de l&apos; algorithme , à partir de solutions initiales différentes ( là encore , pour diminuer l&apos; impact des inconvénients de la méthode et permettre de trouver une meilleure solution finale ) . lorsque la fonction possède de nombreux minima locaux , il arrive fréquemment de converger vers l ’ un d ’ eux et de manquer la solution . dans un tel cas , il est possible d ’ introduirew . h press , s. a. teukolsky , w. t. vetterling , b. p. flannery , « numerical recipes : the art of scientific computing » , cambridge university press , third edition ( 2007 ) dans la méthode de nelder-mead un couplage avec le mécanisme empirique du recuit simulé : à chaque itération , les valeurs effectives de la fonction aux divers sommets sont perturbées par un bruit de fond « thermique » aléatoire dont l ’ importance décroît au fur et à mesure que l ’ algorithme progresse .
