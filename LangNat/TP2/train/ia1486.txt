si vous disposez d&apos; ouvrages ou d&apos; articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici , merci de compléter l&apos; article en donnant les références utiles à sa vérifiabilité et en les liant à la section « notes et références » ( modifier l&apos; article , comment ajouter mes sources ? ) . pour accéder aux propriétés essentielles d&apos; un signal physique il peut être commode de le considérer comme une réalisation d&apos; un processus aléatoire ( voir quelques précisions dans processus continu ) . le problème est largement simplifié si le processus associé au signal peut être considéré comme un processus stationnaire , c&apos; est-à-dire si ses propriétés statistiques caractérisées par des espérances mathématiques sont indépendantes du temps . lorsque cette hypothèse est vraisemblable , le processus bâti autour du signal est rendu ergodique , les moyennes temporelles étant identiques aux moyennes d&apos; ensemble . on trouvera ci-dessous quelques éléments qui précisent un peu ces notions . l&apos; hypothèse stationnaire est admise dans de nombreux modèles théoriques , facile à réaliser dans des simulations numériques , beaucoup plus difficile voire impossible à justifier à propos d&apos; un signal réel , faute de pouvoir accéder à d&apos; autres réalisations du même processus . il faut très généralement se contenter d&apos; une justification grossière , utilisée par exemple dans l&apos; analyse des enregistrements de vagues , qui consiste à dire qu&apos; un enregistrement d&apos; une vingtaine de minutes est assez court pour assurer la stationnarité ( il est peu probable que les conditions météorologiques aient été modifiées ) mais assez long pour qu&apos; il fournisse des informations statistiques pertinentes . pour un autre point de vue voir stationnarité d&apos; une série temporelle . un processus est un ensemble x ( t ) \ , de fonctions ordinaires x ( t ) \ , , chacune d&apos; elles étant une réalisation du processus . on peut caractériser ce processus en lui associant à chaque instant t _ 0 \ , une densité de probabilité p _ x ( x , t _ 0 ) \ , . si ces moyennes d&apos; ensemble , et par conséquent la densité de probabilité , ne dépendent pas de l&apos; instant t _ 0 \ , , on parle de processus stationnaire . si , de plus , les moyennes temporelles leur sont égales , il s&apos; agit d&apos; un processus ergodique . en fait , il ne s&apos; agit là que des propriétés au premier ordre , les propriétés aux ordres supérieurs faisant intervenir la densité de probabilité jointe ( voir loi de probabilité à plusieurs variables ) à des instants différents . elles impliquent également des moyennes temporelles et des moyennes d&apos; ensemble . parmi ces dernières , la plus importante est lautocovariance statistique e &#91; x ( t _ 0 ) x ( t _ 0 + \ tau ) &#93; \ , . si le processus est stationnaire et ergodique au second ordre , elle est identique à l&apos; autocovariance temporelle , elle-même équivalente à la densité spectrale ( voir analyse spectrale ) . la stationnarité au second ordre est suffisante pour assurer la stationnarité forte lorsque le processus peut être supposé gaussien , hypothèse souvent utilisée , parfois faute de mieux . la décomposition des puissances du cosinus montre qu&apos; au premier ordre les moyennes d&apos; ensemble sont indépendantes du temps ( stationnarité ) et identiques aux moyennes temporelles ( ergodicité ) . ce résultat se généralise à tous les ordres . les paragraphes qui suivent montrent comment il est possible de construire une multitude de processus possédant ces deux propriétés en partant de tels processus sinusoïdaux . étant donnés deux processus aléatoires , les sommes de réalisations définissent les réalisations du processus somme . celui-ci est stationnaire et ergodique si les deux processus sont eux-mêmes stationnaires , ergodiques et indépendants ( voir la définition de l&apos; indépendance dans loi de probabilité à plusieurs variables ) . ce résultat se généralise à la somme d&apos; un nombre quelconque de processus . sous certaines conditions de régularité , on peut faire tendre ce nombre vers l&apos; infini . dans ce cas , il y a à chaque instant une somme de variables aléatoires indépendantes qui , selon le théorème central limite , tend vers une variable de gauss . on parle alors de processus de gauss . ce résultat s&apos; applique en particulier aux signaux physiques qui s&apos; interprètent souvent comme des sommes de signaux élémentaires sinusoïdaux ( voir analyse spectrale ) . une somme de processus sinusoïdaux se transforme en une somme de processus sinusoïdaux . la stationnarité et l&apos; ergodicité sont donc conservées . lorsque les processus composants sont indépendants , le caractère gaussien est également conservé . les moyennes du processus y ( t ) = x ( t ) ^ n \ , sont des moyennes du processus initial . la stationnarité et l&apos; ergodicité se conservent donc . elles se conservent également lorsque le monôme est remplacé par un polynôme . en passant à la limite , elles se conservent aussi pour une fonction régulière du processus initial y ( t ) = f ( x ( t ) ) \ , . si le processus d&apos; entrée est gaussien , le processus de sortie ne l&apos; est plus mais l&apos; interprétation du premier comme une somme de processus sinusoïdaux convient également pour le second . un produit de cosinus de fréquences différentes étant égal à une somme de deux cosinus de fréquences somme et différence , à la différence de ce qui se produit avec un filtre linéaire le résultat contient des fréquences plus hautes et plus basses . on obtient ainsi un processus dans lequel , contrairement à ce qui a été exigé dans sommes de processus , les composantes ne sont pas indépendantes mais le résultat reste néanmoins stationnaire et ergodique .
