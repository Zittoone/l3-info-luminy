vous pouvez partager vos connaissances en l ’ améliorant ( comment ? ) selon les recommandations des projets correspondants . si vous disposez d&apos; ouvrages ou d&apos; articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici , merci de compléter l&apos; article en donnant les références utiles à sa vérifiabilité et en les liant à la section « notes et références » ( modifier l&apos; article , comment ajouter mes sources ? ) . l&apos; algorithme de baum-welch est un algorithme utilisé pour réestimer les paramètres d&apos; un modèle de markov caché . il utilise l&apos; algorithme forward-backward et porte les noms de leonard e. baum et lloyd welch ( en ) catégorie : article contenant un appel à traduction en anglais . l&apos; algorithme de baum-welch est un cas particulier d&apos; une généralisation de l&apos; algorithme espérance-maximisation ( gem ) . un des problèmes liés aux modèles de markov cachés ( hmm ) est de trouver un modèle \ mu qui maximise la probabilité d&apos; une séquence d&apos; observations o = ( o _ { 1 } , o _ { 2 } , \ ldots , o _ { t } ) , c&apos; est-à-dire , de déterminer le modèle qui explique le mieux la séquence . le problème est qu&apos; il n&apos; est pas possible de trouver un tel modèle de façon analytique . l&apos; algorithme de baum-welch est un algorithme itératif , qui permet d&apos; estimer les paramètres du modèle qui maximisent la probabilité d&apos; une séquence d&apos; observables . l&apos; algorithme de baum-welch converge vers un maximum local . définissons d&apos; abord \ xi _ { t } { ( i , j ) } la probabilité d&apos; être dans l&apos; état i à l&apos; instant t et dans l&apos; état j à l&apos; instant t + 1 , étant donné une observation o et le modèle \ mu. avec a _ { ij } la probabilité de transition de l&apos; état i vers l&apos; état j , et b _ { j } ( o ) la probabilité d&apos; observer o lorsque l&apos; on est dans l&apos; état j , et où les valeurs \ alpha _ { t } ( i ) et \ beta _ { t } ( i ) définies ci après peuvent se calculer simplement avec l&apos; algorithme forward-backward . la figure montre une vue schématique partielle des éléments nécessaires pour le calcul de \ xi ( i , j ) . partir d&apos; un modèle initial qui peut être choisi aléatoirement . réaliser le calcul des transitions et symboles émis qui sont les plus probables selon le modèle initial . construire un nouveau modèle dans lequel la probabilité des transitions et des observations déterminée à l&apos; étape précédente augmente . pour la séquence des observables en question , le modèle aura désormais une probabilité plus élevée que le modèle précédent . ce processus de formation est répétée plusieurs fois jusqu&apos; à ce qu&apos; il n&apos; y ait plus d&apos; amélioration entre le modèle recalculé et l&apos; ancien .
