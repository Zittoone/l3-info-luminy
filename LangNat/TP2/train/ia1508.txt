pour les articles homonymes , voir régression . une réorganisation et une clarification du contenu est nécessaire . discutez des points à améliorer en page de discussion . la régression logistique ou modèle logit est un modèle de régression binomiale . comme pour tous les modèles de régression binomiale , il s&apos; agit de modéliser l&apos; effet d&apos; un vecteur de variables aléatoires ( x _ 1 , \ ldots , x _ k ) sur une variable aléatoire binomiale génériquement notée y. la régression logistique est un cas particulier du modèle linéaire généralisé . d&apos; après de palma et thisse , la première mention du modèle logit vient de joseph berkson en 1944template : article et 1951template : article , template : article . en médecine , elle permet par exemple de trouver les facteurs qui caractérisent un groupe de sujets malades par rapport à des sujets sains . dans le domaine des assurances , elle permet de cibler une fraction de la clientèle qui sera sensible à une police d ’ assurance sur tel ou tel risque particulier . dans le domaine bancaire , pour détecter les groupes à risque lors de la souscription d ’ un crédit . en économétrie , pour expliquer une variable discrète . par exemple , les intentions de vote aux élections . par exemple , vincent loonis utilise un modèle de régression logistique pour étudier les déterminants de la réélection des députés français depuis les débuts de la iiie républiquetemplate : article . dans ce qui suit , nous noterons y la variable à prédire ( variable expliquée ) , x = ( x _ 1 , x _ 2 , ... , x _ j ) les variables prédictives ( variables explicatives ) . dans le cadre de la régression logistique binaire , la variable y prend deux modalités possibles \ { 1 , 0 \ } . les variables x _ j sont exclusivement continues ou binaires . pour effectuer l&apos; estimation , nous disposons d&apos; un échantillon \ omega d&apos; effectif n. nous notons n _ 1 ( resp. n _ 0 ) les observations correspondants à la modalité 1 ( resp . 0 ) de y. p ( y = 1 ) ( resp . p ( y = 0 ) ) est la probabilité a priori pour que y = 1 ( resp . y = 0 ) . pour simplifier , nous écrirons p ( 1 ) ( resp. p ( 0 ) ) . enfin , la probabilité a posteriori d&apos; obtenir la modalité 1 de y ( resp . 0 ) sachant la valeur prise par xest représentée par p ( 1 \ vert x ) ( resp. p ( 0 \ vert x ) ) . une vaste classe de distributions répondent à cette spécification , la distribution multinormale décrite en analyse discriminante linéaire par exemple , mais également d ’ autres distributions , notamment celles où les variables explicatives sont booléennes ( 0 / 1 ) . par rapport à l ’ analyse discriminante toujours , ce ne sont plus les densités conditionnelles p ( x \ vert 1 ) et p ( x \ vert 0 ) qui sont modélisées mais le rapport de ces densités . la restriction introduite par l&apos; hypothèse est moins forte . il s ’ agit bien d ’ une « régression » car on veut montrer une relation de dépendance entre une variable à expliquer et une série de variables explicatives . il s ’ agit d ’ une régression « logistique » car la loi de probabilité est modélisée à partir d ’ une loi logistique . à partir d ’ un fichier de données , nous devons estimer les coefficients b _ j de la fonction logit . il est très rare de disposer pour chaque combinaison possible des x _ j , \ ( j = 1 , ... , j ) , même si ces variables sont toutes binaires , de suffisamment d ’ observations pour disposer d ’ une estimation fiable des probabilités p ( 1 \ vert x ) et p ( 0 \ vert x ) . la méthode des moindres carrés ordinaire est exclue . la solution passe par une autre approche : la maximisation de la vraisemblance . les paramètres \ hat b _ j ( j = 0 , ... , j ) qui maximisent cette quantité sont les estimateurs du maximum de vraisemblance de la régression logistique . dans la pratique , les logiciels utilisent une procédure approchée pour obtenir une solution satisfaisante de la maximisation ci-dessus . ce qui explique d ’ ailleurs pourquoi ils ne fournissent pas toujours des coefficients strictement identiques . les résultats dépendent de l ’ algorithme utilisé et de la précision adoptée lors du paramétrage du calcul. les itérations sont interrompues lorsque la différence entre deux vecteurs de solutions successifs est négligeable . cette dernière matrice , dite matrice hessienne , est intéressante car son inverse représente l ’ estimation de la matrice de variance covariance de \ beta \ , . elle sera mise en contribution dans les différents tests d ’ hypothèses pour évaluer la significativité des coefficients . l ’ objectif étant de produire un modèle permettant de prédire avec le plus de précision possible les valeurs prises par une variable catégorielle y , une approche privilégiée pour évaluer la qualité du modèle serait de confronter les valeurs prédites avec les vraies valeurs prises par y : c ’ est le rôle de la matrice de confusion . on en déduit alors un indicateur simple , le taux d ’ erreur ou le taux de mauvais classement , qui est le rapport entre le nombre de mauvaises prédictions et la taille de l ’ échantillon . lorsque la matrice de confusion est construite sur les données qui ont servi à élaborer le modèle , le taux d ’ erreur est souvent trop optimiste , ne reflétant pas les performances réelles du modèle dans la population . pour que l ’ évaluation ne soit pas biaisée , il est conseillé de construire cette matrice sur un échantillon à part , dit échantillon de test . par opposition à l ’ échantillon d ’ apprentissage , il n ’ aura pas participé à la construction du modèle . le principal intérêt de cette méthode est qu ’ elle permet de comparer n ’ importe quelle méthode de classement et sélectionner ainsi celle qui s ’ avère être la plus performante face à un problème donné . il est possible d ’ exploiter un schéma probabiliste pour effectuer des tests d ’ hypothèses sur la validité du modèle . ces tests reposent sur la distribution asymptotique des estimateurs du maximum de vraisemblance . la statistique du rapport de vraisemblance s ’ écrit \ lambda = 2 \ times &#91; l ( j + 1 ) -l ( 1 ) &#93; , elle suit une loi du \ chi ^ 2 à j degrés de libertés. l ( 1 ) la log vraisemblance du modèle réduit à la seule constante . si la probabilité critique ( la p-value ) est inférieure au niveau de signification que l ’ on s ’ est fixé , on peut considérer que le modèle est globalement significatif . reste à savoir quelles sont les variables qui jouent réellement un rôle dans cette relation . dans le cas où l ’ on cherche à tester le rôle significatif d ’ une variable . nous réalisons le test suivant h _ 0 : b _ j = 0 , contre h _ 1 : b _ j \ ne 0 . la statistique de wald répond à ce test , elle s ’ écrit w = \ frac { \ hat b ^ 2 } { \ hat v ( \ hat b ) } , elle suit une loi du \ chi ^ 2 à 1 degré de liberté . n.b. : la variance estimée du coefficient \ hat b _ j est lue dans l ’ inverse de la matrice hessienne vue précédemment . les deux tests ci-dessus sont des cas particuliers du test de significativité d ’ un bloc de coefficients . ils découlent du critère de la « déviance » qui compare la vraisemblance entre le modèle courant et le modèle saturé ( le modèle dans lequel nous avons tous les paramètres ) . l ’ hypothèse nulle s ’ écrit dans ce cas h _ 0 : \ beta ( q ) = 0 , où \ beta ( q ) représente un ensemble de q \ , coefficients simultanément à zéro . la statistique du test w ( q ) = 2 \ times &#91; l ( j + 1 ) -l ( j + 1-q ) &#93; suit une loi du \ chi ^ 2 à q degrés de libertés . ce test peut être très utile lorsque nous voulons tester le rôle d ’ une variable explicative catégorielle à q + 1 modalités dans le modèle . après recodage , nous introduisons effectivement q variables indicatrices dans le modèle . pour évaluer le rôle de la variable catégorielle prise dans son ensemble , quelle que soit la modalité considérée , nous devons tester simultanément les coefficients associés aux variables indicatrices . d ’ autres procédures d ’ évaluation sont couramment citées s ’ agissant de la régression logistique . nous noterons entre autres le test de hosmer-lemeshow qui s ’ appuie sur le « score » ( la probabilité d ’ affectation à un groupe ) pour ordonner les observations . en cela , elle se rapproche d ’ autres procédés d ’ évaluation de l ’ apprentissage telles que les courbes roc qui sont nettement plus riches d ’ informations que la simple matrice de confusion et le taux d ’ erreur associé . à partir des données disponibles sur le site du cours en ligne de régression logistique ( paul-marie bernard , université du québec – chapitre 5 ) , nous avons construit un modèle de prédiction qui vise à expliquer le « faible poids ( oui / non ) » d ’ un bébé à la naissance . les variables explicatives sont : fume ( le fait de fumer ou pas pendant la grossesse ) , prem ( historique de prématurés aux accouchements antérieurs ) , ht ( historique de l ’ hypertension ) , visite ( nombre de visites chez le médecin durant le premier trimestre de grossesse ) , age ( âge de la mère ) , pdsm ( poids de la mère durant les périodes des dernières menstruations ) , scol ( niveau de scolarité de la mère : = 1 : &lt; 12 ans , = 2 : 12-15 ans , = 3 : &gt; 15 ans ) . toutes les variables explicatives ont été considérées continues dans cette analyse . dans certains cas , scol par exemple , il serait peut être plus judicieux de les coder en variables indicatrices . les résultats sont consignés dans le tableau suivant . la statistique du rapport de vraisemblance lambda est égale à 31.77 , la probabilité critique associée est 0 . le modèle est donc globalement très significatif , il existe bien une relation entre les variables explicatives et la variable expliquée . en étudiant individuellement les coefficients liés à chaque variable explicative , au risque de 5 % , nous constatons que fume , prem et ht sont néfastes au poids du bébé à la naissance ( entraînent un faible poids du bébé ) ; pdsm et scol en revanche semblent jouer dans le sens d ’ un poids plus élevé du bébé . visite et age ne semblent pas jouer de rôle significatif dans cette analyse . cette première analyse peut être affinée en procédant à une sélection de variables , en étudiant le rôle concomitant de certaines variables , etc. le succès de la régression logistique repose justement en grande partie sur la multiplicité des outils d ’ interprétations qu ’ elle propose . avec les notions d ’ odds , d ’ odds ratios et de risque relatif , calculés sur les variables dichotomiques , continues ou sur des combinaisons de variables , le statisticien peut analyser finement les causalités et mettre en évidence les facteurs qui pèsent réellement sur la variable à expliquer . prenons l ’ observation suivante x ( \ omega ) \ , = ( fume = 1 « oui » ; prem = 1 « un prématuré dans l ’ historique de la mère » ; ht = 0 « non » ; visite = 0 « pas de visite chez le médecin pendant le premier trimestre de grossesse » ; age = 28 ; pdsm = 54.55 ; scol = 2 « entre 12 et 15 ans » ) . en appliquant l ’ équation ci-dessus , nous trouvons 2.893 + 0.853 \ times 1 + 0.691 \ times 1 + 1.744 \ times 0 + 0.030 \ times 0 - 0.028 \ times 28 - 0.038 \ times 54.55 - 0.660 \ times 2 = 0.28125 . le modèle donc prédit un bébé de faible poids pour cette personne . ce qui est justifié puisqu ’ il s ’ agit de l ’ observation n ° 131 de notre fichier , et elle a donné lieu effectivement à la naissance d ’ un enfant de faible poids . la règle d ’ affectation ci-dessus est valide si l ’ échantillon est issu d ’ un tirage au hasard dans la population . ce n ’ est pas toujours le cas . dans de nombreux domaines , nous fixons au préalable les effectifs des classes y = 1 et y = 0 , puis nous procédons au recueil des données dans chacun des groupes . on parle alors de tirage rétrospectif . il est dès lors nécessaire de procéder à un redressement . si les coefficients associés aux variables de la fonction logit ne sont pas modifiés , la constante en revanche doit être corrigée en tenant compte des effectifs dans chaque classe ( n _ 1 et n _ 0 ) et des vraies probabilités a priori p ( 1 ) et p ( 0 ) ( cf. les références ci-dessous ) . la régression logistique s ’ applique directement lorsque les variables explicatives sont continues ou dichotomiques . lorsqu ’ elles sont catégorielles , il est nécessaire de procéder à un recodage . le plus simple est le codage binaire . prenons l ’ exemple d ’ une variable habitat prenons trois modalités { ville , périphérie , autres } . nous créerons alors deux variables binaires : « habitat _ ville » , « habitat _ périphérie » . la dernière modalité se déduit des deux autres , lorsque les deux variables prennent simultanément la valeur 0 , cela indique que l ’ observation correspond à « habitat = autres » . enfin , il est possible de réaliser une régression logistique pour prédire les valeurs d ’ une variable catégorielle comportant k ( k &gt; 2 ) modalités . on parle de régression logistique polytomique . la procédure repose sur la désignation d ’ un groupe de référence , elle produit alors ( k-1 ) combinaisons linéaires pour la prédiction . l ’ interprétation des coefficients est moins évidente dans ce cas . kleinbaum d.g. , logistic regression . a self-learning text , springer-verlag , 1994 . kleinbaum d.g. , kupper l.l. , muller e.m. , applied regression analysis and other multivariate methods , pws-kent publishing company , boston , 1988 . tanagra , un logiciel gratuit pour l&apos; enseignement et la recherche .
