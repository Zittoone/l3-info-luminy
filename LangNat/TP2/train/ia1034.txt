en mathématiques , l&apos; algorithme de gauss-newton est une méthode de résolution des problèmes de moindres carrés non linéaires . elle peut être vue comme une modification de la méthode de newton dans le cas multidimensionnel afin de trouver le minimum d&apos; une fonction ( à plusieurs variables ) . mais l&apos; algorithme de gauss-newton est totalement spécifique à la minimisation d&apos; une somme de fonctions au carré et présente le grand avantage de ne pas nécessiter les dérivées secondes , parfois complexes à calculer . les problèmes de moindres carrés non linéaires surviennent par exemple dans les problèmes de régressions non linéaires , où des paramètres du modèle sont recherchés afin de correspondre au mieux aux observations disponibles . cette méthode est due à carl friedrich gauss . s ( \ boldsymbol \ beta ) = \ sum _ { i = 1 } ^ m r _ i ^ 2 ( \ boldsymbol \ beta ) . \ mathbf { j _ r ^ t r } . ici , on note par r le vecteur des fonctions ri , et par jr la matrice jacobienne m × n de r par rapport à β , tous les deux évalués en βs . la matrice transposée est notée à l&apos; aide de l&apos; exposant t. r _ i ( \ boldsymbol \ beta ) = y _ i - f ( x _ i , \ boldsymbol \ beta ) . \ left ( \ mathbf { j _ f ^ t j _ f } \ right ) \ delta \ boldsymbol \ beta = { \ mathbf { j _ f ^ t r } } . l&apos; ensemble du terme de droite est calculable car ne dépend que de \ boldsymbol \ beta ^ s et permet de trouver l&apos; estimation suivante . l&apos; hypothèse m ≥ n est nécessaire , car dans le cas contraire la matrice \ mathbf { j _ r ^ t j _ r } serait non inversible et les équations normales ne pourraient être résolues. ce qui peut se faire par la technique classique de régression linéaire et qui fournit exactement les équations normales . les équations normales sont un système de m équations linéaires d&apos; inconnu \ delta \ boldsymbol \ beta . ce système peut se résoudre en une étape , en utilisant la factorisation de cholesky ou , encore mieux , la décomposition qr de jr . pour de grands systèmes , une méthode itérative telle que la méthode du gradient conjugué peut être plus efficace . s&apos; il existe une dépendance linéaire entre les colonnes jr , la méthode échouera car \ mathbf { j _ r ^ t j _ r } deviendra singulier . notons enfin que la méthode de gauss-newton est efficace lorsque l&apos; erreur quadratique finale est faible , ou bien lorsque la non-linéarité est « peu prononcée » template : ouvrage . la méthode est en particulier sensible à la présence de points « aberrants » ( c&apos; est-à-dire situés loin de la courbe modèle ) . dans cet exemple , l&apos; algorithme de gauss – newton est utilisé pour ajuster un modèle en minimisant la somme des carrés entre les observations et les prévisions du modèle . dans une expérience de biologie , on étudie la relation entre la concentration du substrat &#91; s &#93; et la vitesse de réaction dans une réaction enzymatique à partir de données reportées dans le tableau suivant . l&apos; estimation par moindres carrés porte sur les paramètres v _ { \ max } et k _ m. \ frac { \ partial r _ i } { \ partial \ beta _ 1 } = - \ frac { x _ i } { \ beta _ 2 + x _ i } , \ \ frac { \ partial r _ i } { \ partial \ beta _ 2 } = \ frac { \ beta _ 1x _ i } { \ left ( \ beta _ 2 + x _ i \ right ) ^ 2 } . la figure ci-contre permet de juger de l&apos; adéquation du modèle aux données en comparant la courbe ajustée ( bleue ) aux observations ( rouge ) . \ end { align } \ right . la régression consiste à ajuster les paramètres a ( 1 ) , a ( 2 ) , a ( 3 ) et a ( 4 ) . demie-largeur à mi-hauteur à droite valant hd = ( 0,45 + 0,03 ) . a ( 3 , 4 ) = 8 ( hg , d / 2,35 ) 2 . a ( 4 ) = 0,313 , &quot; 0,3 . sur la figure ci-contre , les points expérimentaux ( au nombre de 200 ) forment la courbe bleue , le modèle ajusté est représenté par la courbe rouge . si l&apos; on part du jeu de paramètres initiaux arbitraire &#91; 1 , 1 , 1 , 1 &#93; , l&apos; algorithme converge en 10 étape à condition d&apos; utiliser un facteur d&apos; amortissement α ajusté automatiquement à chaque étape ( voir ci-après ) . on peut démontrer que l&apos; incrément \ delta \ beta est une direction de descente pour s björck , p. 260 , et que si l&apos; algorithme converge , alors la limite est un point stationnaire pour la somme des carrés s. toutefois , la convergence n&apos; est pas garantie , pas plus qu&apos; une convergence locale contrairement à la méthode de newton . la vitesse de convergence de l&apos; algorithme de gauss – newton peut approcher la vitesse quadratiquebjörck , p. 341-342 . l&apos; algorithme peut converger lentement voire ne pas converger du tout si le point de départ de l&apos; algorithme est trop loin du minimum ou si la matrice \ mathbf { j _ r ^ t j _ r } est mal conditionnée. r _ 2 ( \ beta ) &amp; = \ lambda \ beta ^ 2 + \ beta - 1 . dans ce qui suit , l&apos; algorithme de gauss – newton sera tiré de l&apos; algorithme d&apos; optimisation de newton ; par conséquent , la vitesse de convergence sera au plus quadratique. g _ j = 2 \ sum _ { i = 1 } ^ m r _ i \ frac { \ partial r _ i } { \ partial \ beta _ j } . h _ { jk } = 2 \ sum _ { i = 1 } ^ m \ left ( \ frac { \ partial r _ i } { \ partial \ beta _ j } \ frac { \ partial r _ i } { \ partial \ beta _ k } + r _ i \ frac { \ partial ^ 2 r _ i } { \ partial \ beta _ j \ partial \ beta _ k } \ right ) . \ boldsymbol { \ beta } ^ { s + 1 } = \ boldsymbol \ beta ^ s + \ delta \ boldsymbol \ beta ; \ \ delta \ boldsymbol \ beta = - \ mathbf { \ left ( j _ r ^ t j _ r \ right ) ^ { -1 } j _ r ^ t r } . les fonctions sont seulement faiblement non linéaires , si bien que \ frac { \ partial ^ 2 r _ i } { \ partial \ beta _ j \ partial \ beta _ k } est relativement petit en magnitude . \ boldsymbol \ beta ^ { s + 1 } = \ boldsymbol \ beta ^ s + \ alpha \ \ delta \ boldsymbol \ beta. en l&apos; augmentant pour la fois suivante lorsque la condition est remplie ( par exemple en le rapprochant de 1 avec prenant ( 1 + α ) / 2 ) . cet démarche n&apos; est pas optimale , mais réduit l&apos; effort nécessaire à déterminer α. où d est une matrice diagonale positive . remarquons que lorsque d est la matrice identité et que \ lambda \ to + \ infty , alors \ delta \ boldsymbol \ beta / \ lambda \ to \ mathbf { j } ^ t \ mathbf { r } , par conséquent la direction de \ delta \ boldsymbol \ beta s&apos; approche de la direction du gradient \ mathbf { j } ^ t \ mathbf { r } . le paramètre de marquardt , \ lambda , peut aussi être optimisé par une méthode de recherche linéaire , mais ceci rend la méthode fort inefficace dans la mesure où le vecteur d&apos; incrément doit être re-calculé à chaque fois que \ lambda change . dans une méthode quasi-newton , comme celle due à davidon , fletcher et powell , une estimation de la matrice hessienne , \ frac { \ partial ^ 2 s } { \ partial \ beta _ j \ partial \ beta _ k } , est approchée numériquement en utilisant les premières dérivées \ frac { \ partial r _ i } { \ partial \ beta _ j } . une autre méthode pour résoudre les problèmes de moindres carrés en utilisant seulement les dérivées premières est l&apos; algorithme du gradient . toutefois , cette méthode ne prend pas en compte les dérivées secondes , même sommairement . par conséquent , cette méthode s&apos; avère particulièrement inefficace pour beaucoup de fonctions . ( en ) cet article est partiellement ou en totalité issu de l ’ article de wikipédia en anglais intitulé « gauss – newton algorithm » ( voir la liste des auteurs ) . calcul d&apos; incertitudes un livre de 246 pages qui étudie la régression linéaire et non-linéaire . l&apos; algorithme est détaillé et appliqué à l&apos; expérience de biologie traitée en exemple dans cet article ( page 79 avec les incertitudes sur les valeurs estimées ) .
