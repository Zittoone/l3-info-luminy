pour les articles homonymes , voir mmc . un modèle de markov caché ( mmc ) — en anglais : hidden markov model ( hmm ) — , ou plus correctement ( mais non employé ) automate de markov à états cachés , est un modèle statistique dans lequel le système modélisé est supposé être un processus markovien de paramètres inconnus . contrairement à une chaîne de markov classique , où les transitions prises sont inconnues de l&apos; utilisateur mais où les états d&apos; une exécution sont connus , dans un modèle de markov caché , les états d&apos; une exécution sont inconnus de l&apos; utilisateur ( seul certains paramètres , comme la température , etc. sont connus de l&apos; utilisateur ) . les modèles de markov cachés sont massivement utilisés notamment en reconnaissance de formes , en intelligence artificielle ou encore en traitement automatique du langage naturel . imaginons un jeu simple , avec des sacs en papier ( opaques ) contenant des jetons numérotés . à chaque tour du jeu nous tirons un jeton d&apos; un sac et , en fonction du jeton , passons à un autre sac . après chaque tour , le jeton est remis dans le sac , nous notons enfin la séquence des numéros tirés . nous disposons de deux sacs , appelés a et b , ainsi que d&apos; un ensemble de jetons numérotés a et b. dans chaque sac nous plaçons un certain nombre de jetons a et un certain nombre de jetons b : dans cet exemple , nous plaçons dans le sac a 19 jetons b et un seul jeton a. dans le sac b nous plaçons 4 jetons a et un seul jeton b. nous commençons par piocher un jeton au hasard dans le sac a. si l&apos; on pioche un jeton a , on reste sur ce sac , si l&apos; on pioche un jeton b , on passe au sac b. on note également quel jeton a été tiré et on le remet dans le sac . on recommence cette étape avec le sac en cours , jusqu&apos; à ce que le jeu s&apos; arrête ( au bon vouloir du joueur ) . ce jeu peut-être modélisé par une chaîne de markov : chaque sac représente un état , la valeur du jeton donne la transition , la proportion de jeton d&apos; une valeur est la probabilité de la transition . des sacs de sortie pour générer la séquence . à partir de la séquence générée , il sera généralement impossible de déterminer quels tirages ont conduit à quelle séquence , la séquence de tirage dans les sacs donnant les transitions est inconnue , c&apos; est pourquoi on parle de sacs en papier cachés . b contient un jeton j et quatre jetons k. on recommence ces opérations autant de fois que le joueur le souhaite . à chaque étape , on tire donc un jeton dans chaque sac d&apos; un même groupe , à savoir a et a &apos; ou b et b &apos; , ce qui permet d&apos; avoir une valeur ( j ou k ) qui n&apos; indique pas directement la transition . la séquence des transitions , inconnue ( ce sont les valeurs a et b contenues dans les sacs a et b ) . on observe que des séquences de transitions identiques peuvent donner des sorties différentes , et vice-versa . ce jeu peut être modélisé par un automate de markov à états cachés : les groupes de sacs sont les états , les tirages donnant le groupe de tirages suivant sont les transitions ( avec la probabilité associée en fonction de la proportion des jetons dans les sacs a ou b ) , les sacs de sortie donnent les valeurs de sortie de l&apos; automate ( avec la probabilité associée en fonction de la proportion des jetons dans les sacs a &apos; ou b &apos; ) . les flèches en pointillés indiquent les sorties probables à chaque passage dans un état . \ forall i , \ sum _ { k } b _ { i } ( k ) = 1 la somme des probabilités des émissions partant d&apos; un état est égale à 1 . étant donné une séquence de sortie , retrouver l&apos; ensemble d&apos; états le plus probable et les probabilités des sorties sur chaque état . se résout avec l&apos; algorithme de baum-welch , appelé aussi algorithme forward-backward . reconnaissance de la parole . traitement automatique du langage naturel ( traduction automatique , étiquetage de texte , reconstruction de texte bruités … ) . reconnaissance de l&apos; écriture manuscrite . bio-informatique , notamment pour la prédiction de gènes . les hmm ont été décrits pour la première fois dans une série de publication de statistiques par leonard e. baum et d&apos; autres auteurs après 1965 . ils ont été appliqués dès la fin des années 1970 à la reconnaissance vocaletemplate : article &#91; 1 &#93; . . dans la seconde moitié des années 1980 , les hmm ont commencé à être appliqués à l&apos; analyse de séquences biologique , en particulier l&apos; adn template : article . . les réseaux bayésiens dynamiques introduits au tout début des années 1990 généralisent le principe des hmm . l&apos; algorithme de baum-welch est un cas particulier de l&apos; algorithme espérance-maximisation et utilise l&apos; algorithme forward-backward . il permet de ré-estimer les paramètres de manière itérative .
