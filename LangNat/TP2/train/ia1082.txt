l&apos; apprentissage par renforcement hors ligne ( ou batch ) est un cas particulier de l&apos; apprentissage par renforcement , qui est une classe de problèmes d&apos; apprentissage automatique dont l&apos; objectif est de déterminer à partir d&apos; expériences une stratégie ( ou politique ) permettant à un agent de maximiser une récompense numérique au cours du temps . dans le cadre de l&apos; apprentissage par renforcement purement hors ligne , l&apos; agent ne peut pas interagir avec l&apos; environnement : une base d&apos; apprentissage lui est fournie au départ et il l&apos; exploite pour apprendre une politique . contrastant avec les algorithmes en-ligne , où l&apos; agent à la possibilité d&apos; interagir comme bon lui semble avec l&apos; environnement , les algorithmes hors-ligne tentent d&apos; exploiter au maximum les exemples d&apos; apprentissage dont ils disposent , sans compter uniquement sur la possibilité d&apos; exploration . cette approche est donc particulièrement avantageuse quand il n&apos; est pas possible d&apos; effectuer des expériences ou lorsque ces expériences sont coûteuses ( casse de matériel possible , obligation d&apos; avoir recours à une assistance humaine pendant les expériences , etc ) . en général cependant , les techniques d&apos; apprentissage par renforcement batch peuvent être utilisées dans un cadre plus large , où la base d&apos; apprentissage peut évoluer au cours du temps . l&apos; agent peut alors alterner entre des phases d&apos; exploration et des phases d&apos; apprentissage . les algorithmes hors-ligne sont en général des adaptations d&apos; autres algorithmes comme le q-learning , eux-mêmes inspirés par les algorithmes de programmation dynamique résolvant les mdps . définir précisément ce qu&apos; est ou n&apos; est pas un algorithme hors-ligne n&apos; est pas forcément évident . une des positions que l&apos; on peut adopter est de considérer &quot; hors-ligne &quot; tout algorithme qui n&apos; est pas purement en-ligne . sans interaction sur l&apos; environnement et en prenant toutes les expériences disponibles : pure batch . le principe du batch pure est d&apos; être complètement hors ligne . c&apos; est-à-dire qu&apos; il prend une fois les expériences et apprend sa politique optimale sur ce jeu uniquement . si une nouvelle expérience arrive , elle ne peut pas être incluse toute seule , il faut l&apos; inclure dans l&apos; ensemble des expériences précédentes et réapprendre sur ce nouvel ensemble . avec interaction sur l&apos; environnement et en prenant toutes les expériences disponibles : growing batch . dans la situation de pure-batch il peut être difficile de trouver une politique optimale si l&apos; ensemble des expériences suit une distribution qui n&apos; est pas celle du système réel sous-jacent . cela peut se traduire par des représentations de probabilités de transitions erronées ou des transitions manquantes . la meilleure solution pour pallier ce manque d&apos; information est donc d ’ interagir avec le système quand c&apos; est possible et d&apos; utiliser à chaque fois l&apos; ensemble de l&apos; information disponible . c&apos; est le principe d&apos; exploration ( effectué par l ’ interaction ) et d&apos; exploitation ( effectué par les expériences ) que l&apos; on retrouve dans les méthodes classique d&apos; apprentissage par renforcement . avec interaction sur l&apos; environnement et en ne prenant pas toutes les expériences disponibles : semi-batch . le semi-batch se situe entre les deux précédentes . le système met à jour plusieurs expériences en même temps ce qui fait que le semi-batch n&apos; est pas purement en ligne mais il n&apos; utilise qu&apos; une seule fois les expériences , ce qui fait qu&apos; il n&apos; est pas complètement hors-ligne . la modélisation classique de l&apos; apprentissage par renforcement ( en ligne ou hors ligne ) utilise la notion sous-jacente de processus de décision markovien ( mdp ) . les politiques sont notées \ pi . le moutain car est un problème classique de la littérature de l&apos; apprentissage par renforcement , dans lequel une voiture doit atteindre le haut d&apos; une colline . les actions sont les accélérations possibles de la voiture : accélérer vers l&apos; avant , vers arrière , ou rester en roue libre . ce problème est un autre exemple classique , dans lequel un agent doit contrôler une perche munie d&apos; un poids à l&apos; aide d&apos; un moteur pour la placer en équilibre en position verticale , avec le poids en haut . le moteur n&apos; étant pas assez puissant pour faire passer de la perche de sa position initiale à la position d&apos; équilibre d&apos; un coup , l&apos; agent doit apprendre à faire balancer la perche . la fonction de récompense vaut 0 si la perche est à la position d&apos; équilibre ( avec une marge de 0.5 ° ) et -1 sinon . il existe une grande variété d&apos; algorithmes hors-ligne dans la littérature , mais de manière générale ils brodent tous autour de quelques principes fondamentaux qui les distinguent des algorithmes purement en-ligne ou de ceux dans lesquels le mdp est connu . initialement pensés pour résoudre les difficultés spécifiques à l&apos; apprentissage hors-ligne , ces principes sont également avantageux dans un cadre plus général . pour un q-learning purement en ligne , l&apos; agent est libre d&apos; explorer et de générer de l&apos; expérience comme bon lui semble . 2. faire converger l&apos; algorithme , car celui-ci demande un grand nombre d&apos; itérations pour propager les informations et finalement se stabiliser vers une politique optimale . dans la situation hors ligne , on ne peut se contenter de n&apos; utiliser qu&apos; une seule fois une expérience donnée car le nombre d&apos; expériences est limité par définition , et ne permettrait pas de faire converger l&apos; algorithme . cette difficulté est levée en &quot; rejouant &quot; plusieurs fois les mêmes expériences , c&apos; est-à-dire que le jeu de données disponible est exploité encore et encore comme s&apos; il s&apos; agissait de nouvelles expériences . dans les méthodes classiques d&apos; apprentissage par renforcement , il est commun de faire la mise à jour de la fonction de valeur de manière désynchronisée . c&apos; est-à-dire qu&apos; après chaque observation d&apos; une transition , la valeur d&apos; un état ou d&apos; un état-action est mise à jour localement et les autres états sont laissés inchangés . ensuite , ce n&apos; est que lors de nouvelles interactions que les autres états seront évalués sur la base de la valeur mise à jour précédemment . les algorithmes batch , dans le but d&apos; exploiter au maximum les données disponibles , de s&apos; adapter à des espaces d&apos; états continus et de contrer certains problèmes d&apos; instabilités lors de l&apos; apprentissage ( cas de non convergence ) , ne se contentent pas de stocker simplement les valeurs pour chaque état ou action et d&apos; appliquer des mises à jour de type programmation dynamique . ils ajoutent au-dessus une forme ou une autre de globalisation et de régularisation des mises à jour qui peut se voir comme un apprentissage supervisé des fonctions de valeurs . par exemple , dans les algorithmes que nous présentons ci-dessous , on trouve l&apos; utilisation d&apos; un noyau gaussien qui permet de moyenner localement les valeurs , d&apos; une base de fonctions dans laquelle on exprime la fonction de valeurs , ou plus explicitement d&apos; un algorithme d&apos; apprentissage supervisé ( réseau de neurones , k-plus-proches-voisins ... ) pour apprendre la fonction de valeurs . de cette manière chaque expérience utilisée va permettre de mettre à jour globalement l&apos; ensemble de la fonction de valeurs ( des états ou états-actions ) sans nécessiter d&apos; attendre que de multiples itérations propagent l&apos; information de manière plus ou moins aléatoire . l&apos; algorithme kernel-based approximate dynamic programming ( kadp ) suppose qu&apos; un ensemble de transitions observées de façon arbitraire ( s , a , r , s &apos; ) \ in f est à sa disposition . il suppose ensuite que l&apos; espace des états est muni d&apos; une distance , à partir de laquelle la notion de moyennage basé sur un noyau gaussien prend sens . ce noyau est utilisé pour mettre à jour itérativement la fonction de valeurs v en tous les états , à partir du jeu d&apos; échantillons f. on voit bien ici l&apos; application de l&apos; idée d&apos; experience replay ( on utilise répétitivement toutes les données ) et celle de fitting puisque le noyau permet à l&apos; ensemble des valeurs d&apos; être mise à jour par chaque transition. avec f _ a un sous ensemble de f qui n&apos; utilise que les actions a. cette équation calcule la moyenne pondérée de la mise à jour habituelle : r + \ gamma \ hat { v } ^ i ( s &apos; ) avec comme pondération k ( s , \ sigma ) . le noyau est choisi de telle sorte que les transitions les plus éloignées aient une moins grande influence que les plus proches . l&apos; algorithme va donc itérer sur les équations définies au-dessus pour évaluer \ hat { v } ^ i avec i = 1,2 , \ dots et espérer converger vers une unique solution \ hat { v } . le problème du choix du portfolio optimal décrit dans template : en dirk ormoneit and peter glynn , « kernel-based reinforcement learning in average-cost problems : an application to optimal portfolio choice » , advances in neural information processing systems , 2000 , template : p. est un cas pratique du kadp . c&apos; est un problème d&apos; investissement financier où l&apos; agent décide d&apos; acheter ou non des actions . le but étant à la fin de la simulation de maximiser le gain de l&apos; agent . l&apos; agent va donc au cours du temps investir ( ou pas ) dans une action pour maximiser sa richesse avec comme but d&apos; être le plus riche possible à la fin du temps maximum défini . pour rendre le problème le plus réaliste possible , l&apos; agent adopte un comportement tel qu&apos; il a peur de perdre ses gains autant qu&apos; il a l&apos; envie d&apos; en gagner . il crée une base de donnée d&apos; apprentissage p constituée des couples entrée-sortie ( s , a ) \ to r + \ gamma \ max _ { a &apos; \ in a } \ hat { q } ^ i ( s , a ) pour chaque transition ( s , a , r , s &apos; ) \ in \ mathcal { f } . la valeur ainsi calculée pour chaque ( s , a ) est en quelque sorte la « meilleure estimation » que l&apos; on puisse faire de la valeur de q ( s , a ) dans l&apos; état actuel des connaissances décrit par q ^ i. il en déduit une fonction \ hat { q } ^ { i + 1 } à partir de l&apos; ensemble p en utilisant une méthode d&apos; apprentissage supervisé . cette fonction est donc une approximation de la fonction q ^ { i + 1 } obtenue à la i + 1ème itération de l&apos; algorithme d&apos; itération sur la valeur des états-actions . il incrémente i et retourne à la première étape tant qu&apos; une condition d&apos; arrêt n&apos; est pas vérifiée . cet algorithme peut-être utilisé avec n&apos; importe quelle méthode d&apos; apprentissage supervisé , en particulier des méthodes non paramétriques , qui ne font aucune hypothèse sur la forme de q. cela permet d&apos; obtenir des approximations précises indépendamment de la taille des espaces d&apos; états et d&apos; actions , y compris dans le cas d&apos; espaces continus . de manière générale , la convergence de l&apos; algorithme n&apos; a été démontrée que pour une certaine classe de méthodes d&apos; apprentissage . différentes conditions d&apos; arrêt peuvent être utilisées ; en pratique , fixer le nombre d&apos; itérations permet d&apos; assurer la terminaison de l&apos; algorithme , et ce quelle que soit la méthode d&apos; apprentissage utilisée . dans template : en damien ernst , pierre geurts , louis wehenkel , « tree-based batch mode reinforcement learning » , journal of machine learning research 6 , 2005 , p. 503 – 556 , plusieurs problèmes ont été utilisés pour évaluer l&apos; algorithme avec différentes méthodes d&apos; apprentissage supervisé , et avec plusieurs types de batch ( pure batch et growing batch ) . sur ces problèmes , l&apos; algorithme de fitted q iteration est très performant , en particulier en utilisant des méthodes d&apos; apprentissage non paramétriques ( comme la méthode des k plus proches voisins ) . ( template : en christophe thiery and bruno scherrer , least-squares \ lambda policy iteration : bias-variance trade-off in control problems , international conference on machine learning , haifa , israel , 2010 , template : lire en ligne , template : en li l. , williams j. d. , and balakrishnan s , reinforcement learning for dialog management using least-squares policy iteration and fast feature selection . in interspeech , 2009 , pp. 2475-2478 . ) . cet algorithme est une adaptation de l&apos; algorithme d&apos; itération de la politique exprimé sur la fonction q d&apos; états-actions . q ( s , a ) = \ sum _ { i = 1 } ^ k w _ i \ phi _ i ( s , a ) . les fonctions \ phi _ i forment donc une base d&apos; un sous-espace \ mathcal { q } de l&apos; espace des fonctions états-actions , choisie une fois pour toutes au départ . ainsi en interne , la fonction q est simplement représentée par les k coefficients w _ i. ceci permet de traiter efficacement des mdp possédant un grand nombre d&apos; états et / ou d&apos; actions . on peut même , dans ce formalisme , considérer des espaces d&apos; états ou d&apos; actions continus . l&apos; inconvénient , en revanche , est que l&apos; on est maintenant restreint aux fonctions de \ mathcal { q } . soit par la résolution d&apos; un système linéaire , soit par calcul itératif sur k. or même si par définition q \ in \ mathcal { q } , on n&apos; a pas en général k ( q ) \ in \ mathcal { q } . et donc d&apos; obtenir une approximation \ hat q ^ \ pi de q ^ \ pi qui est stable par la règle de mise à jour suivie de la projection . template : en michail lagoudakis , ronald parr , « least-squares policy iteration » , journal of machine learning research 4 , 2003 , template : p. ce point fixe est obtenu par résolution d&apos; un système linéaire . 2 . la deuxième différence est que lspi ne suppose pas le mdp connu ; l&apos; algorithme se base uniquement sur un jeu d&apos; échantillons du mdp , donnés sous la forme de quadruplets ( s , a , r , s &apos; ) . ces échantillons peuvent être donnés directement au départ de l&apos; algorithme ( mode pure-batch ) ou progressivement ( mode semi-batch ou en-ligne ) . concrètement , l&apos; algorithme se contente d&apos; effectuer les mises à jour précédentes sur le jeu d&apos; échantillon , éventuellement de façon répétée pour provoquer la convergence ( « experience replay » ) . on peut montrer qu&apos; en fait cela revient à appliquer la mise à jour sur le véritable mdp , mais avec les probabilités de transitions t ( s , a , s &apos; ) obtenues empiriquement selon la distribution des échantillons . ainsi , dans l&apos; hypothèse où cette distribution est conforme aux véritables probabilités de transition du mdp sous-jacent , le résultat converge asymptotiquement vers la véritable valeur q ^ \ pi , et donc finalement vers la fonction optimale q ^ * . on peut également remarquer que chaque échantillon contribue linéairement à chaque itération ( il ajoute un terme dans la somme définissant k et l&apos; opérateur de projection est linéaire ) , ce qui permet de mettre en place des optimisations lorsque les échantillons arrivent progressivement . la figure ci-contre résume schématiquement l&apos; algorithme lspi . dans , plusieurs applications de l&apos; algorithme lspi sont proposées . template : en jette randløv and preben alstrøm , « learning to drive a bicycle using reinforcement learning and shaping » , dans proceedings of the fifteenth international conference on machine learning ( icml &apos; 98 ) , 1998 , jude w. shavlik ( ed . ) . morgan kaufmann publishers inc . , san francisco , ca , usa , 463-471 . . dans ce problème l&apos; apprenant connaît à chaque pas de temps l&apos; angle et la vitesse angulaire du guidon , ainsi que l&apos; angle , la vitesse et accélération angulaire de l&apos; angle que forme le vélo avec la verticale . il doit alors choisir quelle force rotatoire appliquer au guidon ( choisie parmi trois possibilités ) , ainsi que le déplacement de son centre de masse par rapport au plan du vélo ( choisie également parmi 3 possibilités ) , de manière d&apos; une part à rester en équilibre ( c&apos; est-à-dire ici ne pas dépasser un angle vélo / sol de + / - 12 ° ) et d&apos; autre part atteindre une destination cible . la mise en œuvre est purement hors-ligne et consiste à observer le comportement d&apos; un agent aléatoire pour collecter de l&apos; ordre de quelques dizaines de milliers d&apos; échantillons sous forme de trajectoires . les trajectoires collectées sont ensuite coupées après quelques pas de temps , pour ne garder que la partie intéressante avant qu&apos; elles ne rentrent dans un scénario de chute inexorable . les récompenses sont en lien avec la distance atteinte par rapport à l&apos; objectif . quelques passes de l&apos; algorithme sont ensuite effectuées sur ces échantillons ( « experience replay » ) et une convergence très rapide vers des stratégies viables est observée .
